This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
docs-developers/
  generate-llms-script.md
  reveal-js-slides-for-talks.md
public/
  _redirects
  favicon.svg
  q3-browser.js
scripts/
  generate-llms.mjs
src/
  components/
    CommandPalette.svelte
    CopyForChat.svelte
    SearchTrigger.svelte
    Sidebar.astro
    Slides.astro
    ThemeToggle.svelte
  content/
    talks/
      2012/
        cuny_2011.html.md
        dubrovnik_2011.html.md
        glasgow_2012.html.md
        heidelberg_2011.html.md
        how_to_construct_cooperative_agents.html.md
        intention_and_motor_representation_cambridge.html.md
        intention_and_motor_representation_paris.html.md
        interacting_mindreaders.html.md
        joint_action_and_the_emergence_of_mindreading.html.md
        lyon_2012.html.md
        milan_2011.html.md
        minimal_theory_of_mind_cambridge.html.md
        motor_representation_shared_intention.html.md
        nijmegen_2011.html.md
        paris_2011.html.md
        science_festival_2011.html.md
        shared_agency_and_motor_representation.html.md
        shared_agency_with_parallel_planning.html.md
        stirling_2012.html.md
        tuebingen_2011.html.md
      2013/
        collective_agency_oxford.html.md
        collective_intentionality_vienna.html.md
        metacognition_and_mindreading.html.md
        motor_representation_in_joint_action.html.md
        not_just_wide_but_shared.html.md
        perceiving_anger_and_sharing_smiles.html.md
        planning_for_collective_agency_stuttgart.html.md
        planning_for_collective_agency_tuebingen.html.md
        two_systems_two_theories.html.md
        varieties_of_joint_action.html.md
      2014/
        joint_action_warwick.html.md
        joint_action_without_mindreading.html.md
        minimal_models_magdeburg.html.md
        naturalising_joint_actions.html.md
        shared_agency_involves_changing_perspective_manchester.html.md
      2015/
        antwerp_2015.html.md
        berlin_2015_core_knowledge.html.md
        bochum_2015.html.md
        bupdapest_moseo.html.md
        copenhagen_2015.html.md
        mindreading_espp.html.md
        mindreading_srcd.html.md
        practical_reasoning_motor_representation_intro.html.md
        purposive_action_and_intention.html.md
      2016/
        berlin_2016_2systems.md
        milan_2016_core_knowledge.html.md
        perceiving_mental_states_tuebingen.html.md
    teaching/
      joint_action_and_the_emergence/
        lecture01.html.md
        lecture02.html.md
        lecture02b.html.md
        lecture03.html.md
        lecture04.html.md
        lecture05.html.md
      mindreading_and_joint_action/
        lecture01.html.md
        lecture02.html.md
        lecture03.html.md
        lecture04.html.md
        lecture05.html.md
        lecture06.html.md
        lecture07.html.md
        lecture08.html.md
        lecture09.html.md
      joint_action_and_the_emergence.html.md
      mindreading_and_joint_action.html.md
    writing/
      awareness_of_belief.html.md
      blueprint_social_animal.html.md
      bpd_social_media.html.md
      childrens_selective_learning_from_others.html.md
      cognitive_architecture_belief_reasoning.html.md
      collective_goals.html.md
      coordinating_joint_action.html.md
      cue_competition_effects.html.md
      developing_mind.html.md
      drawn_together.html.md
      fizke_limits.html.md
      gaining_knowledge_via_other_minds.html.md
      goals_targets.html.md
      infants_representations_of_causation.html.md
      intention_motor.md
      interacting_mindreaders.md
      intuitions_about_joint_commitment.html.md
      joint_action_development.md
      joint_action_minimalist_approach.html.md
      joint_action_rpp_edited_volume.html.md
      joint_action_rpp_special_issue.html.md
      level_2.md
      lightbulb.html.md
      mindreading_balance.html.md
      minimal_architecture.html.md
      minimal_brains_discussion_intro.html.md
      minimal_brains_discussion_replies.html.md
      minimal_theory_of_mind.html.md
      modularity.html.md
      motor_representation_acting_together.html.md
      motor_representation_action_experience.html.md
      motor_representation_in_goal_ascription.html.md
      motor_representation_skill.html.md
      mummification.html.md
      perceiving_expressions_emotion.md
      planning_for_collective_agency.html.md
      primal_self.md
      psychological_research_on_joint_action.html.md
      puzzle_thought_experience_motoric.md
      review_consciousness.html.md
      review_joint_commitment.html.md
      review_self-knowing_agents.html.md
      review_the_rational_imagination.html.md
      review_thinking_without_words.html.md
      seeing_causes.html.md
      seeing_it_both_ways.html.md
      taking_apart.html.md
      tool_use_book.html.md
      tracking_representing_mental_states.html.md
      two_kinds_of_purposive_action.html.md
      two_systems_goal_ascription.html.md
      two_systems.html.md
      what_does_knowledge_explain.html.md
    config.ts
  layouts/
    BaseLayout.astro
    EmptyLayout.astro
    PageLayout.astro
  lib/
    components/
      ui/
        command/
          command-dialog.svelte
          command-empty.svelte
          command-group.svelte
          command-input.svelte
          command-item.svelte
          command-link-item.svelte
          command-list.svelte
          command-separator.svelte
          command-shortcut.svelte
          command.svelte
          index.ts
        dialog/
          dialog-close.svelte
          dialog-content.svelte
          dialog-description.svelte
          dialog-footer.svelte
          dialog-header.svelte
          dialog-overlay.svelte
          dialog-title.svelte
          dialog-trigger.svelte
          index.ts
    utils.ts
  pages/
    talks/
      [...slug].astro
    writing/
      [...slug].astro
    hashme-q3.astro
    index.astro
  styles/
    global.css
.gitignore
astro.config.mjs
components.json
migration-script.mjs
package.json
README.md
svelte.config.js
tailwind.config.mjs
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs-developers/generate-llms-script.md">
# LLM Text Generation Script Documentation

## Overview

The `generate-llms.mjs` script creates a consolidated text file (`public/llms.txt`) containing all academic content from the website. This file is designed to be consumed by Large Language Models (LLMs) to provide context about Stephen Butterfill's academic work.

## How It Works

### Purpose
The script combines all publications and talks into a single, LLM-friendly text format that includes:
- Publications from the `writing` collection
- Talks from the `talks` collection
- Metadata (title, authors, year, journal/event)
- Full content body in markdown format

### File Location
- **Script**: `scripts/generate-llms.mjs`
- **Output**: `public/llms.txt`

### Integration with Build Process
The script runs automatically during the build process:
```json
{
  "scripts": {
    "build": "node ./scripts/generate-llms.mjs && astro build"
  }
}
```

## Technical Implementation

### Dependencies
The script uses these Node.js modules:
- `fs-extra`: For file system operations
- `path`: For handling file paths
- `yaml`: For parsing YAML frontmatter

### Key Functions

#### 1. `parseFrontmatter(content)`
Extracts YAML frontmatter and markdown body from content files.

```javascript
function parseFrontmatter(content) {
  const frontmatterRegex = /^---\s*\n([\s\S]*?)\n---\s*\n([\s\S]*)$/;
  const match = content.match(frontmatterRegex);
  
  if (!match) {
    return { data: {}, body: content };
  }
  
  const [, frontmatterStr, body] = match;
  const data = yaml.parse(frontmatterStr);
  
  return { data, body };
}
```

#### 2. `readMarkdownFiles(dir)`
Recursively reads all `.md` files from a directory and its subdirectories.

- Handles nested folder structures (like `talks/2013/`, `talks/2014/`, etc.)
- Parses frontmatter for each file
- Adds a `type` field to distinguish between 'writing' and 'talk' content

### Content Processing

The script processes two content collections:

#### Writing Collection (`src/content/writing/`)
- **Frontmatter fields**: `title`, `authors`, `year`, `journal`, etc.
- **Structure**: Flat directory with `.md` files
- **Example**: `minimal_theory_of_mind.html.md`

#### Talks Collection (`src/content/talks/`)
- **Frontmatter fields**: `title`, `authors`, `pubDate`, `event`, etc.
- **Structure**: Nested by year (`2012/`, `2013/`, etc.)
- **Example**: `talks/2013/two_systems_two_theories.html.md`

### Output Format

Each item in the generated file follows this structure:

```
Title: [Title]
Authors: [Authors]
Year: [Year]
[Journal: [Journal Name]] // For publications
[Event: [Event Name]]     // For talks
Type: [Publication|Talk]

[Full markdown content]

---
```

## How to Modify the Script

### Adding New Content Types

1. **Add a new collection directory** (e.g., `src/content/books/`)
2. **Update the script** to read from the new directory:
   ```javascript
   const booksDir = path.join(process.cwd(), 'src/content/books');
   const bookItems = await readMarkdownFiles(booksDir);
   const allItems = [...writingItems, ...talkItems, ...bookItems];
   ```
3. **Handle new frontmatter fields** in the processing loop

### Modifying Output Format

To change how content appears in the output file, modify the loop in the main section:

```javascript
for (const item of allItems) {
  // Customize the output format here
  fullText += `Title: ${item.data.title || 'Untitled'}\n`;
  // Add new fields or change formatting
}
```

### Adding Filtering or Sorting

To filter or sort content before processing:

```javascript
// Example: Only include items from 2020 onwards
const filteredItems = allItems.filter(item => {
  const year = item.type === 'talk' 
    ? new Date(item.data.pubDate).getFullYear()
    : item.data.year;
  return year >= 2020;
});

// Example: Sort by year (newest first)
const sortedItems = allItems.sort((a, b) => {
  const yearA = a.type === 'talk' 
    ? new Date(a.data.pubDate).getFullYear()
    : a.data.year;
  const yearB = b.type === 'talk' 
    ? new Date(b.data.pubDate).getFullYear()
    : b.data.year;
  return yearB - yearA;
});
```

## Running the Script

### Standalone Execution
```bash
node ./scripts/generate-llms.mjs
```

### As Part of Build Process
```bash
npm run build
```

## Troubleshooting

### Common Issues

1. **"ERR_UNSUPPORTED_ESM_URL_SCHEME" Error**
   - This occurs if you try to import Astro-specific modules (like `astro:content`)
   - Solution: Use direct file system operations instead

2. **Missing Content**
   - Check that the directory structure matches what the script expects
   - Ensure `.md` files have proper frontmatter format

3. **YAML Parsing Errors**
   - Verify frontmatter syntax in content files
   - Check for special characters that need escaping

### Debugging Tips

1. **Add logging** to see what files are being processed:
   ```javascript
   console.log(`Processing: ${fullPath}`);
   ```

2. **Check file counts**:
   ```javascript
   console.log(`Found ${writingItems.length} writing items`);
   console.log(`Found ${talkItems.length} talk items`);
   ```

3. **Validate output** by checking the generated file size and content

## Future Enhancements

Potential improvements for the script:
- Add content validation (check for required frontmatter fields)
- Include teaching materials from the `teaching` collection
- Add metadata about file modification dates
- Implement incremental updates (only process changed files)
- Add support for different output formats (JSON, XML, etc.)
</file>

<file path="docs-developers/reveal-js-slides-for-talks.md">
# Reveal.js Slides for Talks

This document explains how the reveal.js slide presentation system is implemented for talk pages in the Astro static site.

## Overview

Talk pages can display interactive slide presentations using reveal.js. When a talk has `slideImages` defined in its frontmatter, the slides are automatically rendered as an interactive presentation that users can navigate through.

## How It Works

### 1. Content Structure

Talk content files are located in `src/content/talks/` and use markdown with frontmatter. To enable slides, add a `slideImages` array to the frontmatter:

```markdown
---
title: "My Talk Title"
authors: "Author Name"
pubDate: 2024-01-01T00:00:00.000Z
slideImages:
  - /img/talks/my-talk/slide-000.jpg
  - /img/talks/my-talk/slide-001.jpg
  - /img/talks/my-talk/slide-002.jpg
---

## Abstract
Your talk content here...
```

### 2. Static Assets Setup

Reveal.js assets must be copied to the `public/` directory so they can be served statically:

```bash
# From the project root
mkdir -p public/reveal.js/dist
cp -r node_modules/reveal.js/dist/* public/reveal.js/dist/
```

This copies:
- `reveal.css` - Core reveal.js styles
- `reveal.js` - Main reveal.js library
- `theme/` directory - Various presentation themes

### 3. Component Architecture

#### Talk Page Template (`src/pages/talks/[...slug].astro`)

The talk page template conditionally renders the Slides component when `slideImages` exist:

```astro
{entry.data.slideImages && entry.data.slideImages.length > 0 && (
  <div class="my-8">
    <h2 class="text-2xl font-bold mb-4">Slides</h2>
    <Slides images={entry.data.slideImages} />
  </div>
)}
```

#### Slides Component (`src/components/Slides.astro`)

The Slides component handles the reveal.js integration:

**Key Features:**
- **HTML Structure**: Creates the required `.reveal > .slides > section` hierarchy
- **Asset Loading**: Links to reveal.js CSS and JavaScript files
- **CSS Overrides**: Uses `!important` styles to override Tailwind CSS conflicts
- **Initialization**: Robust JavaScript initialization with retry logic
- **Responsive Images**: Slides contain images with proper sizing

**Critical Implementation Details:**

1. **Fixed Dimensions**: The reveal container has explicit dimensions to prevent layout issues
2. **CSS Conflicts Resolution**: Uses `!important` to override Tailwind CSS that might hide slides
3. **Retry Logic**: JavaScript initialization retries if reveal.js hasn't loaded yet
4. **Embedded Mode**: Uses `embedded: true` to integrate within the page layout

## Adding Slides to a New Talk

### Step 1: Prepare Slide Images

1. Create a directory in `public/img/talks/` for your talk:
   ```
   public/img/talks/my-new-talk/
   ```

2. Add your slide images (typically JPG or PNG):
   ```
   public/img/talks/my-new-talk/slide-000.jpg
   public/img/talks/my-new-talk/slide-001.jpg
   public/img/talks/my-new-talk/slide-002.jpg
   ```

### Step 2: Update Talk Frontmatter

Add the `slideImages` array to your talk's markdown file:

```markdown
---
title: "My New Talk"
authors: "Your Name"
pubDate: 2024-01-01T00:00:00.000Z
slideImages:
  - /img/talks/my-new-talk/slide-000.jpg
  - /img/talks/my-new-talk/slide-001.jpg
  - /img/talks/my-new-talk/slide-002.jpg
---
```

### Step 3: Test

1. Start the dev server: `npm run dev`
2. Navigate to your talk page: `http://localhost:4321/talks/YYYY/your-talk-slug`
3. Verify slides appear and are navigable with arrow keys

## Troubleshooting

### Slides Flash and Disappear

This usually indicates CSS conflicts. Check:
1. Reveal.js assets are properly copied to `public/reveal.js/dist/`
2. CSS overrides in the Slides component are working
3. Browser console for JavaScript errors

### Slides Don't Initialize

Check browser console for:
- "Initializing Reveal.js..." message
- "Reveal.js initialized successfully" message
- Any JavaScript errors

Common issues:
- Reveal.js files not found (404 errors)
- JavaScript loading order problems
- CSS conflicts preventing display

### Images Don't Load

Verify:
1. Image paths in frontmatter are correct (start with `/`)
2. Images exist in the `public/` directory
3. Image file names match exactly (case-sensitive)

## Technical Notes

### Why Copy Assets to Public?

Astro serves the `public/` directory as static assets. Node modules aren't accessible to the browser, so reveal.js files must be copied to `public/` during setup.

### CSS Conflicts with Tailwind

Tailwind CSS can interfere with reveal.js styles. The Slides component uses:
- `!important` declarations to override Tailwind
- Explicit positioning and display properties
- Fixed dimensions to prevent layout collapse

### Embedded vs Fullscreen Mode

The implementation uses `embedded: true` to integrate slides within the page layout rather than taking over the entire viewport. This allows slides to coexist with other page content.

## Dependencies

- `reveal.js` npm package (installed in package.json)
- Static asset copying (manual step during setup)
- Astro's static site generation
- Compatible with Tailwind CSS (with overrides)
</file>

<file path="public/favicon.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 128 128">
    <path d="M50.4 78.5a75.1 75.1 0 0 0-28.5 6.9l24.2-65.7c.7-2 1.9-3.2 3.4-3.2h29c1.5 0 2.7 1.2 3.4 3.2l24.2 65.7s-11.6-7-28.5-7L67 45.5c-.4-1.7-1.6-2.8-2.9-2.8-1.3 0-2.5 1.1-2.9 2.7L50.4 78.5Zm-1.1 28.2Zm-4.2-20.2c-2 6.6-.6 15.8 4.2 20.2a17.5 17.5 0 0 1 .2-.7 5.5 5.5 0 0 1 5.7-4.5c2.8.1 4.3 1.5 4.7 4.7.2 1.1.2 2.3.2 3.5v.4c0 2.7.7 5.2 2.2 7.4a13 13 0 0 0 5.7 4.9v-.3l-.2-.3c-1.8-5.6-.5-9.5 4.4-12.8l1.5-1a73 73 0 0 0 3.2-2.2 16 16 0 0 0 6.8-11.4c.3-2 .1-4-.6-6l-.8.6-1.6 1a37 37 0 0 1-22.4 2.7c-5-.7-9.7-2-13.2-6.2Z" />
    <style>
        path { fill: #000; }
        @media (prefers-color-scheme: dark) {
            path { fill: #FFF; }
        }
    </style>
</svg>
</file>

<file path="public/q3-browser.js">
/**
 * JavaScript implementation of q3.py functionality as an ES module
 * 
 * This must match the Python implementation in q3.py (which is the key source)
 * You can test the match by running `test_q3.js`
 *
 * 
 */

// A modern, secure replacement for the hex_sha512 function
async function secure_sha512_hex(str) {
  const textEncoder = new TextEncoder();
  const data = textEncoder.encode(str);
  const hashBuffer = await window.crypto.subtle.digest('SHA-512', data);
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  // Convert bytes to a hex string
  const hexHash = hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
  return hexHash;
}

// Helper function to check if password meets requirements
function pwMeetsRequirements(pw) {
  const hasUpper = /[A-Z]/.test(pw);
  const hasLower = /[a-z]/.test(pw);
  const hasDigit = /[0-9]/.test(pw);
  const hasSymbol = /[!@#$%^&*()\-_=+\[\]{}|;:,.<>?\/]/.test(pw);
  // Check the password does not contain commonly banned characters
  const bannedChars = new Set(['\'', '"', '\\', '`', '~', '?', '@']);
  for (let i = 0; i < pw.length; i++) {
    if (bannedChars.has(pw[i])) {
      return false;
    }
  }
  return hasUpper && hasLower && hasDigit && hasSymbol;
}

// Helper function to get password from hash
function _getPassword(hexDigest) {
  const b64 = python_b64encode(hexDigest);
  const res = b64.slice(5, 5+12) + '!'; 
  return res;
}


// Main function to generate password from domain and master password
async function go(domain, pw) {
  let cumulativeInput = `${pw}!@#` + domain;

  // First password generation
  let hexDigest = await secure_sha512_hex(cumulativeInput);
  let res = _getPassword(hexDigest);

  // Second password generation
  cumulativeInput += res;
  hexDigest = await secure_sha512_hex(cumulativeInput);
  res = _getPassword(hexDigest);

  // Continue generating passwords until one meets the requirements.
  while (!pwMeetsRequirements(res)) {
    cumulativeInput += res;
    hexDigest = await secure_sha512_hex(cumulativeInput);
    res = _getPassword(hexDigest);
  }

  return res;
}


// ----------------
// helper functions
// ----------------

/**
 * function equivalent to base64.b64encode in Python
 */
function python_b64encode(str) {
    return base64ArrayBuffer(str2ab(str));
}

/**
 * convert string to array buffer
 * modified from:
 * http://stackoverflow.com/questions/6965107/converting-between-strings-and-arraybuffers
 * note that we're using Uint8Array where the author uses Uint16Array
 */
function str2ab(str) {
  var buf = new ArrayBuffer(str.length); // 1 byte for each char
  var bufView = new Uint8Array(buf);
  for (var i=0, strLen=str.length; i<strLen; i++) {
    bufView[i] = str.charCodeAt(i);
  }
  return buf;
}

/**
 * convert arrayBuffer to base64
 * http://stackoverflow.com/questions/7370943/retrieving-binary-file-content-using-javascript-base64-encode-it-and-reverse-de?rq=1
 * https://gist.github.com/958841
 */
function base64ArrayBuffer(arrayBuffer) {
  var base64    = ''
  var encodings = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/'

  var bytes         = new Uint8Array(arrayBuffer)
  var byteLength    = bytes.byteLength
  var byteRemainder = byteLength % 3
  var mainLength    = byteLength - byteRemainder

  var a, b, c, d
  var chunk

  // Main loop deals with bytes in chunks of 3
  for (var i = 0; i < mainLength; i = i + 3) {
    // Combine the three bytes into a single integer
    chunk = (bytes[i] << 16) | (bytes[i + 1] << 8) | bytes[i + 2]

    // Use bitmasks to extract 6-bit segments from the triplet
    a = (chunk & 16515072) >> 18 // 16515072 = (2^6 - 1) << 18
    b = (chunk & 258048)   >> 12 // 258048   = (2^6 - 1) << 12
    c = (chunk & 4032)     >>  6 // 4032     = (2^6 - 1) << 6
    d = chunk & 63               // 63       = 2^6 - 1

    // Convert the raw binary segments to the appropriate ASCII encoding
    base64 += encodings[a] + encodings[b] + encodings[c] + encodings[d]
  }

  // Deal with the remaining bytes and padding
  if (byteRemainder == 1) {
    chunk = bytes[mainLength]

    a = (chunk & 252) >> 2 // 252 = (2^6 - 1) << 2

    // Set the 4 least significant bits to zero
    b = (chunk & 3)   << 4 // 3   = 2^2 - 1

    base64 += encodings[a] + encodings[b] + '=='
  } else if (byteRemainder == 2) {
    chunk = (bytes[mainLength] << 8) | bytes[mainLength + 1]

    a = (chunk & 64512) >> 10 // 64512 = (2^6 - 1) << 10
    b = (chunk & 1008)  >>  4 // 1008  = (2^6 - 1) << 4

    // Set the 2 least significant bits to zero
    c = (chunk & 15)    <<  2 // 15    = 2^4 - 1

    base64 += encodings[a] + encodings[b] + encodings[c] + '='
  }

  return base64
}




// Export the go function as default
export default go;
</file>

<file path="src/components/CopyForChat.svelte">
<script>
  export let contentToCopy = '';
  let buttonText = 'Copy for Chat';

  async function copyToClipboard() {
    try {
      await navigator.clipboard.writeText(contentToCopy);
      buttonText = 'Copied!';
      setTimeout(() => { buttonText = 'Copy for Chat'; }, 2000);
    } catch (err) {
      console.error('Failed to copy: ', err);
      buttonText = 'Failed to copy';
    }
  }
</script>

<button on:click={copyToClipboard} class="px-4 py-2 bg-slate-200 dark:bg-slate-700 rounded hover:bg-slate-300 dark:hover:bg-slate-600">
  {buttonText}
</button>
</file>

<file path="src/components/SearchTrigger.svelte">
<script lang="ts">
  import { onMount } from 'svelte';
  
  // This component acts as a trigger for the command palette
  // It dispatches a custom event that the CommandPalette component listens for
  let searchTriggerRef: HTMLButtonElement;
  
  function openCommandPalette() {
    // Dispatch a custom event that the CommandPalette component will listen for
    window.dispatchEvent(new CustomEvent('open-command-palette'));
  }
  
  // Listen for the keyboard shortcut to focus this element briefly (for visual feedback)
  onMount(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.key === 'k' && (e.metaKey || e.ctrlKey)) {
        // Add a brief focus effect when the keyboard shortcut is used
        searchTriggerRef?.focus();
        setTimeout(() => searchTriggerRef?.blur(), 100);
      }
    };
    
    document.addEventListener('keydown', handleKeyDown);
    return () => document.removeEventListener('keydown', handleKeyDown);
  });
</script>

<button
  bind:this={searchTriggerRef}
  on:click={openCommandPalette}
  class="flex items-center space-x-2 px-3 py-1.5 text-sm text-gray-500 dark:text-gray-400 bg-gray-50 dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-md hover:bg-gray-100 dark:hover:bg-gray-700 hover:text-gray-700 dark:hover:text-gray-300 transition-colors focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 focus:ring-offset-white dark:focus:ring-offset-gray-900"
>
  <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />
  </svg>
  <span>Search...</span>
  <kbd class="hidden sm:inline-block px-1.5 py-0.5 text-xs font-medium text-gray-400 dark:text-gray-500 bg-gray-100 dark:bg-gray-700 border border-gray-200 dark:border-gray-600 rounded">
    ⌘K
  </kbd>
</button>
</file>

<file path="src/components/Sidebar.astro">
---
const navItems = [
  { href: '#writing', label: 'Writing' },
  { href: '#talks', label: 'Talks' },
  { href: '#teaching', label: 'Teaching' },
  { href: '#about', label: 'About' },
];
---
<nav class="sticky top-8">
  <ul id="sidebar-nav">
    {navItems.map(item => (
      <li>
        <a href={item.href} class="block p-2 rounded hover:bg-slate-200 dark:hover:bg-slate-700">
          {item.label}
        </a>
      </li>
    ))}
  </ul>
</nav>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    const sections = document.querySelectorAll('main section[id]');
    const navLinks = document.querySelectorAll('#sidebar-nav a');

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          navLinks.forEach(link => {
            link.classList.toggle('font-bold', link.getAttribute('href') === `#${entry.target.id}`);
            link.classList.toggle('bg-slate-100', link.getAttribute('href') === `#${entry.target.id}`);
            link.classList.toggle('dark:bg-slate-800', link.getAttribute('href') === `#${entry.target.id}`);
          });
        }
      });
    }, { rootMargin: "-50% 0px -50% 0px" }); // Triggers when a section is in the middle of the viewport

    sections.forEach(section => observer.observe(section));
  });
</script>
</file>

<file path="src/components/ThemeToggle.svelte">
<script>
  import { onMount } from 'svelte';
  
  let theme = 'light';
  let mounted = false;

  onMount(() => {
    mounted = true;
    // Get the current theme
    if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
      theme = localStorage.getItem('theme');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      theme = 'dark';
    }
  });

  function toggleTheme() {
    theme = theme === 'light' ? 'dark' : 'light';
    
    if (theme === 'dark') {
      document.documentElement.classList.add('dark');
    } else {
      document.documentElement.classList.remove('dark');
    }
    
    localStorage.setItem('theme', theme);
  }
</script>

{#if mounted}
  <button
    on:click={toggleTheme}
    class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-blue-500 focus-visible:ring-offset-2 focus-visible:ring-offset-white dark:focus-visible:ring-offset-slate-900 disabled:opacity-50 disabled:pointer-events-none border border-slate-200 dark:border-slate-800 bg-transparent hover:bg-slate-100 dark:hover:bg-slate-800 hover:text-slate-800 dark:hover:text-slate-200 h-10 w-10"
    aria-label="Toggle theme"
  >
    {#if theme === 'dark'}
      <!-- Sun icon for light mode -->
      <svg class="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z" />
      </svg>
    {:else}
      <!-- Moon icon for dark mode -->
      <svg class="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
      </svg>
    {/if}
  </button>
{/if}
</file>

<file path="src/content/talks/2012/cuny_2011.html.md">
---
title: How to Construct a Minimal Theory of Mind
authors: Stephen A. Butterfill
pubDate: 2011-10-12T23:00:00.000Z
address: CUNY Graduate Center, New York
---
</file>

<file path="src/content/talks/2012/dubrovnik_2011.html.md">
---
title: Talking About and Seeing Blue
authors: Stephen A. Butterfill
pubDate: 2011-04-10T23:00:00.000Z
endDate: 2011-04-14T23:00:00.000Z
event: Colour and Sensory Knowledge, IUC Philosophy of Science Conference
address: Dubrovnik
slidesUrl: /pdf/talks/dubrovnik_2011.slides.pdf
slideImages:
  - /img/talks/dubrovnik_2011/slide-000.jpeg
  - /img/talks/dubrovnik_2011/slide-001.jpeg
  - /img/talks/dubrovnik_2011/slide-002.jpeg
  - /img/talks/dubrovnik_2011/slide-003.jpeg
  - /img/talks/dubrovnik_2011/slide-004.jpeg
  - /img/talks/dubrovnik_2011/slide-005.jpeg
  - /img/talks/dubrovnik_2011/slide-006.jpeg
  - /img/talks/dubrovnik_2011/slide-007.jpeg
  - /img/talks/dubrovnik_2011/slide-008.jpeg
  - /img/talks/dubrovnik_2011/slide-009.jpeg
  - /img/talks/dubrovnik_2011/slide-010.jpeg
  - /img/talks/dubrovnik_2011/slide-011.jpeg
  - /img/talks/dubrovnik_2011/slide-012.jpeg
  - /img/talks/dubrovnik_2011/slide-013.jpeg
  - /img/talks/dubrovnik_2011/slide-014.jpeg
  - /img/talks/dubrovnik_2011/slide-015.jpeg
  - /img/talks/dubrovnik_2011/slide-016.jpeg
  - /img/talks/dubrovnik_2011/slide-017.jpeg
  - /img/talks/dubrovnik_2011/slide-018.jpeg
  - /img/talks/dubrovnik_2011/slide-019.jpeg
  - /img/talks/dubrovnik_2011/slide-020.jpeg
  - /img/talks/dubrovnik_2011/slide-021.jpeg
  - /img/talks/dubrovnik_2011/slide-022.jpeg
  - /img/talks/dubrovnik_2011/slide-023.jpeg
  - /img/talks/dubrovnik_2011/slide-024.jpeg
  - /img/talks/dubrovnik_2011/slide-025.jpeg
  - /img/talks/dubrovnik_2011/slide-026.jpeg
  - /img/talks/dubrovnik_2011/slide-027.jpeg
  - /img/talks/dubrovnik_2011/slide-028.jpeg
  - /img/talks/dubrovnik_2011/slide-029.jpeg
  - /img/talks/dubrovnik_2011/slide-030.jpeg
  - /img/talks/dubrovnik_2011/slide-031.jpeg
  - /img/talks/dubrovnik_2011/slide-032.jpeg
  - /img/talks/dubrovnik_2011/slide-033.jpeg
  - /img/talks/dubrovnik_2011/slide-034.jpeg
  - /img/talks/dubrovnik_2011/slide-035.jpeg
  - /img/talks/dubrovnik_2011/slide-036.jpeg
  - /img/talks/dubrovnik_2011/slide-037.jpeg
  - /img/talks/dubrovnik_2011/slide-038.jpeg
  - /img/talks/dubrovnik_2011/slide-039.jpeg
  - /img/talks/dubrovnik_2011/slide-040.jpeg
  - /img/talks/dubrovnik_2011/slide-041.jpeg
  - /img/talks/dubrovnik_2011/slide-042.jpeg
  - /img/talks/dubrovnik_2011/slide-043.jpeg
  - /img/talks/dubrovnik_2011/slide-044.jpeg
  - /img/talks/dubrovnik_2011/slide-045.jpeg
  - /img/talks/dubrovnik_2011/slide-046.jpeg
  - /img/talks/dubrovnik_2011/slide-047.jpeg
  - /img/talks/dubrovnik_2011/slide-048.jpeg
  - /img/talks/dubrovnik_2011/slide-049.jpeg
  - /img/talks/dubrovnik_2011/slide-050.jpeg
  - /img/talks/dubrovnik_2011/slide-051.jpeg
  - /img/talks/dubrovnik_2011/slide-052.jpeg
  - /img/talks/dubrovnik_2011/slide-053.jpeg
  - /img/talks/dubrovnik_2011/slide-054.jpeg
  - /img/talks/dubrovnik_2011/slide-055.jpeg
  - /img/talks/dubrovnik_2011/slide-056.jpeg
  - /img/talks/dubrovnik_2011/slide-057.jpeg
  - /img/talks/dubrovnik_2011/slide-058.jpeg
  - /img/talks/dubrovnik_2011/slide-059.jpeg
  - /img/talks/dubrovnik_2011/slide-060.jpeg
  - /img/talks/dubrovnik_2011/slide-061.jpeg
  - /img/talks/dubrovnik_2011/slide-062.jpeg
  - /img/talks/dubrovnik_2011/slide-063.jpeg
  - /img/talks/dubrovnik_2011/slide-064.jpeg
  - /img/talks/dubrovnik_2011/slide-065.jpeg
  - /img/talks/dubrovnik_2011/slide-066.jpeg
  - /img/talks/dubrovnik_2011/slide-067.jpeg
  - /img/talks/dubrovnik_2011/slide-068.jpeg
  - /img/talks/dubrovnik_2011/slide-069.jpeg
  - /img/talks/dubrovnik_2011/slide-070.jpeg
  - /img/talks/dubrovnik_2011/slide-071.jpeg
  - /img/talks/dubrovnik_2011/slide-072.jpeg
---

## Abstract

Categorical perception is a pervasive and useful feature of human experience much neglected by philosophers despite illuminating psychological research on this topic.  The findings of this research on categorical perception are relevant to the truth of basic claims about which properties humans can perceptually experience and they bear on the validity of arguments about the nature of perception.  Focusing on the case of colour, this talk reviews some of the key evidence for categorical perception and extracts a characterisation.  Together with findings about the development of categorical perception of colour and its complex relations to the use of verbal labels for colour categories, this provides a basis for rejecting what Frank Jackson calls a "subject-determining platitude", namely that "'red' denotes the property of an object putatively presented in visual experience" (Jackson 1996: 200).  Categorical perception of colour differs from other forms of perception with respect to phenomenology and epistemology.  Seeing blue or red, unlike seeing the particular colours of things, does not involve the presentation in visual experience of a property.
</file>

<file path="src/content/talks/2012/glasgow_2012.html.md">
---
title: Intention and Motor Representation in Action Explanation
authors: Stephen A. Butterfill
pubDate: 2012-04-15T23:00:00.000Z
address: School of Psychology, University of Glasgow
---
</file>

<file path="src/content/talks/2012/heidelberg_2011.html.md">
---
title: Mindreading and Joint Action
authors: Stephen A. Butterfill
pubDate: 2011-05-18T23:00:00.000Z
event: Das Gehirn - ein Beziehungsorgan
address: University of Heidelberg
handoutUrl: /pdf/talks/heidelberg_2011.handout.pdf
slidesUrl: /pdf/talks/heidelberg_2011.slides.pdf
slideImages:
  - /img/talks/heidelberg_2011/slide-000.jpeg
  - /img/talks/heidelberg_2011/slide-001.jpeg
  - /img/talks/heidelberg_2011/slide-002.jpeg
  - /img/talks/heidelberg_2011/slide-003.jpeg
  - /img/talks/heidelberg_2011/slide-004.jpeg
  - /img/talks/heidelberg_2011/slide-005.jpeg
  - /img/talks/heidelberg_2011/slide-006.jpeg
  - /img/talks/heidelberg_2011/slide-007.jpeg
  - /img/talks/heidelberg_2011/slide-008.jpeg
  - /img/talks/heidelberg_2011/slide-009.jpeg
  - /img/talks/heidelberg_2011/slide-010.jpeg
  - /img/talks/heidelberg_2011/slide-011.jpeg
  - /img/talks/heidelberg_2011/slide-012.jpeg
  - /img/talks/heidelberg_2011/slide-013.jpeg
  - /img/talks/heidelberg_2011/slide-014.jpeg
  - /img/talks/heidelberg_2011/slide-015.jpeg
  - /img/talks/heidelberg_2011/slide-016.jpeg
  - /img/talks/heidelberg_2011/slide-017.jpeg
  - /img/talks/heidelberg_2011/slide-018.jpeg
  - /img/talks/heidelberg_2011/slide-019.jpeg
  - /img/talks/heidelberg_2011/slide-020.jpeg
  - /img/talks/heidelberg_2011/slide-021.jpeg
  - /img/talks/heidelberg_2011/slide-022.jpeg
  - /img/talks/heidelberg_2011/slide-023.jpeg
  - /img/talks/heidelberg_2011/slide-024.jpeg
  - /img/talks/heidelberg_2011/slide-025.jpeg
  - /img/talks/heidelberg_2011/slide-026.jpeg
  - /img/talks/heidelberg_2011/slide-027.jpeg
  - /img/talks/heidelberg_2011/slide-028.jpeg
  - /img/talks/heidelberg_2011/slide-029.jpeg
  - /img/talks/heidelberg_2011/slide-030.jpeg
  - /img/talks/heidelberg_2011/slide-031.jpeg
  - /img/talks/heidelberg_2011/slide-032.jpeg
  - /img/talks/heidelberg_2011/slide-033.jpeg
  - /img/talks/heidelberg_2011/slide-034.jpeg
  - /img/talks/heidelberg_2011/slide-035.jpeg
  - /img/talks/heidelberg_2011/slide-036.jpeg
  - /img/talks/heidelberg_2011/slide-037.jpeg
  - /img/talks/heidelberg_2011/slide-038.jpeg
  - /img/talks/heidelberg_2011/slide-039.jpeg
  - /img/talks/heidelberg_2011/slide-040.jpeg
  - /img/talks/heidelberg_2011/slide-041.jpeg
  - /img/talks/heidelberg_2011/slide-042.jpeg
  - /img/talks/heidelberg_2011/slide-043.jpeg
  - /img/talks/heidelberg_2011/slide-044.jpeg
  - /img/talks/heidelberg_2011/slide-045.jpeg
  - /img/talks/heidelberg_2011/slide-046.jpeg
  - /img/talks/heidelberg_2011/slide-047.jpeg
  - /img/talks/heidelberg_2011/slide-048.jpeg
  - /img/talks/heidelberg_2011/slide-049.jpeg
  - /img/talks/heidelberg_2011/slide-050.jpeg
  - /img/talks/heidelberg_2011/slide-051.jpeg
  - /img/talks/heidelberg_2011/slide-052.jpeg
  - /img/talks/heidelberg_2011/slide-053.jpeg
  - /img/talks/heidelberg_2011/slide-054.jpeg
  - /img/talks/heidelberg_2011/slide-055.jpeg
  - /img/talks/heidelberg_2011/slide-056.jpeg
  - /img/talks/heidelberg_2011/slide-057.jpeg
  - /img/talks/heidelberg_2011/slide-058.jpeg
  - /img/talks/heidelberg_2011/slide-059.jpeg
  - /img/talks/heidelberg_2011/slide-060.jpeg
  - /img/talks/heidelberg_2011/slide-061.jpeg
  - /img/talks/heidelberg_2011/slide-062.jpeg
  - /img/talks/heidelberg_2011/slide-063.jpeg
  - /img/talks/heidelberg_2011/slide-064.jpeg
  - /img/talks/heidelberg_2011/slide-065.jpeg
  - /img/talks/heidelberg_2011/slide-066.jpeg
  - /img/talks/heidelberg_2011/slide-067.jpeg
  - /img/talks/heidelberg_2011/slide-068.jpeg
  - /img/talks/heidelberg_2011/slide-069.jpeg
  - /img/talks/heidelberg_2011/slide-070.jpeg
  - /img/talks/heidelberg_2011/slide-071.jpeg
  - /img/talks/heidelberg_2011/slide-072.jpeg
  - /img/talks/heidelberg_2011/slide-073.jpeg
  - /img/talks/heidelberg_2011/slide-074.jpeg
  - /img/talks/heidelberg_2011/slide-075.jpeg
  - /img/talks/heidelberg_2011/slide-076.jpeg
  - /img/talks/heidelberg_2011/slide-077.jpeg
  - /img/talks/heidelberg_2011/slide-078.jpeg
  - /img/talks/heidelberg_2011/slide-079.jpeg
  - /img/talks/heidelberg_2011/slide-080.jpeg
  - /img/talks/heidelberg_2011/slide-081.jpeg
---

## Abstract

In this talk I present some research on minimal theory of mind, on joint action and on relations between the two.
</file>

<file path="src/content/talks/2012/how_to_construct_cooperative_agents.html.md">
---
title: How to Construct Cooperative Agents
authors: Stephen A. Butterfill
pubDate: 2013-04-07T23:00:00.000Z
endDate: 2013-04-08T23:00:00.000Z
event: "Cooperation: Why, How and With Whom?"
address: University of Aarhus
---
</file>

<file path="src/content/talks/2012/intention_and_motor_representation_cambridge.html.md">
---
title: Intention and Motor Representation in Joint Action
authors: Stephen A. Butterfill
pubDate: 2012-03-12T00:00:00.000Z
endDate: 2012-03-14T00:00:00.000Z
event: Pre-Reflective and Reflective Processing in Social Interaction
address: University of Cambridge
handoutUrl: /pdf/talks/intention_and_motor_representation_cambridge.handout.pdf
slidesUrl: /pdf/talks/intention_and_motor_representation_cambridge.slides.pdf
slideImages:
  - /img/talks/intention_and_motor_representation_cambridge/slide-000.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-001.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-002.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-003.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-004.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-005.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-006.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-007.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-008.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-009.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-010.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-011.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-012.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-013.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-014.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-015.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-016.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-017.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-018.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-019.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-020.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-021.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-022.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-023.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-024.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-025.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-026.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-027.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-028.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-029.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-030.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-031.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-032.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-033.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-034.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-035.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-036.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-037.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-038.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-039.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-040.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-041.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-042.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-043.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-044.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-045.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-046.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-047.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-048.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-049.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-050.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-051.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-052.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-053.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-054.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-055.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-056.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-057.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-058.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-059.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-060.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-061.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-062.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-063.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-064.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-065.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-066.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-067.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-068.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-069.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-070.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-071.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-072.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-073.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-074.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-075.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-076.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-077.jpeg
  - /img/talks/intention_and_motor_representation_cambridge/slide-078.jpeg
---

## Abstract

On the assumption that social motor representation plays a role in explaining how effective joint action is possible, is there also a role for motor representation in explaining what joint action is? Philosophers tend to assume that motor representation is only an enabling condition for joint action and of no direct interest to narrowly philosophical theories of joint action and shared intention. In this talk I shall argue that social motor representation and shared intention have distinctive roles in explaining the purposiveness of joint action. This gives rise to a challenge. On the one hand, effective joint action--imagine two people erecting a tent in a gale together--sometimes requires both shared intentions and social motor representations plus a certain kind of harmony between the two. On the other hand, recognizing their distinctive roles precludes the existence of direct inferential links between shared intentions and social motor representations. The challenge is to explain how these two kinds of representation could sometimes harmoniously contribute to effective joint action despite the lack of inferential integration.
</file>

<file path="src/content/talks/2012/intention_and_motor_representation_paris.html.md">
---
title: Intention and Motor Representation in Joint Action
authors: Stephen A. Butterfill
pubDate: 2012-03-09T00:00:00.000Z
address: Institut Jean-Nicod, Ecole Normale Sup&eacute;rieure, Paris
---
</file>

<file path="src/content/talks/2012/interacting_mindreaders.html.md">
---
title: Interacting Mindreaders
authors: Stephen A. Butterfill
pubDate: 2012-09-04T23:00:00.000Z
address: Department of Cognitive Science, Central European University, Budapest
---
</file>

<file path="src/content/talks/2012/joint_action_and_the_emergence_of_mindreading.html.md">
---
title: Joint Action and the Emergence of Mindreading
authors: Stephen A. Butterfill
pubDate: 2013-05-01T23:00:00.000Z
event: Origins of Social Cognition Lecture Series
address: University of Antwerp
---
</file>

<file path="src/content/talks/2012/lyon_2012.html.md">
---
title: Intention and Motor Representation in Explaining Action
authors: Stephen A. Butterfill
pubDate: 2012-03-23T00:00:00.000Z
endDate: 2012-03-24T00:00:00.000Z
event: Reconceptions of Action
address: Ecole Normale Superieure, Lyon
---
</file>

<file path="src/content/talks/2012/milan_2011.html.md">
---
title: "Joint Action: Conceptual Tools for Scientific Research"
authors: Stephen A. Butterfill
pubDate: 2011-02-15T00:00:00.000Z
address: Department of Philosophy, Universit&agrave; degli Studi di Milano
---
</file>

<file path="src/content/talks/2012/minimal_theory_of_mind_cambridge.html.md">
---
title: "Minimal Theory of Mind: How to Measure Mental States"
authors: Stephen A. Butterfill
pubDate: 2013-04-29T23:00:00.000Z
address: Department of Psychology, University of Cambridge
---
</file>

<file path="src/content/talks/2012/motor_representation_shared_intention.html.md">
---
title: Motor Representation and Shared Intention
authors: Stephen A. Butterfill
pubDate: 2012-08-27T23:00:00.000Z
endDate: 2012-08-30T23:00:00.000Z
event: Collective Intentionality VII
address: University of Manchester
handoutUrl: /pdf/talks/motor_representation_shared_intention.handout.pdf
slidesUrl: /pdf/talks/motor_representation_shared_intention.slides.pdf
slideImages:
  - /img/talks/motor_representation_shared_intention/slide-000.jpeg
  - /img/talks/motor_representation_shared_intention/slide-001.jpeg
  - /img/talks/motor_representation_shared_intention/slide-002.jpeg
  - /img/talks/motor_representation_shared_intention/slide-003.jpeg
  - /img/talks/motor_representation_shared_intention/slide-004.jpeg
  - /img/talks/motor_representation_shared_intention/slide-005.jpeg
  - /img/talks/motor_representation_shared_intention/slide-006.jpeg
  - /img/talks/motor_representation_shared_intention/slide-007.jpeg
  - /img/talks/motor_representation_shared_intention/slide-008.jpeg
  - /img/talks/motor_representation_shared_intention/slide-009.jpeg
  - /img/talks/motor_representation_shared_intention/slide-010.jpeg
  - /img/talks/motor_representation_shared_intention/slide-011.jpeg
  - /img/talks/motor_representation_shared_intention/slide-012.jpeg
  - /img/talks/motor_representation_shared_intention/slide-013.jpeg
  - /img/talks/motor_representation_shared_intention/slide-014.jpeg
  - /img/talks/motor_representation_shared_intention/slide-015.jpeg
  - /img/talks/motor_representation_shared_intention/slide-016.jpeg
  - /img/talks/motor_representation_shared_intention/slide-017.jpeg
  - /img/talks/motor_representation_shared_intention/slide-018.jpeg
  - /img/talks/motor_representation_shared_intention/slide-019.jpeg
  - /img/talks/motor_representation_shared_intention/slide-020.jpeg
  - /img/talks/motor_representation_shared_intention/slide-021.jpeg
  - /img/talks/motor_representation_shared_intention/slide-022.jpeg
  - /img/talks/motor_representation_shared_intention/slide-023.jpeg
  - /img/talks/motor_representation_shared_intention/slide-024.jpeg
  - /img/talks/motor_representation_shared_intention/slide-025.jpeg
  - /img/talks/motor_representation_shared_intention/slide-026.jpeg
  - /img/talks/motor_representation_shared_intention/slide-027.jpeg
  - /img/talks/motor_representation_shared_intention/slide-028.jpeg
  - /img/talks/motor_representation_shared_intention/slide-029.jpeg
  - /img/talks/motor_representation_shared_intention/slide-030.jpeg
---

## Abstract

On the assumption that motor representation plays a role in explaining how effective joint action is possible, do we also need motor representation to explain what joint action is? Philosophers tend to assume that motor representation is only an enabling condition for joint action and of no direct interest to narrowly philosophical theories of joint action and shared intention. In this talk I shall argue that social motor representation and shared intention have distinctive roles in explaining the purposiveness of joint action. This gives rise to a challenge. On the one hand, effective joint action--imagine two people erecting a tent in a gale together--sometimes requires both shared intentions and social motor representations plus a certain kind of harmony between the two. On the other hand, recognizing their distinctive roles precludes the existence of direct inferential links between shared intentions and social motor representations. The challenge is to explain how these two kinds of representation could sometimes harmoniously contribute to effective joint action despite the lack of inferential integration.
</file>

<file path="src/content/talks/2012/nijmegen_2011.html.md">
---
title: Joint Action and Knowing Others' Minds
authors: Stephen A. Butterfill
pubDate: 2011-01-28T00:00:00.000Z
endDate: 2011-01-29T00:00:00.000Z
event: Mindreading
address: Radbound University Nijmegen
---
</file>

<file path="src/content/talks/2012/paris_2011.html.md">
---
title: Joint Action and the Emergence of Mindreading
authors: Stephen A. Butterfill
pubDate: 2011-11-03T00:00:00.000Z
event: Developmental Aspects of Joint Intentions and Actions
address: Institut Jean-Nicod, Ecole Normale Superieure, Paris
handoutUrl: /pdf/talks/paris_2011.handout.pdf
slidesUrl: /pdf/talks/paris_2011.slides.pdf
slideImages:
  - /img/talks/paris_2011/slide-000.jpeg
  - /img/talks/paris_2011/slide-001.jpeg
  - /img/talks/paris_2011/slide-002.jpeg
  - /img/talks/paris_2011/slide-003.jpeg
  - /img/talks/paris_2011/slide-004.jpeg
  - /img/talks/paris_2011/slide-005.jpeg
  - /img/talks/paris_2011/slide-006.jpeg
  - /img/talks/paris_2011/slide-007.jpeg
  - /img/talks/paris_2011/slide-008.jpeg
  - /img/talks/paris_2011/slide-009.jpeg
  - /img/talks/paris_2011/slide-010.jpeg
  - /img/talks/paris_2011/slide-011.jpeg
  - /img/talks/paris_2011/slide-012.jpeg
  - /img/talks/paris_2011/slide-013.jpeg
  - /img/talks/paris_2011/slide-014.jpeg
  - /img/talks/paris_2011/slide-015.jpeg
  - /img/talks/paris_2011/slide-016.jpeg
  - /img/talks/paris_2011/slide-017.jpeg
  - /img/talks/paris_2011/slide-018.jpeg
  - /img/talks/paris_2011/slide-019.jpeg
  - /img/talks/paris_2011/slide-020.jpeg
  - /img/talks/paris_2011/slide-021.jpeg
  - /img/talks/paris_2011/slide-022.jpeg
  - /img/talks/paris_2011/slide-023.jpeg
  - /img/talks/paris_2011/slide-024.jpeg
  - /img/talks/paris_2011/slide-025.jpeg
  - /img/talks/paris_2011/slide-026.jpeg
  - /img/talks/paris_2011/slide-027.jpeg
  - /img/talks/paris_2011/slide-028.jpeg
  - /img/talks/paris_2011/slide-029.jpeg
  - /img/talks/paris_2011/slide-030.jpeg
  - /img/talks/paris_2011/slide-031.jpeg
  - /img/talks/paris_2011/slide-032.jpeg
  - /img/talks/paris_2011/slide-033.jpeg
  - /img/talks/paris_2011/slide-034.jpeg
  - /img/talks/paris_2011/slide-035.jpeg
  - /img/talks/paris_2011/slide-036.jpeg
  - /img/talks/paris_2011/slide-037.jpeg
  - /img/talks/paris_2011/slide-038.jpeg
  - /img/talks/paris_2011/slide-039.jpeg
  - /img/talks/paris_2011/slide-040.jpeg
  - /img/talks/paris_2011/slide-041.jpeg
  - /img/talks/paris_2011/slide-042.jpeg
  - /img/talks/paris_2011/slide-043.jpeg
  - /img/talks/paris_2011/slide-044.jpeg
  - /img/talks/paris_2011/slide-045.jpeg
  - /img/talks/paris_2011/slide-046.jpeg
  - /img/talks/paris_2011/slide-047.jpeg
  - /img/talks/paris_2011/slide-048.jpeg
  - /img/talks/paris_2011/slide-049.jpeg
  - /img/talks/paris_2011/slide-050.jpeg
  - /img/talks/paris_2011/slide-051.jpeg
  - /img/talks/paris_2011/slide-052.jpeg
  - /img/talks/paris_2011/slide-053.jpeg
  - /img/talks/paris_2011/slide-054.jpeg
  - /img/talks/paris_2011/slide-055.jpeg
  - /img/talks/paris_2011/slide-056.jpeg
  - /img/talks/paris_2011/slide-057.jpeg
  - /img/talks/paris_2011/slide-058.jpeg
  - /img/talks/paris_2011/slide-059.jpeg
  - /img/talks/paris_2011/slide-060.jpeg
  - /img/talks/paris_2011/slide-061.jpeg
  - /img/talks/paris_2011/slide-062.jpeg
  - /img/talks/paris_2011/slide-063.jpeg
  - /img/talks/paris_2011/slide-064.jpeg
  - /img/talks/paris_2011/slide-065.jpeg
  - /img/talks/paris_2011/slide-066.jpeg
  - /img/talks/paris_2011/slide-067.jpeg
  - /img/talks/paris_2011/slide-068.jpeg
  - /img/talks/paris_2011/slide-069.jpeg
  - /img/talks/paris_2011/slide-070.jpeg
  - /img/talks/paris_2011/slide-071.jpeg
  - /img/talks/paris_2011/slide-072.jpeg
  - /img/talks/paris_2011/slide-073.jpeg
  - /img/talks/paris_2011/slide-074.jpeg
  - /img/talks/paris_2011/slide-075.jpeg
  - /img/talks/paris_2011/slide-076.jpeg
  - /img/talks/paris_2011/slide-077.jpeg
  - /img/talks/paris_2011/slide-078.jpeg
  - /img/talks/paris_2011/slide-079.jpeg
  - /img/talks/paris_2011/slide-080.jpeg
  - /img/talks/paris_2011/slide-081.jpeg
  - /img/talks/paris_2011/slide-082.jpeg
  - /img/talks/paris_2011/slide-083.jpeg
  - /img/talks/paris_2011/slide-084.jpeg
  - /img/talks/paris_2011/slide-085.jpeg
  - /img/talks/paris_2011/slide-086.jpeg
  - /img/talks/paris_2011/slide-087.jpeg
  - /img/talks/paris_2011/slide-088.jpeg
  - /img/talks/paris_2011/slide-089.jpeg
  - /img/talks/paris_2011/slide-090.jpeg
  - /img/talks/paris_2011/slide-091.jpeg
  - /img/talks/paris_2011/slide-092.jpeg
  - /img/talks/paris_2011/slide-093.jpeg
  - /img/talks/paris_2011/slide-094.jpeg
  - /img/talks/paris_2011/slide-095.jpeg
  - /img/talks/paris_2011/slide-096.jpeg
---

## Abstract

<p>How can we explain the emergence, in evolution or development, of sophisticated forms of theory of mind cognition?  One conjecture is that their emergence somehow involves joint action (Knoblich & Sebanz, 2006; Hughes & Leekam, 2004; Moll & Tomasello, 2007).  This raises two questions.  First, what forms of theory of mind cognition are required for joint action?  Second, how might abilities to engage in joint action be involved in the emergence of full-blown theory of mind cognition?  This talk will attempt to answer both questions.</p><p>On the first question, it is often held that all joint action involves shared intention.  This is problematic for the conjecture that abilities to engage in joint action partially explain the emergence of theory of mind cognition if, as I argue, shared intention presupposes theory of mind cognition at close to the limits of what humans are capable of.  The problem can be avoided by rejecting the assumption that all joint action involves shared intention.  I shall briefly defend an account of joint action without shared intention.  On this account, joint action presupposes only minimal theory of mind cognition.</p><p>On the second question, I shall explain how abilities to engage in joint action provide a route to knowledge of others' goals distinct from ordinary third-person interpretation.  This allows us to explain how humans are able to break into the Gricean circle and understand communicative intention.  Because communicative intention is a foundation of communication by language, and because communication by language in turn plays a role in the emergence of full-blown theory of mind cognition (Astington & Baird, 2005), this may amount to one (indirect) way in which the combination of joint action with minimal theory of mind cognition partially explains the emergence of full-blown theory of mind cognition.</p>
</file>

<file path="src/content/talks/2012/science_festival_2011.html.md">
---
title: The Problem of Other Minds
authors: Stephen A. Butterfill
pubDate: 2011-09-11T23:00:00.000Z
event: The British Science Festival
address: Bradford
---
</file>

<file path="src/content/talks/2012/shared_agency_and_motor_representation.html.md">
---
title: Shared Agency and Motor Representation
authors: Stephen A. Butterfill
pubDate: 2012-11-20T00:00:00.000Z
address: Department of Philosophy, Central European University, Budapest
handoutUrl: /pdf/talks/shared_agency_and_motor_representation.handout.pdf
slidesUrl: /pdf/talks/shared_agency_and_motor_representation.slides.pdf
slideImages:
  - /img/talks/shared_agency_and_motor_representation/slide-000.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-001.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-002.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-003.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-004.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-005.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-006.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-007.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-008.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-009.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-010.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-011.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-012.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-013.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-014.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-015.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-016.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-017.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-018.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-019.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-020.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-021.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-022.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-023.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-024.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-025.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-026.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-027.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-028.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-029.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-030.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-031.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-032.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-033.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-034.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-035.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-036.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-037.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-038.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-039.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-040.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-041.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-042.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-043.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-044.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-045.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-046.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-047.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-048.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-049.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-050.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-051.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-052.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-053.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-054.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-055.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-056.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-057.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-058.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-059.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-060.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-061.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-062.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-063.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-064.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-065.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-066.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-067.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-068.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-069.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-070.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-071.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-072.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-073.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-074.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-075.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-076.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-077.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-078.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-079.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-080.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-081.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-082.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-083.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-084.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-085.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-086.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-087.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-088.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-089.jpg
  - /img/talks/shared_agency_and_motor_representation/slide-090.jpg
---

## Abstract

Shared agency is paradigmatically involved when two or more people paint a house together, tidy the toys away together, or lift a two-handled basket together.  To characterise shared agency, some philosophers have appealed to a special kind of intention or structure of intention, knowledge or commitment often called `shared intention'.  In this paper we argue that there are forms of shared agency characterising which requires appeal to  motor representation.  Shared agency is not only a matter of what we intend: sometimes it  constitutively involves interlocking structures of motor representation.  This may have consequences for some metaphysical, normative and phenomenological questions about shared agency.
</file>

<file path="src/content/talks/2012/shared_agency_with_parallel_planning.html.md">
---
title: Shared Agency with Parallel Planning
authors: Stephen A. Butterfill
pubDate: 2013-04-01T23:00:00.000Z
endDate: 2013-04-03T23:00:00.000Z
event: Collective Intentionality
address: Center for interdisciplinary Research (ZiF), Bielefeld University
---
</file>

<file path="src/content/talks/2012/stirling_2012.html.md">
---
title: Minimal Theory of Mind and Joint Action
authors: Stephen A. Butterfill
pubDate: 2011-11-17T00:00:00.000Z
address: University of Stirling
---
</file>

<file path="src/content/talks/2012/tuebingen_2011.html.md">
---
title: Which Joint Actions Ground Social Cognition
authors: Stephen A. Butterfill
pubDate: 2011-06-05T23:00:00.000Z
event: Rational Agency
address: Center for Integrative Neuroscience, University of Tuebingen
handoutUrl: /pdf/talks/tuebingen_2011.handout.pdf
slidesUrl: /pdf/talks/tuebingen_2011.slides.pdf
slideImages:
  - /img/talks/tuebingen_2011/slide-000.jpeg
  - /img/talks/tuebingen_2011/slide-001.jpeg
  - /img/talks/tuebingen_2011/slide-002.jpeg
  - /img/talks/tuebingen_2011/slide-003.jpeg
  - /img/talks/tuebingen_2011/slide-004.jpeg
  - /img/talks/tuebingen_2011/slide-005.jpeg
  - /img/talks/tuebingen_2011/slide-006.jpeg
  - /img/talks/tuebingen_2011/slide-007.jpeg
  - /img/talks/tuebingen_2011/slide-008.jpeg
  - /img/talks/tuebingen_2011/slide-009.jpeg
  - /img/talks/tuebingen_2011/slide-010.jpeg
  - /img/talks/tuebingen_2011/slide-011.jpeg
  - /img/talks/tuebingen_2011/slide-012.jpeg
  - /img/talks/tuebingen_2011/slide-013.jpeg
  - /img/talks/tuebingen_2011/slide-014.jpeg
  - /img/talks/tuebingen_2011/slide-015.jpeg
  - /img/talks/tuebingen_2011/slide-016.jpeg
  - /img/talks/tuebingen_2011/slide-017.jpeg
  - /img/talks/tuebingen_2011/slide-018.jpeg
  - /img/talks/tuebingen_2011/slide-019.jpeg
  - /img/talks/tuebingen_2011/slide-020.jpeg
  - /img/talks/tuebingen_2011/slide-021.jpeg
  - /img/talks/tuebingen_2011/slide-022.jpeg
  - /img/talks/tuebingen_2011/slide-023.jpeg
  - /img/talks/tuebingen_2011/slide-024.jpeg
  - /img/talks/tuebingen_2011/slide-025.jpeg
  - /img/talks/tuebingen_2011/slide-026.jpeg
  - /img/talks/tuebingen_2011/slide-027.jpeg
  - /img/talks/tuebingen_2011/slide-028.jpeg
  - /img/talks/tuebingen_2011/slide-029.jpeg
  - /img/talks/tuebingen_2011/slide-030.jpeg
  - /img/talks/tuebingen_2011/slide-031.jpeg
  - /img/talks/tuebingen_2011/slide-032.jpeg
  - /img/talks/tuebingen_2011/slide-033.jpeg
  - /img/talks/tuebingen_2011/slide-034.jpeg
  - /img/talks/tuebingen_2011/slide-035.jpeg
  - /img/talks/tuebingen_2011/slide-036.jpeg
  - /img/talks/tuebingen_2011/slide-037.jpeg
  - /img/talks/tuebingen_2011/slide-038.jpeg
  - /img/talks/tuebingen_2011/slide-039.jpeg
  - /img/talks/tuebingen_2011/slide-040.jpeg
  - /img/talks/tuebingen_2011/slide-041.jpeg
  - /img/talks/tuebingen_2011/slide-042.jpeg
  - /img/talks/tuebingen_2011/slide-043.jpeg
  - /img/talks/tuebingen_2011/slide-044.jpeg
  - /img/talks/tuebingen_2011/slide-045.jpeg
  - /img/talks/tuebingen_2011/slide-046.jpeg
  - /img/talks/tuebingen_2011/slide-047.jpeg
  - /img/talks/tuebingen_2011/slide-048.jpeg
  - /img/talks/tuebingen_2011/slide-049.jpeg
  - /img/talks/tuebingen_2011/slide-050.jpeg
  - /img/talks/tuebingen_2011/slide-051.jpeg
  - /img/talks/tuebingen_2011/slide-052.jpeg
  - /img/talks/tuebingen_2011/slide-053.jpeg
  - /img/talks/tuebingen_2011/slide-054.jpeg
  - /img/talks/tuebingen_2011/slide-055.jpeg
  - /img/talks/tuebingen_2011/slide-056.jpeg
  - /img/talks/tuebingen_2011/slide-057.jpeg
  - /img/talks/tuebingen_2011/slide-058.jpeg
  - /img/talks/tuebingen_2011/slide-059.jpeg
  - /img/talks/tuebingen_2011/slide-060.jpeg
  - /img/talks/tuebingen_2011/slide-061.jpeg
  - /img/talks/tuebingen_2011/slide-062.jpeg
  - /img/talks/tuebingen_2011/slide-063.jpeg
  - /img/talks/tuebingen_2011/slide-064.jpeg
  - /img/talks/tuebingen_2011/slide-065.jpeg
  - /img/talks/tuebingen_2011/slide-066.jpeg
  - /img/talks/tuebingen_2011/slide-067.jpeg
  - /img/talks/tuebingen_2011/slide-068.jpeg
  - /img/talks/tuebingen_2011/slide-069.jpeg
  - /img/talks/tuebingen_2011/slide-070.jpeg
  - /img/talks/tuebingen_2011/slide-071.jpeg
  - /img/talks/tuebingen_2011/slide-072.jpeg
  - /img/talks/tuebingen_2011/slide-073.jpeg
  - /img/talks/tuebingen_2011/slide-074.jpeg
  - /img/talks/tuebingen_2011/slide-075.jpeg
  - /img/talks/tuebingen_2011/slide-076.jpeg
  - /img/talks/tuebingen_2011/slide-077.jpeg
  - /img/talks/tuebingen_2011/slide-078.jpeg
  - /img/talks/tuebingen_2011/slide-079.jpeg
  - /img/talks/tuebingen_2011/slide-080.jpeg
  - /img/talks/tuebingen_2011/slide-081.jpeg
---

## Abstract

<p>Several researchers have conjectured that joint action partially explains how sophisticated* forms of cognition emerge in development or evolution (for example, Knoblich & Sebanz 2006; Moll & Tomasello 2007).  If any such conjecture is correct, what could joint action be? </p><p>The leading approach to characterising joint action focuses on shared intention (Bratman 1993, 2009).  A shared intention is something that stands to a joint action roughly as an ordinary intention stands to an individual action.  Given some widely held claims about rationality and knowledge, shared intention requires knowledge of intentions about intentions (Butterfill forthcoming).  So shared intention already presupposes sophisticated* (social) cognition.  If the conjectures about evolution and development are true, not all significant cases of joint action can involve shared intention: if they did, abilities to engage in joint action would presuppose sophisticated* forms of cognition and so could not explain how they are acquired in evolution or development.</p><p>An alternative approach to joint action starts from the claim, motivated by semantic considerations, that a joint action is an action with two or more agents (Ludwig 2007).  An immediate objection to this claim is that, given any of several widely held views about action, events standardly offered as paradigm joint actions turn out not to be joint actions at all.  This objection can be overcome by refining the claim that a joint action is an action with two or more agents.  This alternative approach is necessary for characterising the joint actions that could explain how sophisticated* forms of cognition emerge in development or evolution.</p><p>* - yes, I do realise that the term 'sophisticated' isn't very scientific; the talk will be more specific.</p>
</file>

<file path="src/content/talks/2013/collective_agency_oxford.html.md">
---
title: Collective Agency and Knowledge of Others’ Minds
authors: Stephen A. Butterfill
pubDate: 2013-08-05T23:00:00.000Z
endDate: 2013-08-06T23:00:00.000Z
event: Aristotelian and Contemporary Perspectives on the Mind
address: University of Oxford
---

## Abstract

When friends walk together, they typically exercise collective agency.  By contrast, two strangers walking side by side exercise parallel but merely individual agency.  This and other contrasts invite the question, What distinguishes collective agency from parallel but merely individual agency?  To answer this question, philosophers standardly appeal to a special kind of intention or structure of intention, knowledge or commitment often called 'collective intention'.  The idea is that exercises of collective agency stand to collective intention much as exercises of ordinary, individual agency stand to ordinary, individual intention.  In this talk I shall use this parallel between individual and collective intention to argue that some forms of collective agency are grounded in representations and processes more primitive than those associated with collective intention.  Collective agency is not always a matter of what we intend: sometimes it  constitutively involves certain structures of motor representation.  One consequence is concerns a role for collective agency in explaining knowledge of others' minds.  Reflection on what is involved in sharing a smile suggests that there is a route to knowledge of others' mental states that is neither straightforwardly perceptual nor inferential but hinges on interaction.
</file>

<file path="src/content/talks/2013/collective_intentionality_vienna.html.md">
---
title: Collective Intentionality and Social Intelligence
authors: Stephen A. Butterfill
pubDate: 2013-12-12T00:00:00.000Z
address: University of Vienna, Austria
---
</file>

<file path="src/content/talks/2013/metacognition_and_mindreading.html.md">
---
title: Monitoring and Controlling the Mental States of Others
authors: Stephen A. Butterfill and Ian A. Apperly
pubDate: 2013-01-24T00:00:00.000Z
event: All Souls Metacognition Seminars
address: University of Oxford
handoutUrl: /pdf/talks/metacognition_and_mindreading.handout.pdf
slidesUrl: /pdf/talks/metacognition_and_mindreading.slides.pdf
slideImages:
  - /img/talks/metacognition_and_mindreading/slide-000.jpeg
  - /img/talks/metacognition_and_mindreading/slide-001.jpeg
  - /img/talks/metacognition_and_mindreading/slide-002.jpeg
  - /img/talks/metacognition_and_mindreading/slide-003.jpeg
  - /img/talks/metacognition_and_mindreading/slide-004.jpeg
  - /img/talks/metacognition_and_mindreading/slide-005.jpeg
  - /img/talks/metacognition_and_mindreading/slide-006.jpeg
  - /img/talks/metacognition_and_mindreading/slide-007.jpeg
  - /img/talks/metacognition_and_mindreading/slide-008.jpeg
  - /img/talks/metacognition_and_mindreading/slide-009.jpeg
  - /img/talks/metacognition_and_mindreading/slide-010.jpeg
  - /img/talks/metacognition_and_mindreading/slide-011.jpeg
  - /img/talks/metacognition_and_mindreading/slide-012.jpeg
  - /img/talks/metacognition_and_mindreading/slide-013.jpeg
  - /img/talks/metacognition_and_mindreading/slide-014.jpeg
  - /img/talks/metacognition_and_mindreading/slide-015.jpeg
  - /img/talks/metacognition_and_mindreading/slide-016.jpeg
  - /img/talks/metacognition_and_mindreading/slide-017.jpeg
  - /img/talks/metacognition_and_mindreading/slide-018.jpeg
  - /img/talks/metacognition_and_mindreading/slide-019.jpeg
  - /img/talks/metacognition_and_mindreading/slide-020.jpeg
  - /img/talks/metacognition_and_mindreading/slide-021.jpeg
  - /img/talks/metacognition_and_mindreading/slide-022.jpeg
  - /img/talks/metacognition_and_mindreading/slide-023.jpeg
  - /img/talks/metacognition_and_mindreading/slide-024.jpeg
  - /img/talks/metacognition_and_mindreading/slide-025.jpeg
  - /img/talks/metacognition_and_mindreading/slide-026.jpeg
  - /img/talks/metacognition_and_mindreading/slide-027.jpeg
  - /img/talks/metacognition_and_mindreading/slide-028.jpeg
  - /img/talks/metacognition_and_mindreading/slide-029.jpeg
  - /img/talks/metacognition_and_mindreading/slide-030.jpeg
  - /img/talks/metacognition_and_mindreading/slide-031.jpeg
  - /img/talks/metacognition_and_mindreading/slide-032.jpeg
  - /img/talks/metacognition_and_mindreading/slide-033.jpeg
  - /img/talks/metacognition_and_mindreading/slide-034.jpeg
  - /img/talks/metacognition_and_mindreading/slide-035.jpeg
  - /img/talks/metacognition_and_mindreading/slide-036.jpeg
  - /img/talks/metacognition_and_mindreading/slide-037.jpeg
  - /img/talks/metacognition_and_mindreading/slide-038.jpeg
  - /img/talks/metacognition_and_mindreading/slide-039.jpeg
  - /img/talks/metacognition_and_mindreading/slide-040.jpeg
  - /img/talks/metacognition_and_mindreading/slide-041.jpeg
  - /img/talks/metacognition_and_mindreading/slide-042.jpeg
  - /img/talks/metacognition_and_mindreading/slide-043.jpeg
  - /img/talks/metacognition_and_mindreading/slide-044.jpeg
  - /img/talks/metacognition_and_mindreading/slide-045.jpeg
  - /img/talks/metacognition_and_mindreading/slide-046.jpeg
  - /img/talks/metacognition_and_mindreading/slide-047.jpeg
  - /img/talks/metacognition_and_mindreading/slide-048.jpeg
  - /img/talks/metacognition_and_mindreading/slide-049.jpeg
---

## Abstract

The success of infants and nonhuman animals on some belief reasoning tasks may be best explained by a cognitively efficient but inflexible capacity for tracking belief-like states. In humans, this capacity persists in parallel with a later-developing, more flexible but more cognitively demanding theory-of-mind abilities. The later-developing ability clearly involves meta-cognition, both because it involves thinking about thoughts as such, and because it involves at least some conscious, controlled processes. In contrast the early-developing capacity may not be conscious or controlled, and may not involve thinking about thoughts as such. We will consider how these characteristics may explain the efficiency of the early-developing capacity, and whether this means the capacity is meta-cognitive in any meaningful sense.
</file>

<file path="src/content/talks/2013/motor_representation_in_joint_action.html.md">
---
title: Motor Representation in Joint Action
authors: Stephen A. Butterfill
pubDate: 2013-10-28T00:00:00.000Z
endDate: 2013-10-30T00:00:00.000Z
event: Vision, Action and Concepts
address: University of Lille, France
---

## Abstract

What do recent findings about mechanisms for interpersonal coordination reveal about the nature of joint action?  Joint action paradigmatically occurs when two or more people paint a house together, tidy the toys away together, or lift a two-handled basket.  Standard accounts of joint action invoke a special kind of intention or structure of intention, knowledge or commitment often called 'shared intention'.  Arguably, our having a shared intention requires that our plans interlock, which in turn requires that we represent each others' plans.  So, on the standard view, joint action involves representing not only what is to be done but also another's plans for doing it.  In this talk I shall explain how recent findings on mechanisms for interpersonal coordination motivate an alternative view, one that appeals to abilities to act instead of abilities to reflect on others' plans.  Rather than representing each other's plans, it is sometimes sufficient for joint action that we plan each other's actions.  (This may initially appear incoherent; I shall show that it isn't.)  Perhaps, then, there are forms of joint action characterising which requires appeal to motor representation.  Joint action is not only a matter of what we intend: sometimes it constitutively involves certain structures of motor representation.
</file>

<file path="src/content/talks/2013/not_just_wide_but_shared.html.md">
---
title: "Not Just Wide but Shared: Joint Action Is a Core Form of Social Intelligence"
authors: Stephen A. Butterfill
pubDate: 2013-08-18T23:00:00.000Z
endDate: 2013-08-22T23:00:00.000Z
event: Wide Cognition and Social Intelligence
address: Kazimierz Dolny, Poland
---
</file>

<file path="src/content/talks/2013/perceiving_anger_and_sharing_smiles.html.md">
---
title: "Perceiving Anger and Sharing Smiles: The Roles of Perception and Social
  Interaction in Acquiring Knowledge of Others’ Mental States"
authors: Stephen A. Butterfill
pubDate: 2013-09-12T23:00:00.000Z
event: Emotion and Social Cognition
address: University of Manchester
---

## Abstract

How could we come to know that another is angry about an obstacle she has encountered, or that she is happy about an outcome?  Some such knowledge is uncontroversially acquired by inference.  However, some philosophers have recently defended the hypothesis that knowledge of others' anger or joy might be acquired by perceiving it (e.g. Smith, 2010; McNeill, 2012a,b).  In the first part of this talk I offer a challenge to the view that humans can perceive anger or joy in the same sense that they can perceive shapes or textures.  The challenge is to specify a model of anger and other mental states which captures how these mental states appear to the perceivers.  This is a hard challenge to meet because available models arguably render mental states imperceptible.  In the second part of the talk I propose an alternative way in which knowledge of others' mental states could be acquired, one that hinges on interaction rather than perception or inference.  Reflection on what is involved in sharing a smile suggests that one route to knowledge of others' mental states might involve interacting with them.
</file>

<file path="src/content/talks/2013/planning_for_collective_agency_stuttgart.html.md">
---
title: Planning for Collective Agency
authors: Stephen A. Butterfill
pubDate: 2013-07-21T23:00:00.000Z
endDate: 2013-07-23T23:00:00.000Z
event: Collective Agency and Cooperation in Natural and Artificial Systems
address: University of Stuttgart, Germany
---
</file>

<file path="src/content/talks/2013/planning_for_collective_agency_tuebingen.html.md">
---
title: Planning for Collective Agency
authors: Stephen A. Butterfill
pubDate: 2013-07-23T23:00:00.000Z
endDate: 2013-07-23T23:00:00.000Z
address: University of Tuebingen, Germany
---

## Abstract

What does exercising collective agency require when our acting collectively is intentional?  On the leading, best developed account, Michael Bratman's, intentional collective agency requires shared intention and shared intention is explained in terms of interconnected planning.  For our plans to be interconnected is for them to concern not just facts about our environment and goals but also facts about each others' plans.  In this talk I shall argue that interconnected planning is neither sufficient nor necessary for intentional collective agency.  I shall also defend the possibility that parallel planning might underpin intentional collective agency.  What matters is not whether our plans are interconnected in the sense that they include facts about each others' plans: in some or all cases, what matters for intentional collective agency is rather that we each individually, in parallel plan all of our actions and so conceive of our own and each other's actions as parts of a single plan.  This may have consequences for understanding the roles of motor cognition in collective agency, as well as for modelling and simulating collective agency.
</file>

<file path="src/content/talks/2013/two_systems_two_theories.html.md">
---
title: Two Systems and Two Theories of Mind
authors: Ian Apperly and Stephen A. Butterfill
pubDate: 2013-01-29T00:00:00.000Z
event: "Theory of Mind, Simulation and Meta-Cognition: Laureate's Colloquium
  with Josef Perner"
address: Center for interdisciplinary Research (ZiF), Bielefeld University
handoutUrl: /pdf/talks/two_systems_two_theories.handout.pdf
slidesUrl: /pdf/talks/two_systems_two_theories.slides.pdf
slideImages:
  - /img/talks/two_systems_two_theories/slide-000.jpeg
  - /img/talks/two_systems_two_theories/slide-001.jpeg
  - /img/talks/two_systems_two_theories/slide-002.jpeg
  - /img/talks/two_systems_two_theories/slide-003.jpeg
  - /img/talks/two_systems_two_theories/slide-004.jpeg
  - /img/talks/two_systems_two_theories/slide-005.jpeg
  - /img/talks/two_systems_two_theories/slide-006.jpeg
  - /img/talks/two_systems_two_theories/slide-007.jpeg
  - /img/talks/two_systems_two_theories/slide-008.jpeg
  - /img/talks/two_systems_two_theories/slide-009.jpeg
  - /img/talks/two_systems_two_theories/slide-010.jpeg
  - /img/talks/two_systems_two_theories/slide-011.jpeg
  - /img/talks/two_systems_two_theories/slide-012.jpeg
  - /img/talks/two_systems_two_theories/slide-013.jpeg
  - /img/talks/two_systems_two_theories/slide-014.jpeg
  - /img/talks/two_systems_two_theories/slide-015.jpeg
  - /img/talks/two_systems_two_theories/slide-016.jpeg
  - /img/talks/two_systems_two_theories/slide-017.jpeg
  - /img/talks/two_systems_two_theories/slide-018.jpeg
  - /img/talks/two_systems_two_theories/slide-019.jpeg
  - /img/talks/two_systems_two_theories/slide-020.jpeg
  - /img/talks/two_systems_two_theories/slide-021.jpeg
  - /img/talks/two_systems_two_theories/slide-022.jpeg
  - /img/talks/two_systems_two_theories/slide-023.jpeg
  - /img/talks/two_systems_two_theories/slide-024.jpeg
  - /img/talks/two_systems_two_theories/slide-025.jpeg
  - /img/talks/two_systems_two_theories/slide-026.jpeg
  - /img/talks/two_systems_two_theories/slide-027.jpeg
  - /img/talks/two_systems_two_theories/slide-028.jpeg
  - /img/talks/two_systems_two_theories/slide-029.jpeg
  - /img/talks/two_systems_two_theories/slide-030.jpeg
  - /img/talks/two_systems_two_theories/slide-031.jpeg
  - /img/talks/two_systems_two_theories/slide-032.jpeg
  - /img/talks/two_systems_two_theories/slide-033.jpeg
  - /img/talks/two_systems_two_theories/slide-034.jpeg
  - /img/talks/two_systems_two_theories/slide-035.jpeg
  - /img/talks/two_systems_two_theories/slide-036.jpeg
  - /img/talks/two_systems_two_theories/slide-037.jpeg
  - /img/talks/two_systems_two_theories/slide-038.jpeg
  - /img/talks/two_systems_two_theories/slide-039.jpeg
  - /img/talks/two_systems_two_theories/slide-040.jpeg
  - /img/talks/two_systems_two_theories/slide-041.jpeg
  - /img/talks/two_systems_two_theories/slide-042.jpeg
---

## Abstract

We argue that mindreading involves not only multiple systems but also multiple models (or theories) of the mind.  The argument turns on three questions.  First, how could mindreading be both flexible and efficient?  We suggest that, in a sense it can't.  Instead mindreading involves two or more systems; some trade efficiency for flexibility and others make the converse trade-off.  But how could mindreading--which paradigmatically involves constructing reason-giving, causal explanations for actions by appeal to mental states with arbitrarily nestable contents and uncodifiably complex functional roles--ever be efficient?  One possibility is that mindreading sometimes involves unsophistcated but useful models of the mind.  If this is right, we face a third question.  How can hypotheses about which model of the mind a mindreader is using be tested?   Unsophisticated models have signature limits, and these limits may make it possible to identify the operation of a given model across contexts and across types of subject.
</file>

<file path="src/content/talks/2013/varieties_of_joint_action.html.md">
---
title: Varieties of Joint Action
authors: Stephen A. Butterfill
pubDate: 2013-12-02T00:00:00.000Z
endDate: 2013-12-03T00:00:00.000Z
event: Varieties of Shared Intentionality
address: Institute of Philosophy, London
---
</file>

<file path="src/content/talks/2014/joint_action_warwick.html.md">
---
title: Joint Action
authors: Stephen A. Butterfill
pubDate: 2014-02-28T00:00:00.000Z
address: Department of Philosophy, University of Warwick
handoutUrl: /pdf/talks/joint_action_warwick.handout.pdf
---
</file>

<file path="src/content/talks/2014/joint_action_without_mindreading.html.md">
---
title: Joint Action without Mindreading
authors: Stephen A. Butterfill
pubDate: 2014-04-22T23:00:00.000Z
address: Berlin School of Mind and Brain, Humboldt Universität zu Berlin, Germany
handoutUrl: /pdf/talks/joint_action_without_mindreading.handout.pdf
---

## Abstract

Joint action is a familiar feature of everyday life.  Paradigmatic joint actions include things like painting a house together, walking together and playing a piano duet.  On leading accounts of joint action, engaging in joint action presupposes mindreading abilities.  But are there forms of joint action engaging in which would not require mindreading?  In this talk I aim to identify interagential structures of motor representation that play a role in coordinating some joint actions, and to argue that this role is parallel to one played by shared intention.  The parallel indicates that we can give a theoretically coherent and empirically motivated account of a form of joint action that does not require mindreading.  This  removes an obstacle to holding that abilities to engage in joint action partly explain the emergence, in evolution or development, of mindreading.
</file>

<file path="src/content/talks/2014/minimal_models_magdeburg.html.md">
---
title: "Minimal Models of the Physical and of the Mental: Processes,
  Representations and  Signature Limits"
authors: Stephen A. Butterfill
pubDate: 2014-11-07T00:00:00.000Z
event: Minimal Mindreading
address: University of Magdeburg, Germany
handoutUrl: /pdf/talks/minimal_models_magdeburg.handout.pdf
---

## Abstract

This talk introduces a series of hypotheses and claims about the systems and models involved in mindreading (or ‘theory of mind’), explains how they generate predictions by drawing a parallel with cognition of physical properties like momentum, and highlights some recent evidence indicating that some of the hypotheses may be correct.
</file>

<file path="src/content/talks/2014/naturalising_joint_actions.html.md">
---
title: Naturalising Joint Actions
authors: Stephen A. Butterfill
pubDate: 2014-12-13T00:00:00.000Z
event: Naturalising Action
address: University of Tuebingen, Germany
---

## Abstract

Joint action is a familiar feature of everyday life. Paradigmatic cases include moving objects together, walking together and playing piano duets. Many joint actions comprise two or more actions involving multiple agents which are purposive in this sense:  among all of their actual and possible outcomes, there is one or more to which the actions are collectively directed. One challenge for an account of joint action is to provide a framework for explaining in virtue of what actions involving multiple agents are collectively  directed to outcomes. A familiar way to meet this challenge is by appeal to one or another notion of shared intention. In this paper we show that more is needed to fully meet the challenge. In some cases, actions involving multiple agents are collectively directed to an outcome not in virtue of any kind of shared intention but in virtue of a certain interagential structure of motor representations. Further, some joint actions are collectively directed to outcomes partly in virtue of this interagential structure of motor representations. This suggests that the building blocks needed for constructing an account of joint action may include not only shared intention but also a certain interagential structure of motor representations.  We conclude by discussing some consequences for accounts of shared intention.
</file>

<file path="src/content/talks/2014/shared_agency_involves_changing_perspective_manchester.html.md">
---
title: Shared Agency Involves Changing Perspective
authors: Stephen A. Butterfill
pubDate: 2014-02-19T00:00:00.000Z
address: Department of Philosophy, University of Manchester
handoutUrl: /pdf/talks/shared_agency_involves_changing_perspective_manchester.handout.pdf
---
</file>

<file path="src/content/talks/2015/antwerp_2015.html.md">
---
title: Acting Together & Acting as One
authors: Stephen A. Butterfill
pubDate: 2015-09-22T23:00:00.000Z
event: Desire and Action
address: University of Antwerp
---
</file>

<file path="src/content/talks/2015/berlin_2015_core_knowledge.html.md">
---
title: Only Phenomenal Expectations Connect Core Knowledge of Objects to Thought
authors: Stephen A. Butterfill
pubDate: 2015-06-17T23:00:00.000Z
event: The Nature and Origins of Human Cognition
address: Berlin School of Mind and Brain, Humboldt Universität zu Berlin, Germany
handoutUrl: /pdf/talks/berlin_2015_core_knowledge.handout.pdf
---

## Abstract

Infants have core knowledge of objects, causes, numbers, actions and much else besides (Spelke, 2003; Carey, 2009).  But what is core knowledge?  There are challenges which apply to the leading theoretical accounts (compare Butterfill, 2007; Keren and Schul, 2009; Adolphs, 2010).  A way of responding to these challenges exists for the case of core knowledge of objects, however: several researchers have conjectured that infants’ core knowledge consists in a system of object indexes (Leslie et al. 1998; Scholl and Leslie 1999; Carey and Xu 2001; Scholl 2007). One consequence of this conjecture is the existence of an interface problem.  The representations and processes which comprise the workings of object indexes have only limited influences on thought and action.  How then could core knowledge play a role in explaining the emergence, in development, of knowledge concerning physical objects and their interactions?  One possibility hinges on the notion of a phenomenal expectation, which is approximately a sensation in Reid’s sense (Reid, 1785a,b). You have a phenomenal expectation concerning an object’s movements where these are unperceived in the most straightforward sense (because they are occluded, for example) and yet your overall experience is not neutral concerning the object’s movements either.  Perhaps the transition from core knowledge to knowledge proper has such a protracted developmental course because only phenomenal expectations connect core knowledge of objects to thoughts about objects.
</file>

<file path="src/content/talks/2015/bochum_2015.html.md">
---
title: "Acting Together: Motor Representation and Cooperation"
authors: Stephen A. Butterfill
pubDate: 2015-11-11T00:00:00.000Z
event: Kolloquium des Instituts für Philosophie II
address: Ruhr-Universität Bochum, Germany
handoutUrl: /pdf/talks/bochum_2015.handout.pdf
---
</file>

<file path="src/content/talks/2015/bupdapest_moseo.html.md">
---
title: Commentary on Wolfgang Prinz and Michael Graziano
authors: Stephen A. Butterfill
pubDate: 2015-05-18T23:00:00.000Z
event: Modelling Self on Other
address: Department of Cognitive Science, Central European University, Budapest, Hungary
---
</file>

<file path="src/content/talks/2015/copenhagen_2015.html.md">
---
title: Acting Together and Acting As One
authors: Stephen A. Butterfill
pubDate: 2015-05-21T23:00:00.000Z
event: Empathy, Group Membership and We-Intentionality
address: Center for Subjectivity Research, University of Copenhagen, Denmark
---
</file>

<file path="src/content/talks/2015/mindreading_espp.html.md">
---
title: How Do Mindreaders Model Minds?
authors: Stephen A. Butterfill
pubDate: 2015-07-14T23:00:00.000Z
event: Mindreading, an invited symposium at the European Soceity for Philosophy
  and Psychology (ESPP)
address: University of Tartu, Estonia
---

## Abstract

"What it is to be a mindreader?
Mindreading involves identifying mental states; this requires having some model of the mental, much as identifying physical states requires having some model of the physical.  
And philosophical failures to characterise minds reveal that there are multiple models of the mental, much as 
the history of science reveals that there are multiple models of the physical.  
To say that someone is a mindreader therefore leaves open the question of which model of the mental she is using.  
Just as humans use multiple models of the physical (an expert physicist will probably leave quantum mechanics behind when putting up a garden fence in favour of a model she can more efficiently deploy), so it is likely that they use multiple models of the mental.  
This may matter for understanding why mindreading is sometimes but not always automatic—some models are relatively easy to acquire or apply but are limited in accuracy, others are harder to acquire and use but also more accurate.  
But how can we test hypotheses about which model is used in a particular task?  As in the physical case, different conjectures about models of the mental can be distinguished because different models have different signature limits.
In particular, one class of models, minimal models of the mental, have signature limits involving identity.  
Recent scientific discoveries indicate that these limits can be used to confirm conjectures about how infant and adult mindreaders model minds.
</file>

<file path="src/content/talks/2015/mindreading_srcd.html.md">
---
title: Systems, Models and Signature Limits
authors: Stephen A. Butterfill
pubDate: 2015-03-20T00:00:00.000Z
event: "Signature limits in implicit theory of mind: Evidence for two systems of
  mindreading?  Workshop at the Society for Research in Child Development"
address: Philadelphia, Pennsylvania, USA
---

## Abstract

"Two puzzles about mindreading, and in particular the nature of belief ascription, require resolution.  Can infants ascribe false beliefs in their first or second year of life?  Some measures indicate that they can, others that they cannot.  Is belief ascription automatic?  Some findings suggest that it is, others that it is not.  Reflection on these puzzling patterns of findings suggests that we should step back to ask, What it is to ascribe beliefs?  More generally, What it is to be a mindreader?  

Mindreading involves representing mental states; this requires having some model of the mental, much as representing physical states requires having some model of the physical.  The history of science reveals that there are multiple models of the physical.  Some models are relatively easy to acquire or apply but are limited in accuracy, others are harder to acquire and use but also more accurate.  The first part of this talk will show that there are also multiple models of the mental.  To say that someone represents beliefs or other mental states leaves open the question of which model of the mental she is using.  

Just as humans use multiple models of the physical (an expert physicist will probably leave quantum mechanics behind when putting up a garden fence in favour of a model she can more efficiently deploy), so it is likely that they use multiple models of the mental.  But how can we distinguish among hypotheses about which model is used in a particular task?  In the physical case, such hypotheses can be distinguished by identifying signature limits.  To illustrate, impetus mechanics makes incorrect predictions about certain trajectories.  If a certain group of individuals use impetus mechanics on a particular task, they should make incorrect predictions about these trajectories.  Testing whether they make such predictions can therefore yield evidence about which model of the physical they are using.  The second part of this talk will show that certain models of the mental have signature limits.  In particular, simpler models of the mental are limited in making incorrect predictions when beliefs essentially involve identity.  Other talks in this symposium provide evidence that, in certain cases, mindreading exhibits this signature limit.

A conjecture about how mindreaders variously model minds does not completely solve the puzzles about the automaticity and development of mindreading.  The third part of this talk concerns systems; that is, the different ways in which a model of the mind can be implemented cognitively in an actual mindreader.  It is a familiar idea that different models of the physical are implemented in different cognitive systems; representing physical states can involve core systems or modules, for example.  Representing mental states may similarly involve multiple systems.  Conjectures about multiple systems are linked to conjectures about the models they implement.  If mindreading involves multiple systems which implement multiple models of the mind, we can discover this thanks to relations between systems and models and their signature limits.
</file>

<file path="src/content/talks/2015/practical_reasoning_motor_representation_intro.html.md">
---
title: Introduction to the Workshop on Practical Reasoning and Motor Representation
authors: Stephen A. Butterfill and Corrado Sinigaglia
pubDate: 2015-05-31T23:00:00.000Z
event: Practical Reasoning and Motor Representation
address: University of Warwick, UK
slidesUrl: /pdf/talks/practical_reasoning_motor_representation_intro.slides.pdf
---
</file>

<file path="src/content/talks/2015/purposive_action_and_intention.html.md">
---
title: Purposive Action from Motor Representation to Intention
authors: Stephen A. Butterfill
pubDate: 2015-11-12T00:00:00.000Z
event: How much mind do we need for responsibility?
address: Center for Interdisciplinary Research (ZiF), Bielefeld University, Germany
---
</file>

<file path="src/content/talks/2016/berlin_2016_2systems.md">
---
title: How to Distinguish Two (Or More) Systems for Social Cognition
authors: Stephen A. Butterfill
pubDate: 2016-03-30T23:00:00.000Z
event: The Nature and Origins of Human Cognition
address: Berlin School of Mind and Brain, Humboldt Universität zu Berlin, Germany
---

## Abstract

Suppose you observe two exercises of competence in social cognition and want to know 
whether the systems underpinning the exercises are the same or distinct.  How can you tell?  An immediate obstacle is theoretical: there are many attempts to characterise ‘system’, each only slightly different from the next, and no obvious principle for deciding between competing characterisations.  You therefore cannot rely on any one theory.  Despite this obstacle, it seems to be possible to formulate hypotheses about the distinctness (or identity) of systems which generate testable predictions.  But how?  This talk aims to extract a recipe for distinguishing (or identifying) systems from research on understanding speech, action, emotion and belief.
</file>

<file path="src/content/talks/2016/milan_2016_core_knowledge.html.md">
---
title: Core Knowledge, Phenomenal Expectations and Thought
authors: Stephen A. Butterfill
pubDate: 2016-02-18T00:00:00.000Z
event: New Directions in Philosophical Psychlogy
address: Università Cattolica del Sacro Cuore, Milan, Italy
---

## Abstract

How is core knowledge linked to full-blown thought and intentional action?  Infants in the first months of life have core knowledge of objects, causes, numbers, actions and much else besides (Spelke, 2003; Carey, 2009).  But what is core knowledge?  There are challenges which apply to the leading theoretical accounts (compare Butterfill, 2007; Keren and Schul, 2009; Adolphs, 2010).  One way of responding to these challenges exists for the case of core knowledge of objects, however: several researchers have conjectured that infants’ core knowledge consists in a system of object indexes (Leslie et al. 1998; Scholl and Leslie 1999; Carey and Xu 2001; Scholl 2007). Accepting this conjecture does enable us to answer challenges to the theoretical coherence of postulating core knowledge, but it also means we are confronted with a further challenge about the interface between core knowledge and thought.  The representations and processes which comprise the workings of object indexes have only limited influences on thought and action.  How then could core knowledge play a role in explaining the emergence, in development, of full-blown thought about, and intentional action on, physical objects?  I propose that one possibility hinges on the notion of a phenomenal expectation, which is approximately a sensation in Reid’s sense (Reid, 1785a,b). In general, an aspect of the overall phenomenal character of an experience is a phenomenal expectation just if its subject routinely and unthinkingly takes it to be informative about things that are only distantly related (if at all) to what experiences associated with this aspect of phenomenal character intentionally relate the subject to.  Phenomenal expectations can provide a low-cost, efficient link between core knowledge and full-blown thought and intentional action.  Perhaps the transition from core knowledge to knowledge proper has such a protracted developmental course because only phenomenal expectations connect core knowledge of objects to thoughts about objects.
</file>

<file path="src/content/talks/2016/perceiving_mental_states_tuebingen.html.md">
---
title: Perceiving Mental States
authors: Stephen A. Butterfill
pubDate: 2015-12-16T00:00:00.000Z
endDate: 2015-12-24T00:00:00.000Z
address: University of Tuebingen, Germany
---

## Abstract

How could we come to know that another is angry about an insult or happy about an outcome?  Some philosophers have defended the hypothesis that knowledge of others' anger or joy is sometimes perceptual (e.g. Smith, 2010, 2015; McNeill, 2012a,b).  In this talk I consider two challenges to this hypothesis.  The first challenge is to identify evidence in favour of it, or at least to explain what evidence could bear on the hypothesis.  Perhaps this challenge can be met by considering categorical perception of expressions of emotion, for which there is some evidence. The second challenge is to specify a model of anger and other mental states which captures how these mental states appear to the perceivers.  This is a hard challenge to meet because available models arguably render mental states imperceptible.  In the last part of the talk I propose an alternative way in which knowledge of others' mental states could be acquired, one that hinges on interaction rather than perception.  Reflection on what is involved in sharing a smile suggests that one route to knowledge of others’ mental states might involve interacting with them.
</file>

<file path="src/content/teaching/joint_action_and_the_emergence/lecture01.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/joint_action_and_the_emergence/lecture02.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/joint_action_and_the_emergence/lecture02b.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/joint_action_and_the_emergence/lecture03.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/joint_action_and_the_emergence/lecture04.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/joint_action_and_the_emergence/lecture05.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture01.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture02.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture03.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture04.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture05.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture06.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture07.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture08.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action/lecture09.html.md">
---
title: Untitled
authors: Unknown
---
</file>

<file path="src/content/teaching/joint_action_and_the_emergence.html.md">
---
title: Joint Action and the Emergence of Mindreading
authors: Stephen A. Butterfill
year: 2011-2
term: Summer
place: Department of Philosophy, University of Warwick
lectures:
  "01": "Joint Action and Mindreading: Some Problems"
  "02": Minimal Theory of Mind
  02b: What Are Modules and What Is Their Role in Development?
  "03": Which Joint Actions Ground Social Cognition?
  "04": Intention and Motor Representation in Joint Action
  "05": Interacting Mindreaders
abstract: "How can we explain the emergence, in evolution or development, of
  mindreading? Some conjecture that its emergence involves joint action
  (Knoblich & Sebanz, 2006; Moll & Tomasello, 2007). Reflection on objections to
  this conjecture reveals mistakes in leading philosophical accounts of both
  mindreading and joint action. These lectures aim to identify the mistakes and
  provide fixes. The fixes involve two steps: the construction of a minimal
  theory of mind; and an account of the distinct roles for shared intention and
  social motor representation in explaining what joint action is."
---
</file>

<file path="src/content/teaching/mindreading_and_joint_action.html.md">
---
title: "Mindreading and Joint Action: Philosophical Tools"
authors: Stephen A. Butterfill
year: 2011-2
term: Autumn
place: Department of Cognitive Science, Central European University, Budapest
lectures:
  "01": Introduction
  "02": What Are Mental States?
  "03": Tracking, Measuring and Representing Beliefs
  "04": What is Core Knowledge (or Modularity)?
  "05": Actions, Intentions and Goals
  "06": "Goal Ascription: the Teleological Stance and Motor Awareness"
  "07": What Is Joint Action?
  "08": Shared Intention and Motor Representation in Joint Action
  "09": Interacting Mindreaders
abstract: This course will introduce a variety of new and established
  philosophical ideas that might usefully inform experimental research on
  mindreading or on joint action (or both) but have so far been neglected or
  misunderstood by cognitive scientists. Starting from foundational questions
  like <i>What is a mental state?</i> and <i>Which events are actions?</i> we
  shall search for tools that might help us with two tasks. First, we need
  theoretically coherent and empirically motivated ways of distinguishing kinds
  of mindreading, and kinds of joint action. Second, we need ways of decomposing
  mindreading in something like the way that actual reading can be decomposed
  into orthographic, lexical, syntactic, semantic and pragmatic components.
---
</file>

<file path="src/content/writing/awareness_of_belief.html.md">
---
title: Awareness of Belief
authors: Stephen A. Butterfill
year: 2001
isForthcoming: false
booktitle: Argument & Analyse, vol. 2
pdfUrl: /pdf/awareness_of_belief.pdf
---

## Abstract

Is it possible to appreciate how different people may have beliefs different from one's own (including beliefs which are inconsistent with one's own beliefs) without understanding what it is for beliefs to be true or false?  In this paper I argue that it is.
</file>

<file path="src/content/writing/blueprint_social_animal.html.md">
---
title: Towards a Blueprint for a Social Animal
authors: Stephen A. Butterfill and Elisabeth Pacherie
year: 2020
isForthcoming: false
booktitle: Minimal Cooperation and Shared Agency
pages: 111-125
doi: 10.1007/978-3-030-29783-1_7
pdfUrl: /pdf/blueprint_social_animal.pdf
---

## Abstract

<p>In this chapter, we attempt to answer the question, By what steps could members of a group capable of acting together with a purpose, coordinating flexibly, communicating cooperatively and deceiving competitors be constructed from creatures with minimal social skills and cognitive abilities? The method we use is creature construction: the idea is to adopt the perspective of a designer tasked with specifying a sequence of creatures, where each is independently viable and has the capacities of its predecessors together with some new capacity which enables it to overcome limits its predecessors faced. In creature construction, the aim is not to characterise actual species, nor to describe actual evolutionary or developmental processes. Instead the aims are to understand how various forms (or prototypes) of joint action are related to, and diverge from, each other; and to identify limits on what can be achieved with a given set of cognitive and social skills.</p><p>We start with Alphonso and his kin, whose social cognition is limited to tracking the goals of others’ actions. We show that despite little cognitive sophistication, the salience and triangulation heuristics enables them to initiate simple joint actions requiring coordination. One group of their descendants, Beki and her kin, develop abilities to produce pointing gestures and object-directed vocalisations, that enable them to enlist others not yet as partners but as social tools, thus extending the range of situations in which they can rely on the salience and triangulation heuristics. Another group of Alphonso’s descendants, Bemi’s kin, learn the art of strategic deception, acquiring increasingly elaborate tactics for manipulating others’ action possibilities. This advantages them in competition. Finally, the Kimi, who are mixed descendants of both the Beki and the Bemi, inherit the former’s communicative abilities and the latter’s abilities for tactical deception. Progressively integrating the two allows them to develop new capacities of selective deception.</p><p>We argue that although our creatures do not yet have all the cognitive capacities classical accounts imply are needed for joint action, they have proxies for some of these capacities. These proxies allow them to coordinate in a limited but useful range of ordinary circumstances. Further, relying on such proxies provide ways of avoiding both omni-doxasticity and omni-intentionality when acting together.</p>
</file>

<file path="src/content/writing/bpd_social_media.html.md">
---
title: "Interpersonal Functioning in Borderline Personality Disorder Traits: A
  Social Media Perspective"
authors: Jinnie Ooi, John Michael, Sakari Lemola, Stephen A. Butterfill, Cynthia
  S. Q. Siew & Lukasz Walasek
year: 2020
isForthcoming: false
journal: Scientific Reports
volume: "1068"
number: "1"
pages: 1-9
doi: 10.1038/s41598-020-58001-x
pdfUrl: /pdf/bpd_social_media.pdf
---

## Abstract

This is the first study to demonstrate interpersonal difficulties associated with borderline personality disorder (BPD) features in the domain of social media. Using crowdsourcing, we presented participants with a battery of questions about their recent social media use, and then assessed their BPD features using the short form of the Five-Factor Borderline Inventory. The results revealed that individuals with higher BPD trait scores reported posting more often on social media, as well as a higher incidence of experiencing regret after posting on social media, and of deleting or editing their posts. They also report a higher degree of importance of social media in their social behavior and daily routines. These results highlight the pervasiveness of interpersonal difficulties associated with BPD features even in the non-clinical population, and demonstrate that these difficulties are also observable in social media behavior. Our findings may provide a starting point for research using data from social media to illuminate the cognitive and emotional processes underpinning the interpersonal difficulties associated with BPD features, and to inform and assess therapeutic interventions.
</file>

<file path="src/content/writing/childrens_selective_learning_from_others.html.md">
---
title: Children's Selective Learning from Others
authors: Erika Nurmsoo, Elizabeth Robinson, and Stephen A. Butterfill
year: 2010
isForthcoming: false
journal: Review of Philosophy and Psychology
volume: "1"
number: "4"
pages: 551-561
doi: 10.1007/s13164-010-0043-y
---

## Abstract

Psychological research into children's sensitivity to testimony has primarily focused on their ability to judge the likely reliability of speakers. However, verbal testimony is only one means by which children learn from others. We review recent research exploring children's early social referencing and imitation, as well as their sensitivity to speakers' knowledge, beliefs, and biases, to argue that children treat information and informants with reasonable scepticism. As children's understanding of mental states develops, they become ever more able to critically evaluate whether to believe new information.
</file>

<file path="src/content/writing/cognitive_architecture_belief_reasoning.html.md">
---
title: "Cognitive Architecture of Belief Reasoning in Children and Adults: A
  Two-Systems Account Primer"
authors: Jason Low, Ian Apperly, Stephen A. Butterfill and Hannes Rakoczy
year: 2016
isForthcoming: false
journal: Child Development Perspectives
volume: "10"
number: "3"
pages: 184-189
doi: 10.1111/cdep.12183
pdfUrl: /pdf/cognitive_architecture_belief_reasoning.pdf
---

## Abstract

Characterizing the cognitive architecture of human mindreading 
forces us to address two puzzles in people’s attributions of belief:
why children show inconsistent expectations about others’ belief-based actions, and why adults’ belief reasoning is sometimes automatic and sometimes not. The seemingly puzzling data suggest humans have multiple mindreading systems that use different models of the mental. The efficient system is shared by infants, children and adults, and uses a minimal model of mind, which enables belief-like states to be tracked. The flexible system is late-developing and uses a canonical model, which incorporates propositional attitudes. A given model’s operation has signature limits that produce performance contrasts, in children as well as adults, between certain types of mindreading tasks
</file>

<file path="src/content/writing/collective_goals.html.md">
---
title: "Towards a Mechanistically Neutral Account of Acting Jointly: The Notion
  of a Collective Goal "
authors: Stephen A. Butterfill and Corrado Sinigaglia
year: 2022
isForthcoming: true
journal: Mind
volume: fzab096
number: "1"
pages: 1-29
doi: 10.1093/mind/fzab096
pdfUrl: /pdf/collective_goals.pdf
---

## Abstract

Anyone who has ever walked, cooked or crafted with a friend is in a position to know that acting jointly is not just acting side-by-side.  But what distinguishes acting jointly from acting in parallel yet merely individually?  Four decades of philosophical research have yielded broad consensus on a strategy for answering this question.  This strategy is emph{mechanistically committed}; that is, it hinges on invoking states of the agents who are acting jointly (often dubbed ‘shared’, ‘we-’ or ‘collective’ intentions).  Despite the consensus, enduring disagreement remains. The disagreement may be a consequence of the strategy; at least this is plausible enough to motivate considering the prospects for an alternative. Our aim is therefore to draw attention to a coherent alternative that is present in the literature but often overlooked. This alternative is emph{mechanistically neutral}: it avoids invoking states of agents. Implementing the alternative, we introduce the notion of a collective goal and a characterisation of acting jointly which meets criteria standardly used in evaluating other accounts and may have some advantages over those accounts.
</file>

<file path="src/content/writing/coordinating_joint_action.html.md">
---
title: Coordinating Joint Action
authors: Stephen A. Butterfill
year: 2017
isForthcoming: false
booktitle: Routledge Handbook on Collective Intentionality
pages: 68-82
pdfUrl: /pdf/coordinating_joint_action.pdf
---
</file>

<file path="src/content/writing/cue_competition_effects.html.md">
---
title: Cue Competition Effects and Young Children's Causal and Counterfactual
  Inferences
authors: Teresa McCormack, Stephen A. Butterfill, Christoph Hoerl and Patrick Burns
year: 2009
isForthcoming: false
journal: Developmental Psychology
volume: "45"
number: "6"
pages: 1563-1575
doi: 10.1037/a0017408
---

## Abstract

The authors examined cue competition effects in young children using the blicket detector paradigm, in which objects are placed either singly or in pairs on a novel machine and children must judge which objects have the causal power to make the machine work. Cue competition effects were found in a 5- to 6-year-old group but not in a 4-year-old group. Equivalent levels of forward and backward blocking were found in the former group. Children's counterfactual judgments were subsequently examined by asking whether or not the machine would have gone off in the absence of 1 of 2 objects that had been placed on it as a pair. Cue competition effects were demonstrated only in 5- to 6-year-olds using this mode of assessing causal reasoning.
</file>

<file path="src/content/writing/drawn_together.html.md">
---
title: "Drawn Together: When Motor Representations Ground Joint Actions"
authors: Francesco della Gatta, Francesca Garbarini, Marco Rabuffetti, Luca
  Viganò, Stephen A. Butterfill and Corrado Sinigaglia
year: 2017
isForthcoming: false
journal: Cognition
volume: "165"
pages: 53--60
doi: 10.1016/j.cognition.2017.04.008
pdfUrl: /pdf/drawn_together.pdf
---

## Abstract

What enables individuals to act together? Recent discoveries suggest that a variety of mechanisms are involved. But something fundamental is yet to be investigated. In joint action, agents represent a collective goal, or so it is often assumed. But how, if at all, are collective goals represented in joint action and how do such representations impact performance? To investigate this question we adapted a bimanual paradigm, the circle-line drawing paradigm, to contrast two agents acting in parallel with two agents performing a joint action. Participants were required to draw lines or circles while observing circles or lines being drawn. The findings indicate that interpersonal motor coupling may occur in joint but not parallel action. This suggests that participants in joint actions can represent collective goals motorically.
</file>

<file path="src/content/writing/fizke_limits.html.md">
---
title: Are There Signature Limits in Early Theory of Mind?
authors: Ella Fizke, Stephen A. Butterfill, Lea van de Loo, Eva Reindl, and
  Rakoczy, Hannes
year: 2017
isForthcoming: false
journal: Journal of Experimental Child Psychology
volume: "162"
number: Supplement C
pages: 209--224
doi: 10.1016/j.jecp.2017.05.005
---

## Abstract

Current theory-of-mind research faces the challenge of reconciling two sets of seemingly incompatible findings: Whereas children come to solve explicit verbal false belief (FB) tasks from around 4 years of age, recent studies with various less explicit measures such as looking time, anticipatory looking, and spontaneous behavior suggest that even infants can succeed on some FB tasks. In response to this tension, two-systems theories propose to distinguish between an early-developing system, tracking simple forms of mental states, and a later-developing system, based on fully developed concepts of belief and other propositional attitudes. One prediction of such theories is that the early-developing system has signature limits concerning aspectuality. We tested this prediction in two experiments. The first experiment showed (in line with previous findings) that 2- and 3-year-olds take into account a protagonist’s true or false belief about the location of an object in their active helping behavior. In contrast, toddlers’ helping behavior did not differentiate between true and false belief conditions when the protagonist’s belief essentially involved aspectuality. Experiment 2 replicated these findings with a more stringent method designed to rule out more parsimonious explanations. Taken together, the current findings are compatible with the possibility that early theory-of-mind reasoning is subject to signature limits as predicted by the two-systems account.
</file>

<file path="src/content/writing/gaining_knowledge_via_other_minds.html.md">
---
title: "Gaining Knowledge via Other Minds: Children's Flexible Trust in Others
  as Sources of Information"
authors: Elizabeth J. Robinson, Stephen A. Butterfill and Erika Nurmsoo
year: 2011
isForthcoming: false
journal: British Journal of Developmental Psychology
volume: "29"
number: "4"
pages: 961-980
doi: 10.1111/j.2044-835X.2011.02036.x
---

## Abstract

In five experiments, we examined 3- to 6-year-olds' understanding that they could gain knowledge indirectly from someone who had seen something they had not. Consistent with previous research, children judged that an informant, who had seen inside a box, knew its contents. Similarly, when an informant marked a picture to indicate her suggestion as to the content of the box, 3- to 4-year-olds trusted this more frequently when the informant had seen inside the box than when she had not. Going beyond previous research, 3- to 4-year-olds were also sensitive to informants' relevant experience when they had to look over a barrier to see the marked picture, or ask for the barrier to be raised. Yet when children had to elicit the informant's suggestion, rather than just consult a suggestion already present, even 4- to 5-year-olds were no more likely to do so when the informant had seen the box's content than when she had not, and no more likely to trust the well-informed suggestion than the uninformed one. We conclude that young children who can ask questions may not yet fully understand the process by which they can gain accurate information from someone who has the experience they lack.
</file>

<file path="src/content/writing/goals_targets.html.md">
---
title: "Goals and Targets: A Developmental Puzzle about Sensitivity to Others’
  Actions"
authors: Stephen A. Butterfill
year: 2021
isForthcoming: false
journal: Synthese
volume: "198"
number: "17"
pages: 3969–3990
doi: 10.1007/s11229-019-02214-9
pdfUrl: /pdf/goals_targets.pdf
---

## Abstract

Sensitivity to others’ actions is essential for social animals like humans and a fundamental requirement for any kind of social cognition. Unsurprisingly, it is present in humans from early in the first year of life. But what processes underpin infants’ sensitivity to others’ actions? Any attempt to answer this question must solve twin puzzles about the development of goal tracking. Why does some, but not all, of infants’ goal tracking appear to be limited by their abilities to represent the observed action motorically at the time it occurs? And why does their sensitivity to action sometimes manifest itself differently in dishabituation, pupil dilation and anticipatory looking? Solving these twin puzzles is critical for understanding humans’ earliest sensitivity to others’ actions. After introducing the puzzles, this paper argues that solving them may require identifying multiple, distinct processes for tracking the targets and goals of actions.
</file>

<file path="src/content/writing/infants_representations_of_causation.html.md">
---
title: Infants' Representations of Causation (Commentary on Susan Carey, The
  Origin of Concepts)
authors: Stephen A. Butterfill
year: 2011
isForthcoming: false
journal: Behavioral and Brain Sciences
volume: "34"
number: "3"
pages: 126-127
doi: 10.1017/S0140525X10002426
---

## Abstract

It is consistent with the evidence in The Origin of Concepts to conjecture that infants' causal representations, like their numerical representations, are not continuous with adults', so that bootstrapping is needed in both cases.
</file>

<file path="src/content/writing/intuitions_about_joint_commitment.html.md">
---
title: Intuitions about Joint Commitment
authors: John Michael and Stephen A. Butterfill
year: 2022
isForthcoming: true
journal: Philosophical Psychology
volume: "0"
number: "0"
pages: 1-16
doi: 10.1080/09515089.2022.2153659
pdfUrl: /pdf/intuitions_about_joint_commitment.pdf
---

## Abstract

In what sense is commitment essential to joint action, and do the participants in a joint action themselves perceive commitment as essential? Attempts to answer this question have so far been hampered by clashes of intuition. Perhaps this is because the intuitions in question have mostly been investigated using informal methods only. To explore this possibility, we adopted a more formal approach to testing intuitions about joint action, sampling naïve participants’ intuitions about experimentally controlled scenarios. This approach did reveal patterns in participants’ responses which may hint at potential conceptual links between commitment and joint action. It did not however provide evidence to support the view that commitment is essential to joint action, at least not from the agents’ own perspective. We conclude that intuitions alone, even when drawn systematically from a large sample, may be a poor basis for theorizing about joint action.
</file>

<file path="src/content/writing/joint_action_minimalist_approach.html.md">
---
title: "Joint Action: A Minimalist Approach"
authors: Stephen A. Butterfill
year: 2016
isForthcoming: false
booktitle: Routledge Handbook on the Social Mind
pages: 357–369
pdfUrl: /pdf/joint_action_minimalist_approach.pdf
---
</file>

<file path="src/content/writing/joint_action_rpp_edited_volume.html.md">
---
title: "Joint Action: What Is Shared?"
authors: Stephen A. Butterfill and Natalie Sebanz (eds.)
year: 2011
isForthcoming: false
journal: Review of Philosophy and Psychology
volume: "2"
number: "2"
---


<h3>Description</h3>
<p>A collection of papers by philosophers and cognitive scientists on joint action and shared intention.</p>
<h3>Contents</h3>
<ol>
  <li>Stephen A. Butterfill and Natalie Sebanz: Editorial: Joint Action: What Is Shared?</li>
  <li>Dorit Wenke, Silke Atmaca, Antje Holl&auml;nder: What is Shared in Joint Action? Issues of Co-representation, Response Conflict, and Agent Identification</li>
  <li>Elisabeth Pacherie: Framing Joint Action</li>
  <li>Celia A. Brownell: Early Developments in Joint Action</li>
  <li>Thomas H. Smith: Playing One's Part</li>
  <li>Jay R. Elliott: Stag Hunts and Committee Work: Cooperation and the Mutualistic Paradigm</li>
  <li>Christopher Woodard: Rationality and the Unit of Action</li>
  <li>Axel Seemann: Joint Motor Action and Cross-Creature Embodiment</li>
  <li>Giovanni Pezzulo: Shared Representations as Coordination Tools for Interaction</li>
  <li>Olle Blomberg: Socially Extended Intentions-in-Action</li>
  <li>John Michael: Shared Emotions and Joint Action</li>
</ol>
</file>

<file path="src/content/writing/joint_action_rpp_special_issue.html.md">
---
title: "Joint Action: What Is Shared?  Introduction to the special issue"
authors: Stephen A. Butterfill and Natalie Sebanz
year: 2011
isForthcoming: false
journal: Review of Philosophy and Psychology
volume: "2"
number: "2"
pages: 137-146
doi: 10.1007/s13164-011-0062-3
---

## Abstract

Joint action raises a tangle of philosophical, developmental and cognitive questions. Many of these questions are naturally understood to be about sharing, about the sharing of intentions, emotions, task representations, and action plans. Of course it is not easy to explain what it means to share in this context. Few researchers hold that agents can literally share intentions, emotions or other states in the sense in which two siblings share a parent; and while it is uncontroversial that agents can share intentions, emotions and other states in the sense in which siblings share genes, on almost any account this kind of sharing is not sufficient for joint action. The overarching question in this special issue is which forms of sharing (if any) are needed to explain the development of joint action, to characterise the mechanisms which make effective joint action possible and to explain what joint action is. In this Introduction we will explain how the papers in this special issue contribute to answering this question and, in some cases, raise new puzzles.
</file>

<file path="src/content/writing/level_2.md">
---
title: Direct and indirect measures of Level-2 perspective-taking in children
  and adults
authors: Andrew Surtees, Stephen A. Butterfill and Ian Apperly
year: 2012
isForthcoming: false
journal: British Journal of Developmental Psychology
volume: "30"
number: "1"
pages: 75-86
doi: 10.1111/j.2044-835X.2011.02063.x
---

## Abstract

Studies with infants show divergence between performance on theory of mind tasks depending on whether direct or indirect measures are used. It has been suggested that direct measures assess a flexible but cognitively demanding ability to reason about the minds of others, whereas indirect measures assess distinct processes which afford more efficient but less flexible theory of mind abilities (Apperly & Butterfill, 2009). This leads to the prediction that performance on indirect measures should be subject to signature limits. The current study tested whether the Level-1/Level-2 distinction might constitute one such limit. The study adapted a task that has shown evidence of Level-1 perspective-taking on both direct and indirect measures (Samson, Apperly, Braithwaite, Andrews, & Bodley-Scott, 2010). The aim was to test Level-2 perspective-taking in a sample of 6- to 11-year-olds (N = 80) and adults (N = 20). Participants were able to make Level-2 judgements on the direct measure. In contrast with the findings from Level-1 perspective-taking, there was no evidence of automatic processing of Level-2 perspectives on the indirect measure. This finding is consistent with the view that theory of mind abilities assessed by indirect measures are subject to signature limits. The Level-1/Level-2 distinction, suitably refined, marks one way in which efficient but inflexible theory of mind abilities are limited.
</file>

<file path="src/content/writing/lightbulb.html.md">
---
title: Joint Action Goals Reduce Visuomotor Interference Effects from a
  Partner’s Incongruent Actions
authors: Sam Clarke, Luke McEllin, Anna Francová, Marcell Székely, Stephen A.
  Butterfill and John Michael
year: 2019
isForthcoming: false
journal: Scientific Reports
volume: "9"
number: "1"
pages: 1-9
doi: 10.1038/s41598-019-52124-6
pdfUrl: /pdf/lightbulb.pdf
---

## Abstract

Joint actions often require agents to track others’ actions while planning and executing physically incongruent actions of their own. Previous research has indicated that this can lead to visuomotor interference effects when it occurs outside of joint action. How is this avoided or overcome in joint actions? We hypothesized that when joint action partners represent their actions as interrelated components of a plan to bring about a joint action goal, each partner’s movements need not be represented in relation to distinct, incongruent proximal goals. Instead they can be represented in relation to a single proximal goal – especially if the movements are, or appear to be, mechanically linked to a more distal joint action goal. To test this, we implemented a paradigm in which participants produced finger movements that were either congruent or incongruent with those of a virtual partner, and either with or without a joint action goal (the joint flipping of a switch, which turned on two light bulbs). Our findings provide partial support for the hypothesis that visuomotor interference effects can be reduced when two physically incongruent actions are represented as mechanically interdependent contributions to a joint action goal.
</file>

<file path="src/content/writing/mindreading_balance.html.md">
---
title: "Mindreading in the Balance: Adults’ Mediolateral Leaning and
  Anticipatory Looking Foretell Others’ Action Preparation in a False-Belief
  Interactive Task"
authors: Giovanni Zani, Stephen A. Butterfill and Jason Low
year: 2020
isForthcoming: false
journal: Royal Society Open Science
volume: "7"
number: "1"
pages: 1-14
doi: 10.1098/rsos.191167
pdfUrl: /pdf/mindreading_balance.pdf
---

## Abstract

Anticipatory looking on mindreading tasks can indicate our expectation of an agent's action. The challenge is that social situations are often more complex, involving instances where we need to track an agent's false belief to successfully identify the outcome to which an action is directed. If motor processes can guide how action goals are understood, it is conceivable—where that kind of goal ascription occurs in false-belief tasks—for motor representations to account for someone's belief-like state. Testing adults (N = 42) in a real-time interactive helping scenario, we discovered that participants' early mediolateral motor activity (leftwards–rightwards leaning on balance board) foreshadowed the agent's belief-based action preparation. These results suggest fast belief-tracking can modulate motor representations generated in the course of one's interaction with an agent. While adults' leaning, and anticipatory looking, revealed the contribution of fast false-belief tracking, participants did not correct the agent's mistake in their final helping action. These discoveries suggest that adults may not necessarily use another's belief during overt social interaction or find reflecting on another's belief as being normatively relevant to one's own choice of action. Our interactive task design offers a promising way to investigate how motor and mindreading processes may be variously integrated.
</file>

<file path="src/content/writing/minimal_architecture.html.md">
---
title: A Minimal Architecture for Joint Action
authors: Cordula Vesper, Stephen A. Butterfill, Guenther Knoblich and Natalie Sebanz
year: 2010
isForthcoming: false
journal: Neural Networks
volume: "23"
number: 8-9
pages: 998-1003
doi: 10.1016/j.neunet.2010.06.002
---

## Abstract

What kinds of processes and representations make joint action possible? In this paper, we suggest a minimal architecture for joint action that focuses on representations, action monitoring and action prediction processes, as well as ways of simplifying coordination. The architecture spells out minimal requirements for an individual agent to engage in a joint action. We discuss existing evidence in support of the architecture as well as open questions that remain to be empirically addressed. In addition, we suggest possible interfaces between the minimal architecture and other approaches to joint action. The minimal architecture has implications for theorising about the emergence of joint action, for human-machine interaction, and for understanding how coordination can be facilitated by exploiting relations between multiple agents' actions and between actions and the environment.
</file>

<file path="src/content/writing/minimal_brains_discussion_intro.html.md">
---
title: "Introduction: Symposium on ‘How to Constuct a Minimal Theory of Mind’"
authors: Stephen A. Butterfill & Ian A. Apperly
year: 2013
isForthcoming: false
booktitle: Brains (philosophyofbrains.com)
pdfUrl: /pdf/minimal_brains_discussion_intro.pdf
---


<p>The target paper is here: <a href="/writing/minimal_theory_of_mind/">How to Construct a Minimal Theory of Mind</a></p>
<p>There are commentaries by <a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/rakoczy.pdf" target="_blank">Hannes Rakoczy [pdf]</a><span>, </span><a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/spaulding.pdf" target="_blank">Shannon Spaulding [pdf]</a><span> and </span><a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/zawidzki.pdf" target="_blank">Tadeusz Zawidzki [pdf]</a><span> and comments from many others on  </span><a href="http://philosophyofbrains.com/2013/11/11/symposium-on-butterfill-and-apperlys-how-to-construct-a-minimal-theory-of-mind-mind-language-28-5-606-63.aspx">Brains</a><span>.</span></p>
<p>We also wrote  <a href="/writing/minimal_brains_discussion_replies/">replies to the commentaries</a><span>.</span></p>
</file>

<file path="src/content/writing/minimal_brains_discussion_replies.html.md">
---
title: Replies to Three Commentaries on ‘How to Construct a Minimal Theory of Mind’
authors: Stephen A. Butterfill & Ian A. Apperly
year: 2013
isForthcoming: false
booktitle: Brains (philosophyofbrains.com)
pdfUrl: /pdf/minimal_brains_discussion_replies.pdf
---

## Abstract

We are grateful to Hannes Rakoczy, Shannon Spaulding and Tadeusz Zawidzki for three illuminating and very helpful critical commentaries. Here we report some of what we have learned from them and answer the objections.


<p>The target paper is here: <a href="/writing/minimal_theory_of_mind/">How to Construct a Minimal Theory of Mind</a></p>
<p>There are commentaries by <a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/rakoczy.pdf" target="_blank">Hannes Rakoczy [pdf]</a><span>, </span><a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/spaulding.pdf" target="_blank">Shannon Spaulding [pdf]</a><span> and </span><a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/zawidzki.pdf" target="_blank">Tadeusz Zawidzki [pdf]</a><span> and comments from many others on  </span><a href="http://philosophyofbrains.com/2013/11/11/symposium-on-butterfill-and-apperlys-how-to-construct-a-minimal-theory-of-mind-mind-language-28-5-606-63.aspx">Brains</a><span>.</span></p>
</file>

<file path="src/content/writing/minimal_theory_of_mind.html.md">
---
title: How to Construct a Minimal Theory of Mind
authors: Stephen A. Butterfill and Ian A. Apperly
year: 2013
isForthcoming: false
journal: Mind and Language
volume: "28"
number: "5"
pages: 606-637
pdfUrl: /pdf/minimal_theory_of_mind.pdf
---

## Abstract

What could someone represent that would enable her to track, at least within limits, others' perceptions, knowledge states and beliefs including false beliefs?  An obvious possibility is that she might represent these very attitudes as such. It is sometimes tacitly or explicitly assumed that this is the only possible answer. However we argue that several recent discoveries in developmental, cognitive, and comparative psychology indicate the need for other, less obvious possibilities. Our aim is to meet this need by describing the construction of a minimal theory of mind.  Minimal theory of mind is rich enough to explain systematic success on tasks held to be acid tests for theory of mind cognition including many false belief tasks. Yet minimal theory of mind does not require representing propositional attitudes, or any other kind of representation, as such. Minimal theory of mind may be what enables those with limited cognitive resources or little conceptual sophistication, such as infants, chimpanzees, scrub-jays and human adults under load,  to track others' perceptions, knowledge states and beliefs.


<p> <a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/rakoczy.pdf" target="_blank">Hannes Rakoczy [pdf]</a><span>, </span><a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/spaulding.pdf" target="_blank">Shannon Spaulding [pdf]</a><span> and </span><a href="http://philosophyofbrains.com/wp-content/uploads/2013/11/zawidzki.pdf" target="_blank">Tadeusz Zawidzki [pdf]</a><span> have commentaries  on this paper in  a symposium in </span><a href="http://philosophyofbrains.com/2013/11/11/symposium-on-butterfill-and-apperlys-how-to-construct-a-minimal-theory-of-mind-mind-language-28-5-606-63.aspx">Brains</a><span>, which includes comments from many others.  </span><span>We also wrote  </span><a href="/writing/minimal_brains_discussion_replies/">replies to the commentaries</a><span>.</span></p>
</file>

<file path="src/content/writing/modularity.html.md">
---
title: What are Modules and What Is Their Role in Development?
authors: Stephen A. Butterfill
year: 2007
isForthcoming: false
journal: Mind and Language
volume: "22"
number: "4"
pages: 450-73
doi: 10.1111/j.1468-0017.2007.00316.x
---

## Abstract

Modules are widely held to play a central role in explaining mental development and in accounts of the mind generally. But there is much disagreement about what modules are, which shows that we do not adequately understand modularity. This paper outlines a Fodoresque approach to understanding one type of modularity. It suggests that we can distinguish modular from nonmodular cognition by reference to the kinds of process involved, and that modular cognition differs from nonmodular forms of cognition in being a special kind of computational process. The paper concludes by considering implications for the role of modules in explaining mental development.
</file>

<file path="src/content/writing/motor_representation_acting_together.html.md">
---
title: Motor Representation in Acting Together
authors: Corrado Sinigaglia and Stephen A. Butterfill
year: 2022
isForthcoming: false
journal: Synthese
volume: "200"
number: "82"
pages: 1-16
doi: 10.1007/s11229-022-03539-8
pdfUrl: /pdf/motor_representation_acting_together.pdf
---

## Abstract

People walk, build, paint and otherwise act together with a purpose in myriad ways. What is the relation between the actions people perform in acting together with a purpose and the outcome, or outcomes, to which their actions are directed? We argue that fully characterising this relation will require appeal not only to intention, knowledge and other familiar philosophical paraphernalia but also to another kind of representation involved in preparing and executing actions, namely motor representation. If we are right, motor representation plays a central role in the story of acting together.
</file>

<file path="src/content/writing/motor_representation_action_experience.html.md">
---
title: Motor Representation and Action Experience in Joint Action
authors: Corrado Sinigaglia and Stephen A. Butterfill
year: 2020
isForthcoming: false
booktitle: Minimal Cooperation and Shared Agency
pages: 181–193
doi: 10.1007/978-3-030-29783-1_11
pdfUrl: /pdf/motor_representation_action_experience.pdf
---

## Abstract

Acting together with a purpose is a familiar feature of everyday life. We jump together, play music together and move tables together. But what do we experience of action in acting together? It is perhaps tempting to suppose that there is a special way in which we can experience our own actions, and that we cannot experience the actions of others in this way. This view would imply that in acting together, our own actions are experienced in a way that our partners’ actions are not. However recent research on motor representation suggests that, in observing another act, it may be possible to experience her actions in whatever sense we can experience our own actions. This makes it at least conceivable that in acting together we can experience the actions each of us performs in the same way. But the occurrence of a joint action involves more than merely the occurrences of two individual actions. Are there experiences of joint actions which involve more than merely two or more experiences of individual actions? In this chapter we defend a positive answer. In some cases, experiences associated with joint action are experiences of action in whatever sense experiences of acting alone are.
</file>

<file path="src/content/writing/motor_representation_in_goal_ascription.html.md">
---
title: Motor Representation in Goal Ascription
authors: Corrado Sinigaglia and Stephen A. Butterfill
year: 2016
isForthcoming: false
booktitle: "Foundations of embodied cognition 2: Conceptual and Interactive Embodiment"
pages: 150-164
pdfUrl: /pdf/motor_representation_in_goal_ascription.pdf
---

## Abstract

Goal ascription, the process of identifying outcomes to which purposive actions are directed, is indispensable for predicting others’ behaviours and understanding their minds.  
But which mechanisms underpin goal ascription?
This chapter examines several ways in which motor representations and processes are involved in different forms of goal ascription.
We argue that motor representations and processes matter for goal ascription in two ways.
They provide for capturing the directedness of an action to an outcome, and they shape the observers’ experiences of actions in such a way that these experiences reveal the goals of actions.
The occurrence of motor representations in action observation thereby makes available, independently of any prior knowledge of others’ mental states, a route to knowledge of the goals of their actions.
</file>

<file path="src/content/writing/motor_representation_skill.html.md">
---
title: Motor Representation and Knowledge of Skilled Action
authors: Corrado Sinigaglia and Stephen A. Butterfill
year: 2020
isForthcoming: false
booktitle: Routledge Handbook of Skill and Expertise
pages: 292-305
pdfUrl: /pdf/motor_representation_skill.pdf
---

## Abstract

If you are more skilled in performing certain actions, you are probably also better able to acquire knowledge when observing those actions. Why are performance skills so connected to observation skills? In this chapter, the authors defend a conjecture: it is because performing and observing actions involves a common element, namely motor representations of outcomes to which the actions are directed. This conjecture, which is supported by a significant body of evidence, implies that motor representations can have content-respecting influences on knowledge states. How is this possible? How do motor representations interface with knowledge states? Several distinct candidate answers have been proposed, but the evidence that would distinguish them is not yet available. There is, then, a major gap in our understanding of how expertise matters for gaining knowledge of observed actions. We know that motor representations do, in fact, facilitate the acquisition of observational knowledge, but no one can yet say how they do so.
</file>

<file path="src/content/writing/mummification.html.md">
---
title: Visibly constraining an agent modulates observers’ automatic false-belief
  tracking
authors: Jason Low, Katheryn Edwards & Stephen A. Butterfill
year: 2020
isForthcoming: false
journal: Scientific Reports
volume: "10"
number: "1"
pages: 1-12
doi: 10.1038/s41598-020-68240-7
pdfUrl: /pdf/mummification.pdf
---

## Abstract

Our motor system can generate representations which carry information about the goals of another agent’s actions. However, it is not known whether motor representations play a deeper role in social understanding, and, in particular, whether they enable tracking others’ beliefs. Here we show that, for adult observers, reliably manifesting an ability to track another’s false belief critically depends on representing the agent’s potential actions motorically. One signature of motor representations is that they can be disrupted by constraints on an observed agent’s action capacities. We therefore used a ‘mummification’ technique to manipulate whether the agent in a visual ball-detection task was free to act or whether he was visibly constrained from acting. Adults’ reaction times reliably reflected the agent’s beliefs only when the agent was free to act on the ball and not when the agent was visibly constrained from acting. Furthermore, it was the agent’s constrained action capabilities, rather than any perceptual novelty, that determined whether adult observers’ reaction times reliably reflected the agent’s beliefs. These findings signal that our motor system may underpin more of social cognition than previously imagined, and, in particular, that motor representations may underpin automatic false-belief tracking.
</file>

<file path="src/content/writing/planning_for_collective_agency.html.md">
---
title: Planning for Collective Agency
authors: Stephen A. Butterfill
year: 2016
isForthcoming: false
booktitle: Collective Agency and Cooperation in Natural and Artificial Systems
pages: 149--168
pdfUrl: /pdf/planning_for_collective_agency.pdf
---

## Abstract

Which planning mechanisms enable agents to coordinate their actions, and what if anything do these tell us about the nature of collective agency?  On the leading, best developed account, Michael Bratman's, collective agency  is explained in terms of interconnected planning.  For our plans to be interconnected is for them to concern not just facts about our environment and goals but also facts about each others' plans.  This chapter contrasts interconnected with parallel planning.  In parallel planning, we each individually plan all of our actions and so are in a position to conceive of our own and each other's actions as parts of a single plan or exercises of a single ability.  (The very idea of parallel planning may initially seem incoherent; the chapter examines this issue.)  Could parallel rather than interconnected planning underpin intentional collective agency?  Some considerations in favour of a positive answer are provided by appeal to recent evidence on the role of motor representation in coordinating exercises of collective agency.
</file>

<file path="src/content/writing/psychological_research_on_joint_action.html.md">
---
title: Psychological Research on Joint Action
authors: Guenther Knoblich, Stephen A. Butterfill and Natalie Sebanz
year: 2011
isForthcoming: false
booktitle: Psychology of Learning and Motivation (Vol. 51)
pages: 59-101
doi: 10.1016/B978-0-12-385527-5.00003-6
---

## Abstract

When two or more people coordinate their actions in space and time to produce a joint outcome, they perform a joint action. The perceptual, cognitive, and motor processes that enable individuals to coordinate their actions with others have been receiving increasing attention during the last decade, complement- ing earlier work on shared intentionality and discourse. This chapter reviews current theoretical concepts and empirical findings in order to provide a structured overview of the state of the art in joint action research. We distin- guish between planned and emergent coordination. In planned coordination, agents' behavior is driven by representations that specify the desired outcomes of joint action and the agent's own part in achieving these outcomes. In emergent coordination, coordinated behavior occurs due to perception-action couplings that make multiple individuals act in similar ways, independently of joint plans. We review evidence for the two types of coordination and discuss potential synergies between them.
</file>

<file path="src/content/writing/review_consciousness.html.md">
---
title: "Review of Consciousness: New Philosophical Perspectives. Edited by
  Quentin Smith and Aleksander Jokic"
authors: Stephen A. Butterfill
year: 2005
isForthcoming: false
journal: Philosophical Quarterly
volume: "55"
number: "219"
pages: 373-375
doi: 10.1111/j.0031-8094.2005.00405.x
---
</file>

<file path="src/content/writing/review_joint_commitment.html.md">
---
title: Review of Joint Commitment by Margaret Gilbert
authors: Stephen A. Butterfill
year: 2017
isForthcoming: false
journal: Journal of Moral Philosophy
volume: "14"
number: "4"
doi: 10.1163/17455243-01404004
pdfUrl: /pdf/review_joint_commitment.pdf
---
</file>

<file path="src/content/writing/review_self-knowing_agents.html.md">
---
title: Review of Self-Knowing Agents by Lucy O'Brien
authors: Stephen A. Butterfill
year: 2009
isForthcoming: false
journal: Philosophical Review
volume: "118"
number: "3"
pages: 413-5
doi: 10.1215/00318108-2009-013
---


<p>The first-person is "expressive of self-consciousness" (64).  Someone who uses the first-person, for example in saying or thinking "Where am I?",  not only succeeds in referring to herself but also knows that she refers to herself.  In Self-Knowing Agents, O'Brien's primary aim is to explain how it is that using the first-person guarantees this knowledge-that is, to explain "how it is that 'I' expresses self-consciousness" (57).</p>
<p>The explanation hinges on agent's awareness, the awareness an agent has of an action by virtue of controlling it.  Control consists in this: the agent evaluates possibilities for action which she is aware of as possibilities; then she selects, endorses or accepts one (116-7, 183).  The possibilities in question are basic actions, that is, actions the subject can carry out without having to do anything else (163-5).  The process of evaluation and selection immediately determines how the agent will act.  For this reason, an agent who knows that some action resulted from her evaluation and selection is in a position to know which basic action resulted (165).  Further, nobody else's actions can be immediately determined by her process of evaluation and selection, only the agent's (184).  For this reason, if an agent knows that some action occurred as an immediate consequence of her process of evaluation and selection, then she is able to know that the action is her action (119).  The general idea, then, is that agent's awareness does not involve first-person reference but does enable agents to know with respect to their own actions both who is acting and what the action is (123).</p>
<p>The explanation of how 'I' expresses self-consciousness is now straightforward.  O'Brien holds that there are mental actions and that judging, wondering and supposing are all actions (89).  Using the first-person in thought or talk is a basic action (or perhaps a component of one) and so can be done with agent's awareness.  Where the first-person is used with agent's awareness, the agent can know that her action is one of using the first-person and that she is the agent of this action.  Now users of the first-person also know that the first-person is governed by the rule that it refers to the subject who produced it (77).  Knowledge of this rule in combination with agent's awareness enables subjects who use the first-person to judge that they have referred to themselves.  </p>
<p>In short, given some general knowledge about the term 'I', what enables us to know who we are referring to when using 'I' is the act of using 'I' itself.  </p>
<p>The book also explores a set of related claims about self-knowledge.  These centre on the question, How do we know which bodily action we are performing and, in the case of mental action, which thought we are thinking?  As sketched above, agent's awareness provides an answer.  When an action results from evaluating and selecting possibilities that we are aware of as possibilities, the agent can know what she is thinking or doing because she selected it.  So it is the fact of acting with agent's awareness and not any perception or representation of our actions that enables us to know what we are thinking and doing.</p>
<p>The positive claims of Self-Knowing Agents are supported by critical discussion of alternative positions.  One type of alternative is to reject the datum O'Brien sets out to explain, that the first-person is expressive of self-consciousness.  This might be rejected either on the grounds that 'I' does not refer (Anscombe) or the grounds that using the first-person need not involve knowing that one refers to oneself (Mellor).   Both views are countered in detail (Chapter 2 and 59-65).  Perceptual accounts are another type of alternative to O'Brien's position.  In Chapter 3, O'Brien discusses the view that self-conscious self-reference might be grounded in bodily awareness and Gareth Evans' view that self-conscious self-reference depends on being able to use perceptual information to track one's own location.  Both views imply that perception, or at least memory based on perception, is necessary for self-conscious self-reference (47-8).  O'Brien objects that we could use the first-person self-consciously while suffering complete memory loss and full sensory deprivation (4-5, 34, 46).</p>
<p>Is it true, as O'Brien and others claim, that agents would know who their own uses of 'I' referred to even if deprived of memory and perception?  Although this claim carries much weight in Self-Knowing Agents, it is not defended.  Unless this claim is obviously true, there is a gap in the main argument against perceptual accounts.</p>
<p>O'Brien interprets others' views in the most charitable way possible and faces objections to her own squarely.  She also identifies a counterexample to her own position.  Recall that agent's awareness was characterised in terms of control over action.  This control takes the form of evaluating possibilities for action and selecting one.  When the action is judging, O'Brien illustrates the general idea by suggesting that we "accept or endorse a given thought in the light of our awareness of the possible judgements we could make, and the reasons in favour of one over another" (116).  But consider being suddenly struck by the thought, "I am supposed to be outside the school gates right now" (90).  Thinking this thought seems to involve self-reference as expressive of self-consciousness as any other use of the first-person.  But being struck by a thought involves no process of evaluating possible thoughts and so does not involve agent's awareness.  On the face of it, then, O'Brien's account cannot explain self-conscious uses of the first-person in all cases.</p>
<p>The problem also affects O'Brien's account of self-knowledge.  Many of our actions, including our judgements, are triggered by things others say or do, and by changes in our situation.  The rude remark provokes a swift retort, the infant's smile induces a hug.  Agents don't generally evaluate and select possibilities in such cases, but they do exercise control to the extent that they can inhibit their actions.  Control by inhibition cannot explain self-knowledge; indeed, inhibition sometimes requires awareness of which action one is inhibiting.  So agent's awareness construed as evaluation and selection cannot explain an agent's knowledge of which action she is performing in these cases.</p>
<p>O'Brien's main response to these counterexamples is to suggest that they are derivative in the sense that self-knowledge in cases lacking agent's awareness depends on the existence of other cases in which there is agent's awareness (90-2).  This suggestion is hard to evaluate without an account of how the dependence works.  When an infant's smile induces hugging (say), how does the agent's knowledge of who is acting now depend on her knowledge of who acted on other occasions?  O'Brien's core idea is that the fact of acting enables an agent to know who is acting.  This seems equally plausible in cases where action is not the outcome of a process evaluation and selection.  Perhaps, then, the substantive account of agent's awareness needs modification.  As it stands, O'Brien's account makes no appeal to connections among a subject's thoughts and actions.  The account works as well for an agent who acts just once as it does for an agent who enjoys a long and eventful life.  If the lives of self-consciousness agents necessarily form more than a series of isolated actions, one way to modify the account of agent's awareness would be to appeal to how an agent's judgements fit with her other thoughts and how her bodily actions affect her plans.  </p>
<p>Self-Knowing Agents is a deep and ambitious book which develops and defends a new thesis about the role of agency in self-reference and self-knowledge.  Readers will be grateful that O'Brien sets the scene for her account with sympathetic and rigorous discussion of competing and connected positions.  And there is a richness to the book which this review entirely fails capture, for O'Brien deftly weaves the main arguments into larger-scale views about the nature of action, bodily awareness and agency.</p>
</file>

<file path="src/content/writing/review_the_rational_imagination.html.md">
---
title: Review of The Rational Imagination by Ruth Byrne
authors: Stephen A. Butterfill
year: 2008
isForthcoming: false
journal: Mind
volume: "117"
number: "468"
pages: 1065-1069
doi: 10.1093/mind/fzn127
---

## Abstract

This book develops a mental models approach to counterfactual thinking. It brings together a large body of empirical research by the author and collaborators, and interprets the findings in the light of some novel views about meaning.  In this review, I argue that unless indicative conditional, counterfactual, and causal assertions really do all mean the same thing, the mental models discussed in <i>The Rational Imagination</i>  are not adequate for explaining counterfactual and causal reasoning.
</file>

<file path="src/content/writing/review_thinking_without_words.html.md">
---
title: Review of Thinking without Words by Jose Luis Bermudez
authors: Stephen A. Butterfill
year: 2004
isForthcoming: false
journal: Mind
volume: "113"
number: "452"
pages: 733-736
doi: 10.1093/mind/113.452.733
---


<p>That some animals, early hominids and infants think (or did so once) is increasingly hard to doubt.  Our evidence includes everything from the manufacture and use of tools (55, 127), to the ways infants react when shown apparently impossible physical events (81).  Berm&uacute;dez' project is to explain the nature of this thinking.  How can we know what an infant, ape or early hominid thinks?  What determines the contents of their thoughts?  Are they thinkers in just the sense that you and I are, or do we humans become fundamentally different types of thinker with our own distinctive cognitive abilities after infancy?  Thinking without Words offers answers to these questions, mixing philosophical argument with accounts of empirical research.</p>
<p>The notion that there is such a thing as 'thinking without words' is doubly controversial.  First, some philosophers hold that creatures incapable of communication by language are also  incapable of thought.  Berm&uacute;dez counters this view in Chapter 3, where he argues at length that some non- and pre-linguistic creatures are capable of real thinking-that is, their decision-making involves both structured representations and instrumental reasoning about the consequences of actions (51).  Second, the notion of 'thinking without words' is controversial because, assuming that nonlinguistic creatures can think, it is not obvious that their thinking differs in kind from ours.  And even supposing their thinking does differ in kind from ours, it is not obvious that this has anything to do with language.  However, as I will shortly explain, Berm&uacute;dez argues that languageless creatures' thinking is more limited than ours and that 'an important class of thoughts ... is in principle unavailable to nonlinguistic creatures' (150).  Not only does their thinking exclude inference (111) and practical reasoning (131-2); they are also unable to think thoughts involving logical constants such as NOT and quantifiers such as EVERY (Chapters 8-9).  If Berm&uacute;dez is right that languageless creatures don't infer or quantify, what could their thinking be like?  Come to think of it, how could they be thinking at all?</p>
<p>Berm&uacute;dez adopts a nice strategy for answering these questions.  Take a theory about some aspect of thinking, a theory which is coherent but doesn't quite work because it lacks the resources to capture our ordinary ways of thinking.  Then see whether that theory might work for a creature whose thinking is more limited than ours.  Recycling theories in this way makes a virtue of their limitations, provided the theories are limited in just the ways that the target creatures' thinking is limited.  Take NOT, a concept which apes are supposed to lack on account of their languageless state (as I'll soon explain).  Some philosophers have held that NOT modifies predicates rather than whole sentences.  While that probably isn't how our concept NOT works, there could be an element of ape thinking-'protonegation'-that does work this way (142-5).  So it is that Berm&uacute;dez attempts to characterise nonlinguistic thinking by recycling a failed theory of negation.  The same strategy is used to explain what determines the contents of nonlinguistic creatures' thoughts.  Take 'success semantics', a theory according to which 'the content of a belief is given by ... the condition that would have to obtain for the various desires associated with it to be satisfied' (65).  While success semantics probably isn't right for our thoughts (66-8), Berm&uacute;dez suggests, it's the sort of theory we need for nonlinguistic thought.</p>
<p>Why is nonlinguistic thinking supposed to be limited?  In Chapters 8 and 9, Berm&uacute;dez argues like this:</p>
<ol>
  <li>Thinking thoughts containing NOT requires the ability to think 'about how the truth-value of one thought might be related to the truth-value of another thought' (178); this is 'intentional ascent'.</li>
  <li>Intentional ascent requires 'semantic ascent', which is the ability to think about sentences.</li>
</ol>
<p>Therefore:</p>
<ol start="3">
  <li>Using NOT requires the ability to communicate by language.</li>
</ol>
<p>In addition to NOT, this argument is supposed to work for any truth-functional connective, quantifier, tense operator or modal operator.</p>
<p>The first premise, [1], is merely stated.  Although quite widely held, it is not obvious how this premise could be true.  Is Berm&uacute;dez' position that the ability to think about the truth values of thoughts is always, or even just sometimes, actually exercised in thinking thoughts containing NOT?  If it is, then we need to know how this ability is exercised.  As he notes, thinking THE CAT IS NOT HERE doesn't involve thinking about the truth values of thoughts in anything like the sense in which it involves thinking about cats (178).  If, on the other hand, Berm&uacute;dez' position is that thinking thoughts containing NOT never involves exercising an ability to think about the truth values of thoughts, then he needs to explain how merely having this ability could be a necessary condition for thinking thoughts containing NOT.  Either way, it is unclear how, if at all, an ability to think about the truth values of thoughts is involved in thinking NOT-thoughts.</p>
<p>The second premise, [2], is argued for.  First, assume Dummett's view that thinking about thoughts requires thinking about their vehicles.  Then, since intentional ascent involves 'conscious cognitive access to the target thoughts', it follows that the vehicles of these thoughts must be accessible to consciousness (159).  Finally, Berm&uacute;dez argues that 'public language sentences are the only possible ... vehicles for thoughts that are to be the objects of reflexive thinking', and he does this by eliminating rival candidates (160).  One problem with this argument is that since, as Berm&uacute;dez says, 'we have little idea of what the vehicle of nonlinguistic thought might be' (192), he can hardly have eliminated this particular vehicle or have shown that a thought's having this vehicle would not permit it to be an object of reflexive thinking.  Another problem with arguing by elimination is that it sheds no light on why the conclusion is true.  If we really can think about a thought only when we have somehow expressed it in a sentence that we use to communicate with, I'd like to know why.  An argument from elimination leaves it mysterious why there should be a link between sentences we use for communication and thoughts we can think about.  In short, then, much more needs to be said to explicate and defend the two premises of Berm&uacute;dez' main argument relating thought and language.</p>
<p>The project of understanding early hominid, infant and animal thought is surely interesting and important, but I don't yet see what their lack of language has to do with it.  A second way of relating thought and language is implicit in Thinking without Words.  Berm&uacute;dez remarks that '[w]e have no theoretical framework for understanding the content and nature of nonlinguistic thought' (vii).  Does this mean that we do have a theoretical framework for understanding language-users' thoughts?  In Chapter 2, Berm&uacute;dez gives the impression that we do, and that Gotlob Frege and Michael Dummett have together succeeded in providing it.  The key to their theory is the claim that to grasp a thought is to understand a sentence which expresses it.  This theory doesn't work for the languageless, Berm&uacute;dez claims, because 'no account has yet been given of what it might be to grasp a thought independently of understanding a sentence' (19).  I read Berm&uacute;dez as implying that thinking without words is especially problematic because we have an adequate conception of thinking which applies only to language-users.  But do we?  Let's concede that grasping a thought is understanding a sentence which expresses it.  Now we need to know what constitutes understanding a sentence.  And of course no one has got very far with explaining this-or, at least, no one has got further with this than they have with explaining what it is to grasp a thought.  Why not?  Probably because understanding a sentence involves grasping the thought it expresses.  An animal's having a language would make it easier to understand the nature of its thinking only if we knew what it is to understand a sentence; but the problem of understanding sentences turns out to be no easier than the problem of grasping thoughts.  In this respect, language-using animals' thinking is no better understood than languageless animals'.  </p>
<p>The arguments relating thought and language are quite a small part of this book.  The main part, Chapters 3-7, is a systematic attempt to describe a kind of thought and an associated kind of thinking, and to relate this theoretical description to a wide variety of experimental research.  Even if Berm&uacute;dez hasn't fully explained what this kind of thinking has to do with language, the more important question is surely whether he's right about how any of his subjects-animals, infants and early hominids-think.  On this point the book offers many valuable insights.  I'm about to recommend it to you as a tremendous resource which takes in an enormous range of experimental work and is full of ideas about the nature of thought.  But first I want to raise another objection.</p>
<p>Berm&uacute;dez describes his book as a first step towards providing 'a conceptual foundation for the various disciplines that are committed to giving psychological explanations of the behavior of nonlinguistic creatures' (193).  Talk of foundations is misleading because Berm&uacute;dez' position is in part an error theory.  You wouldn't know it from reading this book, but an important cluster of theories in developmental psychology are deeply committed to the hypothesis that infants think and reason in just the ways that adults do.  Take Ren&eacute;e Baillargeon's research on infants' understanding of how objects behave.  The presentation of her research in Thinking without Words is selective; Berm&uacute;dez says only that Baillargeon has shown that infants are 'sensitive' to certain higher-order principles about the ways objects behave (54, 78).  But Baillargeon, like other researchers, is not content to describe infant cognition using uninformative notions like 'sensitivity'.  She aims for a deeper understanding of what happens when infants perceive objects.  Her experiments are designed to show that 'infants, like older learners, formulate rules or hypotheses about physical events and revise and elaborate these hypotheses in light of additional input' (Aguiar and Baillargeon, 'Developments in Young Infants' Reasoning About Occluded Objects', Cognitive Psychology, 2002, 45:267-336, p. 329).  According to Berm&uacute;dez, however, infants are incapable of testing or revising hypotheses; at best they are capable of 'protoinference', which is not how older learners reason.  So, far from constituting a foundation for Baillargeon's psychological explanations, Berm&uacute;dez' views are incompatible with them.  </p>
<p>Berm&uacute;dez engages with scientists' views about thought in other work (for example, in The Paradox of Self-Consciousness, Cambridge, MA: MIT Press, 1998, pp. 62-76); a deeper level of engagement would also have benefited this book.  Take infants' perceptions of objects again.  Whether and how infants think about objects is currently much debated within psychology.  There are several versions of the view that infants discover the properties of objects by formulating and testing hypotheses; there are also psychologists who deny that infants think about (as opposed to perceive) objects at all; and there are those who, like Berm&uacute;dez, hold that infants do think about objects but not in the same way that older people do.  The fact that his view conflicts with other researchers' views is part of what makes it worth studying.  </p>
<p>Thinking without Words is an accessible and fascinating book; it develops a theory of how animals, early hominids and infants think, which is supported by reference to many different areas of research as well as by philosophical arguments.  Anyone who cares about any kind of thinking should enjoy reading it.</p>
</file>

<file path="src/content/writing/seeing_causes.html.md">
---
title: Seeing Causes and Hearing Gestures
authors: Stephen A. Butterfill
year: 2009
isForthcoming: false
journal: Philosophical Quarterly
volume: "59"
number: "236"
pages: 405-428
doi: 10.1111/j.1467-9213.2008.585.x
---

## Abstract

Can humans see causal interactions? Evidence on the visual perception of causal interactions, from Michotte to contemporary work, is best interpreted as showing that we can see some causal interactions in the same sense as that in which we can hear speech. Causal perception, like speech perception, is a form of categorical perception.
</file>

<file path="src/content/writing/seeing_it_both_ways.html.md">
---
title: "Seeing It Both Ways: Using a Double-Cuing Task to Investigate the Role
  of Spatial Cuing in Level-1 Visual Perspective-Taking"
authors: John Michael, Thomas Wolf, Clément Letesson, Stephen A. Butterfill,
  Joshua Skewes Jakob Hohwy
year: 2018
isForthcoming: false
journal: "Journal of Experimental Psychology: Human Perception and Performance"
volume: "44"
number: "5"
pages: 693-702
doi: 10.1037/xhp0000486
---

## Abstract

Previous research using the dot-perspective task has produced evidence that humans may be equipped with a mechanism that spontaneously tracks others’ gaze direction and thereby acquires information about what they can see. Other findings, however, support the alternative hypothesis that a spatial-cuing mechanism underpins the effect observed in the dot-perspective task. To adjudicate between these hypotheses, we developed a double-cuing version of Posner’s (1980) spatial-cuing paradigm to be implemented in the dot-perspective task, and conducted 3 experiments in which we manipulated stimulus-onset asynchrony, as well as secondary task demands. Crucially, the 2 conflicting hypotheses generated divergent patterns of predictions across these experimental conditions. Our results support the hypothesis of an automatic perspective-taking mechanism.
</file>

<file path="src/content/writing/taking_apart.html.md">
---
title: "Taking Apart What Brings Us Together: The Role of Action Prediction,
  Perspective-Taking, and Theory of Mind in Joint Action"
authors: Lucia Maria Sacheli, Elisa Arcangeli, Desiré Carioti, Stephen A.
  Butterfill and  Manuela Berlingeri
year: 2022
isForthcoming: false
journal: Quarterly Journal of Experimental Psychology
volume: "75"
number: "7"
pages: 1228-1243
doi: 10.1177/17470218211050198
---

## Abstract

The ability to act together with others to achieve common goals is crucial in life, yet there is no full consensus on the underlying cognitive skills. While influential theoretical accounts suggest that interaction requires sophisticated insights into others’ minds, alternative views propose that high-level social skills might not be necessary because interactions are grounded on sensorimotor predictive mechanisms. At present, empirical evidence is insufficient to decide between the two. This study addressed this issue and explored the association between performance at joint action tasks and cognitive abilities in three domains—action prediction, perspective-taking, and theory of mind—in healthy adults (N = 58). We found that, while perspective-taking played a role in reading the behaviour of others independently of the social context, action prediction abilities specifically influenced the agents’ performance in an interactive task but not in a control (social but non-interactive) task. In our study, performance at a theory of mind test did not play any role, as confirmed by Bayesian analyses. The results suggest that, in adults, sensorimotor predictive mechanisms might play a significant and specific role in supporting interpersonal coordination during motor interactions. We discuss the implications of our findings for the contrasting theoretical views described earlier and propose a way they might be partly reconciled.
</file>

<file path="src/content/writing/tool_use_book.html.md">
---
title: Tool Use and Causal Cognition
authors: Teresa McCormack, Christoph Hoerl and Stephen A. Butterfill (eds)
year: 2012
isForthcoming: false
doi: 10.1093/acprof:oso/9780199571154.001.0001
---


<h3>Description</h3>
<p>A collection of essays by philosophers and cognitive scientists on tool use and causal cognition.</p>
<h3>Contents</h3>
<ol>
  <li>Teresa McCormack, Christoph Hoerl, & Stephen A. Butterfill: Tool Use and Causal Cognition: An Introduction</li>
  <li>Jim Woodward: A Philosopher Looks at Tool Use and Causal Understanding</li>
  <li>Melissa L. Greif & Amy Needham: The Development of Tool Use Early in Life</li>
  <li>Daniel Povinelli & Derek C. Penn: Through a Floppy Tool Darkly: Toward a Conceptual Overthrow of Animal Alchemy</li>
  <li>Amanda Seed, Daniel Hanus, & Josep Call: Causal Knowledge in Corvids, Primates and Children: More Than Meets the Eye?</li>
  <li>Brian J. Edwards, Benjamin M. Rottman, & Laurie R. Santos: The Evolutionary Origins of Causal Cognition: Learning and Using Causal Structures</li>
  <li>Teresa McCormack & Christoph Hoerl: Tool Use, Planning, and Future Thinking in Children and Animals</li>
  <li>Christopher Peacocke: Representing Causality</li>
  <li>John Campbell: Why Do Language and Tool Use Both Count as Manifestations of Intelligence?</li>
  <li>Georg Goldenberg: Effects of brain damage on human tool use</li>
  <li>Lucilla Cardinali, Claudio Brozzoli, Francesca Frassinetti, Alice C. Roy, Alessandro Farn&egrave;: Human tool-use: a causal role in plasticity of bodily and spatial representations</li>
  <li>Charles Spence: Tool-use and the representation of peripersonal space in humans</li>
</ol>
</file>

<file path="src/content/writing/tracking_representing_mental_states.html.md">
---
title: Tracking and Representing Others’ Mental States
authors: Stephen A. Butterfill
year: 2017
isForthcoming: false
booktitle: Routledge Companion to the Philosophy of Animal Minds
pages: 269–279
pdfUrl: /pdf/tracking_representing_mental_states.pdf
---

## Abstract

Few things matter more than the mental states of those nearby. Their ignorance defines
limits on cooperation and presents opportunities to exploit in competition. (If she’s seen
where you stashed those mealworms she’ll pilfer them when you’re gone, leaving you without
breakfast. And you won’t get that grape if he hears you sneaking past.) What others feel,
see and know can also provide information about events otherwise beyond your ken. It’s no
surprise, then, that abilities to track others’ mental states are widespread. Many animals
including scrub jays (Clayton, Dally and Emery 2007), ravens (Bugnyar, Reber and Buckner
2016), goats (Kaminski, Call and Tomasello 2006), dogs (Kaminski et al. 2009), ringtailed
lemurs (Sandel, MacLean and Hare 2011), monkeys (Burkart and Heschl 2007; Hattori,
Kuroshima and Fujita 2009) and chimpanzees (Melis, Call and Tomasello 2006; Karg et al.
2015) reliably vary their actions in ways that are appropriate given facts about another’s
mental states. What underpins such abilities to track others’ mental states?
</file>

<file path="src/content/writing/two_kinds_of_purposive_action.html.md">
---
title: Two Kinds of Purposive Action
authors: Stephen A. Butterfill
year: 2001
isForthcoming: false
journal: European Journal of Philosophy
volume: "9"
number: "2"
pages: 141-165
doi: 10.1111/1468-0378.00133
---

## Abstract

It is normally assumed that there is only one kind of purposive action. This article argues that there are two kinds of purposive action, which require different models of explanation. One kind of action is done without awareness of reasons; another kind of action is done because the agent is aware of reasons for that action. The argument starts by noting that philosophers disagree about what explains action. Some claim that actions are explained by impersonal facts, such as facts about how things should be or have been historically (e.g. Millikan, Stout). Others claim that actions are explained by mental states, such as beliefs and desires (e.g. Davidson, Velleman). These philosophers are usually regarded as offering conflicting accounts of one thing. However, they are best understood as giving accounts of different models of action-explanation. Neither model fits every case, so there are at least two kinds of purposive action.
</file>

<file path="src/content/writing/two_systems_goal_ascription.html.md">
---
title: Is Goal Ascription Possible in Minimal Mindreading? [Reply to Michael and
  Christensen]
authors: Stephen A. Butterfill and Ian A. Apperly
year: 2016
isForthcoming: false
journal: Psychological Review
volume: "123"
number: "2"
pages: 228-233
doi: 10.1037/rev0000022
pdfUrl: /pdf/two_systems_goal_ascription.pdf
---

## Abstract

In this response to the commentary by Michael and Christensen, 
we first explain how minimal mindreading is compatible with the 
development of increasingly sophisticated mindreading behaviours 
that involve both executive functions and general knowledge, and then 
sketch one approach to a minimal account of goal ascription.
</file>

<file path="src/content/writing/two_systems.html.md">
---
title: Do Humans Have Two Systems to Track Beliefs and Belief-like States?
authors: Ian A. Apperly and Stephen A. Butterfill
year: 2009
isForthcoming: false
journal: Psychological Review
volume: "116"
number: "4"
pages: 953-970
doi: 10.1037/a0016923
---

## Abstract

The lack of consensus on how to characterize humans' capacity for belief reasoning has been brought into sharp focus by recent research. Children fail critical tests of belief reasoning before 3 to 4 years of age (H. Wellman, D. Cross, & J. Watson, 2001; H. Wimmer & J. Perner, 1983), yet infants apparently pass false-belief tasks at 13 or 15 months (K. H. Onishi & R. Baillargeon, 2005; L. Surian, S. Caldi, & D. Sperber, 2007). Nonhuman animals also fail critical tests of belief reasoning but can show very complex social behavior (e.g., J. Call & M. Tomasello, 2005). Fluent social interaction in adult humans implies efficient processing of beliefs, yet direct tests suggest that belief reasoning is cognitively demanding, even for adults (e.g., I. A. Apperly, D. Samson, & G. W. Humphreys, 2009). The authors interpret these findings by drawing an analogy with the domain of number cognition, where similarly contrasting results have been observed. They propose that the success of infants and nonhuman animals on some belief reasoning tasks may be best explained by a cognitively efficient but inflexible capacity for tracking belief-like states. In humans, this capacity persists in parallel with a later-developing, more flexible but more cognitively demanding theory-of-mind abilities.
</file>

<file path="src/layouts/EmptyLayout.astro">
---
// src/layouts/EmptyLayout.astro
// this is used for hashme-q3 etc (which do not have any styles, toobar etc)


interface Props {
  title: string;
}

const { title } = Astro.props;
---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <title>{title}</title>
  </head>
  <body>
    <slot /> <!-- Page content will be injected here -->
  </body>
</html>
</file>

<file path="src/lib/components/ui/command/command-dialog.svelte">
<script lang="ts">
	import type { Command as CommandPrimitive, Dialog as DialogPrimitive } from "bits-ui";
	import type { Snippet } from "svelte";
	import Command from "./command.svelte";
	import * as Dialog from "$lib/components/ui/dialog/index.js";
	import type { WithoutChildrenOrChild } from "$lib/utils.js";

	let {
		open = $bindable(false),
		ref = $bindable(null),
		value = $bindable(""),
		title = "Command Palette",
		description = "Search for a command to run",
		portalProps,
		children,
		...restProps
	}: WithoutChildrenOrChild<DialogPrimitive.RootProps> &
		WithoutChildrenOrChild<CommandPrimitive.RootProps> & {
			portalProps?: DialogPrimitive.PortalProps;
			children: Snippet;
			title?: string;
			description?: string;
		} = $props();
</script>

<Dialog.Root bind:open {...restProps}>
	<Dialog.Header class="sr-only">
		<Dialog.Title>{title}</Dialog.Title>
		<Dialog.Description>{description}</Dialog.Description>
	</Dialog.Header>
	<Dialog.Content class="overflow-hidden p-0" {portalProps}>
		<Command
			class="**:data-[slot=command-input-wrapper]:h-12 [&_[data-command-group]:not([hidden])_~[data-command-group]]:pt-0 [&_[data-command-group]]:px-2 [&_[data-command-input-wrapper]_svg]:h-5 [&_[data-command-input-wrapper]_svg]:w-5 [&_[data-command-input]]:h-12 [&_[data-command-item]]:px-2 [&_[data-command-item]]:py-3 [&_[data-command-item]_svg]:h-5 [&_[data-command-item]_svg]:w-5"
			{...restProps}
			bind:value
			bind:ref
			{children}
		/>
	</Dialog.Content>
</Dialog.Root>
</file>

<file path="src/lib/components/ui/command/command-link-item.svelte">
<script lang="ts">
	import { Command as CommandPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: CommandPrimitive.LinkItemProps = $props();
</script>

<CommandPrimitive.LinkItem
	bind:ref
	data-slot="command-item"
	class={cn(
		"aria-selected:bg-accent aria-selected:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground outline-hidden relative flex cursor-default select-none items-center gap-2 rounded-sm px-2 py-1.5 text-sm data-[disabled=true]:pointer-events-none data-[disabled=true]:opacity-50 [&_svg:not([class*='size-'])]:size-4 [&_svg]:pointer-events-none [&_svg]:shrink-0",
		className
	)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/command/command-list.svelte">
<script lang="ts">
	import { Command as CommandPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: CommandPrimitive.ListProps = $props();
</script>

<CommandPrimitive.List
	bind:ref
	data-slot="command-list"
	class={cn("max-h-[300px] scroll-py-1 overflow-y-auto overflow-x-hidden", className)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/command/command-separator.svelte">
<script lang="ts">
	import { Command as CommandPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: CommandPrimitive.SeparatorProps = $props();
</script>

<CommandPrimitive.Separator
	bind:ref
	data-slot="command-separator"
	class={cn("bg-border -mx-1 h-px", className)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/command/command-shortcut.svelte">
<script lang="ts">
	import { cn, type WithElementRef } from "$lib/utils.js";
	import type { HTMLAttributes } from "svelte/elements";

	let {
		ref = $bindable(null),
		class: className,
		children,
		...restProps
	}: WithElementRef<HTMLAttributes<HTMLSpanElement>> = $props();
</script>

<span
	bind:this={ref}
	data-slot="command-shortcut"
	class={cn("text-muted-foreground ml-auto text-xs tracking-widest", className)}
	{...restProps}
>
	{@render children?.()}
</span>
</file>

<file path="src/lib/components/ui/command/index.ts">
import { Command as CommandPrimitive } from "bits-ui";

import Root from "./command.svelte";
import Dialog from "./command-dialog.svelte";
import Empty from "./command-empty.svelte";
import Group from "./command-group.svelte";
import Item from "./command-item.svelte";
import Input from "./command-input.svelte";
import List from "./command-list.svelte";
import Separator from "./command-separator.svelte";
import Shortcut from "./command-shortcut.svelte";
import LinkItem from "./command-link-item.svelte";

const Loading = CommandPrimitive.Loading;

export {
	Root,
	Dialog,
	Empty,
	Group,
	Item,
	LinkItem,
	Input,
	List,
	Separator,
	Shortcut,
	Loading,
	//
	Root as Command,
	Dialog as CommandDialog,
	Empty as CommandEmpty,
	Group as CommandGroup,
	Item as CommandItem,
	LinkItem as CommandLinkItem,
	Input as CommandInput,
	List as CommandList,
	Separator as CommandSeparator,
	Shortcut as CommandShortcut,
	Loading as CommandLoading,
};
</file>

<file path="src/lib/components/ui/dialog/dialog-close.svelte">
<script lang="ts">
	import { Dialog as DialogPrimitive } from "bits-ui";

	let { ref = $bindable(null), ...restProps }: DialogPrimitive.CloseProps = $props();
</script>

<DialogPrimitive.Close bind:ref data-slot="dialog-close" {...restProps} />
</file>

<file path="src/lib/components/ui/dialog/dialog-description.svelte">
<script lang="ts">
	import { Dialog as DialogPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: DialogPrimitive.DescriptionProps = $props();
</script>

<DialogPrimitive.Description
	bind:ref
	data-slot="dialog-description"
	class={cn("text-muted-foreground text-sm", className)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/dialog/dialog-footer.svelte">
<script lang="ts">
	import { cn, type WithElementRef } from "$lib/utils.js";
	import type { HTMLAttributes } from "svelte/elements";

	let {
		ref = $bindable(null),
		class: className,
		children,
		...restProps
	}: WithElementRef<HTMLAttributes<HTMLDivElement>> = $props();
</script>

<div
	bind:this={ref}
	data-slot="dialog-footer"
	class={cn("flex flex-col-reverse gap-2 sm:flex-row sm:justify-end", className)}
	{...restProps}
>
	{@render children?.()}
</div>
</file>

<file path="src/lib/components/ui/dialog/dialog-header.svelte">
<script lang="ts">
	import type { HTMLAttributes } from "svelte/elements";
	import { cn, type WithElementRef } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		children,
		...restProps
	}: WithElementRef<HTMLAttributes<HTMLDivElement>> = $props();
</script>

<div
	bind:this={ref}
	data-slot="dialog-header"
	class={cn("flex flex-col gap-2 text-center sm:text-left", className)}
	{...restProps}
>
	{@render children?.()}
</div>
</file>

<file path="src/lib/components/ui/dialog/dialog-title.svelte">
<script lang="ts">
	import { Dialog as DialogPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: DialogPrimitive.TitleProps = $props();
</script>

<DialogPrimitive.Title
	bind:ref
	data-slot="dialog-title"
	class={cn("text-lg font-semibold leading-none", className)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/dialog/dialog-trigger.svelte">
<script lang="ts">
	import { Dialog as DialogPrimitive } from "bits-ui";

	let { ref = $bindable(null), ...restProps }: DialogPrimitive.TriggerProps = $props();
</script>

<DialogPrimitive.Trigger bind:ref data-slot="dialog-trigger" {...restProps} />
</file>

<file path="src/lib/components/ui/dialog/index.ts">
import { Dialog as DialogPrimitive } from "bits-ui";

import Title from "./dialog-title.svelte";
import Footer from "./dialog-footer.svelte";
import Header from "./dialog-header.svelte";
import Overlay from "./dialog-overlay.svelte";
import Content from "./dialog-content.svelte";
import Description from "./dialog-description.svelte";
import Trigger from "./dialog-trigger.svelte";
import Close from "./dialog-close.svelte";

const Root = DialogPrimitive.Root;
const Portal = DialogPrimitive.Portal;

export {
	Root,
	Title,
	Portal,
	Footer,
	Header,
	Trigger,
	Overlay,
	Content,
	Description,
	Close,
	//
	Root as Dialog,
	Title as DialogTitle,
	Portal as DialogPortal,
	Footer as DialogFooter,
	Header as DialogHeader,
	Trigger as DialogTrigger,
	Overlay as DialogOverlay,
	Content as DialogContent,
	Description as DialogDescription,
	Close as DialogClose,
};
</file>

<file path="src/lib/utils.ts">
import { clsx, type ClassValue } from "clsx";
import { twMerge } from "tailwind-merge";

export function cn(...inputs: ClassValue[]) {
	return twMerge(clsx(inputs));
}

// eslint-disable-next-line @typescript-eslint/no-explicit-any
export type WithoutChild<T> = T extends { child?: any } ? Omit<T, "child"> : T;
// eslint-disable-next-line @typescript-eslint/no-explicit-any
export type WithoutChildren<T> = T extends { children?: any } ? Omit<T, "children"> : T;
export type WithoutChildrenOrChild<T> = WithoutChildren<WithoutChild<T>>;
export type WithElementRef<T, U extends HTMLElement = HTMLElement> = T & { ref?: U | null };
</file>

<file path="src/pages/talks/[...slug].astro">
---
// src/pages/talks/[...slug].astro
import { getCollection } from 'astro:content';
import PageLayout from '../../layouts/PageLayout.astro';
import Slides from '../../components/Slides.astro'; // Import the new component

// This function runs at build time to find all talks
// and tell Astro to create a page for each one.
export async function getStaticPaths() {
  const talkEntries = await getCollection('talks');
  return talkEntries.map(entry => ({
    params: { slug: entry.slug },
    props: { entry },
  }));
}

import CopyForChat from '../../components/CopyForChat.svelte';

const { entry } = Astro.props;
const { Content } = await entry.render();

// Format the content specifically for the clipboard
// Use the raw markdown body instead of the rendered Content component
const textForLLM = `
Title: ${entry.data.title}
Authors: ${entry.data.authors}
Year: ${entry.data.pubDate ? new Date(entry.data.pubDate).getFullYear() : 'Unknown'}

---

${entry.body}
`.trim();
---
<PageLayout title={entry.data.title}>
  <div class="flex justify-end mb-4">
    <CopyForChat contentToCopy={textForLLM} client:load />
  </div>
  
  {/* ... (TODO: code for talk metadata) ... */}

  <!-- Conditionally render the Slides component -->
  {entry.data.slideImages && entry.data.slideImages.length > 0 && (
    <div class="my-8">
      <h2 class="text-2xl font-bold mb-4">Slides</h2>
      <Slides images={entry.data.slideImages} />
    </div>
  )}

  <Content />
</PageLayout>
</file>

<file path="src/pages/hashme-q3.astro">
---
import EmptyLayout from '../layouts/EmptyLayout.astro';
---
<EmptyLayout title="q3-hashme Utility">

    <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 600px;
      margin: 0 auto;
      padding: 20px;
      font-size: 16px;
    }
    h1 {
      font-size: 1.8rem;
      margin-bottom: 1.5rem;
      text-align: center;
    }
    form {
      margin-bottom: 20px;
    }
    .input-group {
      margin-bottom: 15px;
      position: relative;
    }
    input {
      padding: 12px;
      width: 100%;
      border: 1px solid #ccc;
      border-radius: 4px;
      box-sizing: border-box;
      font-size: 16px; /* Prevents zoom on iOS */
    }
    input:focus {
      outline: none;
      border-color: #4a90e2;
      box-shadow: 0 0 0 2px rgba(74, 144, 226, 0.2);
    }
    label {
      display: block;
      margin-bottom: 5px;
      font-weight: bold;
    }
    #result {
      font-weight: bold;
      cursor: pointer;
      padding: 12px;
      border: 1px dashed #ccc;
      border-radius: 4px;
      display: block;
      width: 100%;
      box-sizing: border-box;
      text-align: center;
      background-color: #f9f9f9;
      transition: background-color 0.2s;
      word-break: break-all;
      margin-top: 5px;
    }
    #result:hover {
      background-color: #f0f0f0;
    }
    #result:active {
      background-color: #e0e0e0;
    }
    .result-container {
      margin-top: 25px;
    }
    .result-label {
      display: block;
      margin-bottom: 5px;
      font-weight: bold;
    }
    .green {
      color: #2ecc71;
    }
    .red {
      color: #e74c3c;
    }
    .note {
      font-size: 0.85em;
      color: #666;
      margin-top: 25px;
      background-color: #f9f9f9;
      padding: 15px;
      border-radius: 4px;
      border-left: 4px solid #4a90e2;
    }
    .note p {
      margin: 0.5em 0;
    }
    
    /* Media queries for responsive design */
    @media (max-width: 480px) {
      body {
        padding: 15px;
      }
      h1 {
        font-size: 1.5rem;
      }
      input {
        padding: 10px;
      }
      .note {
        padding: 10px;
      }
    }
  </style>


  <!-- Paste the old HTML form structure here -->
  <h1>q3-hashme</h1>
  
  <form>
    <div class="input-group">
      <label for="pw1">Master Password:</label>
      <input id="pw1" type="password" autocomplete="off" placeholder="Enter your master password" />
    </div>
    
    <div class="input-group">
      <label for="dom1">Domain:</label>
      <input id="dom1" type="text" placeholder="e.g., example.com" />
    </div>
    
    <div class="input-group">
      <label for="pw2">Confirm Password:</label>
      <input id="pw2" type="password" autocomplete="off" placeholder="Re-enter your master password" />
    </div>
    
    <div class="input-group">
      <label for="dom2">Confirm Domain:</label>
      <input id="dom2" type="text" placeholder="Re-enter the domain" />
    </div>
  </form>
  
  <div class="result-container">
    <div>
      <span class="result-label">Generated Password:</span>
      <div id="result">Enter details above</div>
    </div>
    <div style="margin-top: 15px;">
      <span class="result-label">Inputs Match:</span>
      <span id="match" style="font-weight: bold;">false</span>
    </div>
  </div>
  
  <div class="note">
    <p>Click on the generated password to copy it to clipboard.</p>
    <p>Note: This tool works entirely in your browser. Your master password is never sent to any server.</p>
  </div>

  <!-- Import the q3.js module -->
  <script type="module">
    // Import the go function from q3.js (from public directory)
    import generatePassword from '/q3-browser.js';
    
    // Make the function available globally for the UI code
    window.q3_generate_password = async function(masterPassword, domain) {
      try {
        return await generatePassword(domain, masterPassword);
      } catch (error) {
        console.error('Error generating password:', error);
        return 'Error generating password';
      }
    };

    // UI functionality - define and initialize immediately
    function initUI() {
      function match() {
        var pw1 = document.getElementById('pw1').value;
        var pw2 = document.getElementById('pw2').value;
        var dom1 = document.getElementById('dom1').value;
        var dom2 = document.getElementById('dom2').value;
        return (pw1 === pw2) && (dom1 === dom2);
      }
      
      function match_display() {
        var matches = match();
        document.getElementById('match').innerHTML = matches;
        if (matches) {
          document.getElementById('match').className = 'green';
          document.getElementById('result').className = '';
        } else {
          document.getElementById('match').className = 'red';
          document.getElementById('result').className = 'red';
        }
      }
      
      // Generate and display the password
      async function result_display() {
        var pw1 = document.getElementById('pw1').value;
        var pw2 = document.getElementById('pw2').value;
        var dom1 = document.getElementById('dom1').value;
        var dom2 = document.getElementById('dom2').value;
        
        var condition = match();
        condition = condition || (dom2 === '' && pw2 === '');
        
        if (condition && pw1 && dom1) {
          try {
            const generatedPassword = await window.q3_generate_password(pw1, dom1);
            document.getElementById('result').innerHTML = generatedPassword;
          } catch (error) {
            document.getElementById('result').innerHTML = 'Error';
            console.error(error);
          }
        } else if (!pw1 || !dom1) {
          document.getElementById('result').innerHTML = 'Enter details above';
        } else {
          document.getElementById('result').innerHTML = '--- match inputs first! ---';
        }
      }
      
      async function update() {
        match_display();
        await result_display();
      }
      
      // Copy password to clipboard
      function copy_pw() {
        var res = document.getElementById('result').innerHTML;
        if (res && res !== 'Enter details above' && res !== '--- match inputs first! ---' && res !== 'Error') {
          // Try to use the modern clipboard API
          if (navigator.clipboard && navigator.clipboard.writeText) {
            navigator.clipboard.writeText(res)
              .then(() => {
                showCopiedFeedback();
              })
              .catch(err => {
                fallbackCopyToClipboard(res);
              });
          } else {
            fallbackCopyToClipboard(res);
          }
        }
      }
      
      // Fallback method for copying to clipboard
      function fallbackCopyToClipboard(text) {
        // Create a temporary textarea element
        const textArea = document.createElement('textarea');
        textArea.value = text;
        
        // Make it non-visible
        textArea.style.position = 'fixed';
        textArea.style.opacity = '0';
        textArea.style.left = '0';
        textArea.style.top = '0';
        
        document.body.appendChild(textArea);
        
        // Handle iOS devices specifically
        if (navigator.userAgent.match(/ipad|ipod|iphone/i)) {
          // Save current selection
          const editable = textArea.contentEditable;
          const readOnly = textArea.readOnly;
          
          textArea.contentEditable = true;
          textArea.readOnly = false;
          
          // Create a range and select the text
          const range = document.createRange();
          range.selectNodeContents(textArea);
          
          const selection = window.getSelection();
          selection.removeAllRanges();
          selection.addRange(range);
          
          textArea.setSelectionRange(0, 999999);
          
          // Restore contentEditable and readOnly states
          textArea.contentEditable = editable;
          textArea.readOnly = readOnly;
        } else {
          textArea.select();
        }
        
        try {
          document.execCommand('copy');
          showCopiedFeedback();
        } catch (err) {
          console.error('Failed to copy text: ', err);
          alert('Could not copy password. Please long-press and copy manually.');
        }
        
        document.body.removeChild(textArea);
      }
      
      // Show visual feedback that the password was copied
      function showCopiedFeedback() {
        const originalText = document.getElementById('result').innerHTML;
        const resultElement = document.getElementById('result');
        
        resultElement.innerHTML = 'Copied!';
        resultElement.style.backgroundColor = '#d4edda';
        resultElement.style.color = '#155724';
        resultElement.style.borderColor = '#c3e6cb';
        
        setTimeout(() => {
          resultElement.innerHTML = originalText;
          resultElement.style.backgroundColor = '';
          resultElement.style.color = '';
          resultElement.style.borderColor = '';
        }, 1500);
      }
      
      // Configure event listeners
      document.getElementById('pw1').addEventListener('input', update);
      document.getElementById('pw2').addEventListener('input', update);
      document.getElementById('dom1').addEventListener('input', update);
      document.getElementById('dom2').addEventListener('input', update);
      
      // Configure click for password copying
      document.getElementById('result').addEventListener('click', copy_pw);
      
      // Initial update
      update();
    }

    // Initialize the UI now that everything is loaded
    initUI();
  </script>
</EmptyLayout>
</file>

<file path=".gitignore">
# build output
dist/
# generated types
.astro/

# dependencies
node_modules/

# logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*


# environment variables
.env
.env.production

# macOS-specific files
.DS_Store

# jetbrains setting folder
.idea/
</file>

<file path="components.json">
{
	"$schema": "https://shadcn-svelte.com/schema.json",
	"tailwind": {
		"css": "src/styles/global.css",
		"baseColor": "slate"
	},
	"aliases": {
		"components": "$lib/components",
		"utils": "$lib/utils",
		"ui": "$lib/components/ui",
		"hooks": "$lib/hooks",
		"lib": "$lib"
	},
	"typescript": true,
	"registry": "https://shadcn-svelte.com/registry"
}
</file>

<file path="README.md">
# Astro Starter Kit: Minimal

```sh
npm create astro@latest -- --template minimal
```

> 🧑‍🚀 **Seasoned astronaut?** Delete this file. Have fun!

## 🚀 Project Structure

Inside of your Astro project, you'll see the following folders and files:

```text
/
├── public/
├── src/
│   └── pages/
│       └── index.astro
└── package.json
```

Astro looks for `.astro` or `.md` files in the `src/pages/` directory. Each page is exposed as a route based on its file name.

There's nothing special about `src/components/`, but that's where we like to put any Astro/React/Vue/Svelte/Preact components.

Any static assets, like images, can be placed in the `public/` directory.

## 🧞 Commands

All commands are run from the root of the project, from a terminal:

| Command                   | Action                                           |
| :------------------------ | :----------------------------------------------- |
| `npm install`             | Installs dependencies                            |
| `npm run dev`             | Starts local dev server at `localhost:4321`      |
| `npm run build`           | Build your production site to `./dist/`          |
| `npm run preview`         | Preview your build locally, before deploying     |
| `npm run astro ...`       | Run CLI commands like `astro add`, `astro check` |
| `npm run astro -- --help` | Get help using the Astro CLI                     |

## 👀 Want to learn more?

Feel free to check [our documentation](https://docs.astro.build) or jump into our [Discord server](https://astro.build/chat).
</file>

<file path="svelte.config.js">
import { vitePreprocess } from '@astrojs/svelte';

export default {
	preprocess: vitePreprocess(),
}
</file>

<file path="tailwind.config.mjs">
/** @type {import('tailwindcss').Config} */
export default {
  darkMode: 'class',
  content: ['./src/**/*.{astro,html,js,jsx,md,mdx,svelte,ts,tsx,vue}'],
  theme: {
    extend: {},
  },
  plugins: [],
}
</file>

<file path="public/_redirects">
/talks/2012/cuny_2011.html.html    /talks/2012/cuny_2011.html/    301
/talks/2012/dubrovnik_2011.html.html    /talks/2012/dubrovnik_2011.html/    301
/talks/2012/glasgow_2012.html.html    /talks/2012/glasgow_2012.html/    301
/talks/2012/heidelberg_2011.html.html    /talks/2012/heidelberg_2011.html/    301
/talks/2012/how_to_construct_cooperative_agents.html.html    /talks/2012/how_to_construct_cooperative_agents.html/    301
/talks/2012/intention_and_motor_representation_cambridge.html.html    /talks/2012/intention_and_motor_representation_cambridge.html/    301
/talks/2012/intention_and_motor_representation_paris.html.html    /talks/2012/intention_and_motor_representation_paris.html/    301
/talks/2012/interacting_mindreaders.html.html    /talks/2012/interacting_mindreaders.html/    301
/talks/2012/joint_action_and_the_emergence_of_mindreading.html.html    /talks/2012/joint_action_and_the_emergence_of_mindreading.html/    301
/talks/2012/lyon_2012.html.html    /talks/2012/lyon_2012.html/    301
/talks/2012/milan_2011.html.html    /talks/2012/milan_2011.html/    301
/talks/2012/minimal_theory_of_mind_cambridge.html.html    /talks/2012/minimal_theory_of_mind_cambridge.html/    301
/talks/2012/motor_representation_shared_intention.html.html    /talks/2012/motor_representation_shared_intention.html/    301
/talks/2012/nijmegen_2011.html.html    /talks/2012/nijmegen_2011.html/    301
/talks/2012/paris_2011.html.html    /talks/2012/paris_2011.html/    301
/talks/2012/science_festival_2011.html.html    /talks/2012/science_festival_2011.html/    301
/talks/2012/shared_agency_and_motor_representation.html.html    /talks/2012/shared_agency_and_motor_representation.html/    301
/talks/2012/shared_agency_with_parallel_planning.html.html    /talks/2012/shared_agency_with_parallel_planning.html/    301
/talks/2012/stirling_2012.html.html    /talks/2012/stirling_2012.html/    301
/talks/2012/tuebingen_2011.html.html    /talks/2012/tuebingen_2011.html/    301
/talks/2013/collective_agency_oxford.html.html    /talks/2013/collective_agency_oxford.html/    301
/talks/2013/collective_intentionality_vienna.html.html    /talks/2013/collective_intentionality_vienna.html/    301
/talks/2013/metacognition_and_mindreading.html.html    /talks/2013/metacognition_and_mindreading.html/    301
/talks/2013/motor_representation_in_joint_action.html.html    /talks/2013/motor_representation_in_joint_action.html/    301
/talks/2013/not_just_wide_but_shared.html.html    /talks/2013/not_just_wide_but_shared.html/    301
/talks/2013/perceiving_anger_and_sharing_smiles.html.html    /talks/2013/perceiving_anger_and_sharing_smiles.html/    301
/talks/2013/planning_for_collective_agency_stuttgart.html.html    /talks/2013/planning_for_collective_agency_stuttgart.html/    301
/talks/2013/planning_for_collective_agency_tuebingen.html.html    /talks/2013/planning_for_collective_agency_tuebingen.html/    301
/talks/2013/two_systems_two_theories.html.html    /talks/2013/two_systems_two_theories.html/    301
/talks/2013/varieties_of_joint_action.html.html    /talks/2013/varieties_of_joint_action.html/    301
/talks/2014/joint_action_warwick.html.html    /talks/2014/joint_action_warwick.html/    301
/talks/2014/joint_action_without_mindreading.html.html    /talks/2014/joint_action_without_mindreading.html/    301
/talks/2014/minimal_models_magdeburg.html.html    /talks/2014/minimal_models_magdeburg.html/    301
/talks/2014/naturalising_joint_actions.html.html    /talks/2014/naturalising_joint_actions.html/    301
/talks/2014/shared_agency_involves_changing_perspective_manchester.html.html    /talks/2014/shared_agency_involves_changing_perspective_manchester.html/    301
/talks/2015/antwerp_2015.html.html    /talks/2015/antwerp_2015.html/    301
/talks/2015/berlin_2015_core_knowledge.html.html    /talks/2015/berlin_2015_core_knowledge.html/    301
/talks/2015/bochum_2015.html.html    /talks/2015/bochum_2015.html/    301
/talks/2015/bupdapest_moseo.html.html    /talks/2015/bupdapest_moseo.html/    301
/talks/2015/copenhagen_2015.html.html    /talks/2015/copenhagen_2015.html/    301
/talks/2015/mindreading_espp.html.html    /talks/2015/mindreading_espp.html/    301
/talks/2015/mindreading_srcd.html.html    /talks/2015/mindreading_srcd.html/    301
/talks/2015/practical_reasoning_motor_representation_intro.html.html    /talks/2015/practical_reasoning_motor_representation_intro.html/    301
/talks/2015/purposive_action_and_intention.html.html    /talks/2015/purposive_action_and_intention.html/    301
/talks/2016/berlin_2016_2systems.html    /talks/2016/berlin_2016_2systems/    301
/talks/2016/milan_2016_core_knowledge.html.html    /talks/2016/milan_2016_core_knowledge.html/    301
/talks/2016/perceiving_mental_states_tuebingen.html.html    /talks/2016/perceiving_mental_states_tuebingen.html/    301
/teaching/joint_action_and_the_emergence.html.html    /teaching/joint_action_and_the_emergence.html/    301
/teaching/joint_action_and_the_emergence/lecture01.html.html    /teaching/joint_action_and_the_emergence/lecture01.html/    301
/teaching/joint_action_and_the_emergence/lecture02.html.html    /teaching/joint_action_and_the_emergence/lecture02.html/    301
/teaching/joint_action_and_the_emergence/lecture02b.html.html    /teaching/joint_action_and_the_emergence/lecture02b.html/    301
/teaching/joint_action_and_the_emergence/lecture03.html.html    /teaching/joint_action_and_the_emergence/lecture03.html/    301
/teaching/joint_action_and_the_emergence/lecture04.html.html    /teaching/joint_action_and_the_emergence/lecture04.html/    301
/teaching/joint_action_and_the_emergence/lecture05.html.html    /teaching/joint_action_and_the_emergence/lecture05.html/    301
/teaching/mindreading_and_joint_action.html.html    /teaching/mindreading_and_joint_action.html/    301
/teaching/mindreading_and_joint_action/lecture01.html.html    /teaching/mindreading_and_joint_action/lecture01.html/    301
/teaching/mindreading_and_joint_action/lecture02.html.html    /teaching/mindreading_and_joint_action/lecture02.html/    301
/teaching/mindreading_and_joint_action/lecture03.html.html    /teaching/mindreading_and_joint_action/lecture03.html/    301
/teaching/mindreading_and_joint_action/lecture04.html.html    /teaching/mindreading_and_joint_action/lecture04.html/    301
/teaching/mindreading_and_joint_action/lecture05.html.html    /teaching/mindreading_and_joint_action/lecture05.html/    301
/teaching/mindreading_and_joint_action/lecture06.html.html    /teaching/mindreading_and_joint_action/lecture06.html/    301
/teaching/mindreading_and_joint_action/lecture07.html.html    /teaching/mindreading_and_joint_action/lecture07.html/    301
/teaching/mindreading_and_joint_action/lecture08.html.html    /teaching/mindreading_and_joint_action/lecture08.html/    301
/teaching/mindreading_and_joint_action/lecture09.html.html    /teaching/mindreading_and_joint_action/lecture09.html/    301
/writing/awareness_of_belief.html.html    /writing/awareness_of_belief.html/    301
/writing/blueprint_social_animal.html.html    /writing/blueprint_social_animal.html/    301
/writing/bpd_social_media.html.html    /writing/bpd_social_media.html/    301
/writing/childrens_selective_learning_from_others.html.html    /writing/childrens_selective_learning_from_others.html/    301
/writing/cognitive_architecture_belief_reasoning.html.html    /writing/cognitive_architecture_belief_reasoning.html/    301
/writing/collective_goals.html.html    /writing/collective_goals.html/    301
/writing/coordinating_joint_action.html.html    /writing/coordinating_joint_action.html/    301
/writing/cue_competition_effects.html.html    /writing/cue_competition_effects.html/    301
/writing/developing_mind.html.html    /writing/developing_mind.html/    301
/writing/drawn_together.html.html    /writing/drawn_together.html/    301
/writing/fizke_limits.html.html    /writing/fizke_limits.html/    301
/writing/gaining_knowledge_via_other_minds.html.html    /writing/gaining_knowledge_via_other_minds.html/    301
/writing/goals_targets.html.html    /writing/goals_targets.html/    301
/writing/infants_representations_of_causation.html.html    /writing/infants_representations_of_causation.html/    301
/writing/intention_motor.html    /writing/intention_motor/    301
/writing/interacting_mindreaders.html    /writing/interacting_mindreaders/    301
/writing/intuitions_about_joint_commitment.html.html    /writing/intuitions_about_joint_commitment.html/    301
/writing/joint_action_development.html    /writing/joint_action_development/    301
/writing/joint_action_minimalist_approach.html.html    /writing/joint_action_minimalist_approach.html/    301
/writing/joint_action_rpp_edited_volume.html.html    /writing/joint_action_rpp_edited_volume.html/    301
/writing/joint_action_rpp_special_issue.html.html    /writing/joint_action_rpp_special_issue.html/    301
/writing/level_2.html    /writing/level_2/    301
/writing/lightbulb.html.html    /writing/lightbulb.html/    301
/writing/mindreading_balance.html.html    /writing/mindreading_balance.html/    301
/writing/minimal_architecture.html.html    /writing/minimal_architecture.html/    301
/writing/minimal_brains_discussion_intro.html.html    /writing/minimal_brains_discussion_intro.html/    301
/writing/minimal_brains_discussion_replies.html.html    /writing/minimal_brains_discussion_replies.html/    301
/writing/minimal_theory_of_mind.html.html    /writing/minimal_theory_of_mind.html/    301
/writing/modularity.html.html    /writing/modularity.html/    301
/writing/motor_representation_acting_together.html.html    /writing/motor_representation_acting_together.html/    301
/writing/motor_representation_action_experience.html.html    /writing/motor_representation_action_experience.html/    301
/writing/motor_representation_in_goal_ascription.html.html    /writing/motor_representation_in_goal_ascription.html/    301
/writing/motor_representation_skill.html.html    /writing/motor_representation_skill.html/    301
/writing/mummification.html.html    /writing/mummification.html/    301
/writing/perceiving_expressions_emotion.html    /writing/perceiving_expressions_emotion/    301
/writing/planning_for_collective_agency.html.html    /writing/planning_for_collective_agency.html/    301
/writing/primal_self.html    /writing/primal_self/    301
/writing/psychological_research_on_joint_action.html.html    /writing/psychological_research_on_joint_action.html/    301
/writing/puzzle_thought_experience_motoric.html    /writing/puzzle_thought_experience_motoric/    301
/writing/review_consciousness.html.html    /writing/review_consciousness.html/    301
/writing/review_joint_commitment.html.html    /writing/review_joint_commitment.html/    301
/writing/review_self-knowing_agents.html.html    /writing/review_self-knowing_agents.html/    301
/writing/review_the_rational_imagination.html.html    /writing/review_the_rational_imagination.html/    301
/writing/review_thinking_without_words.html.html    /writing/review_thinking_without_words.html/    301
/writing/seeing_causes.html.html    /writing/seeing_causes.html/    301
/writing/seeing_it_both_ways.html.html    /writing/seeing_it_both_ways.html/    301
/writing/taking_apart.html.html    /writing/taking_apart.html/    301
/writing/tool_use_book.html.html    /writing/tool_use_book.html/    301
/writing/tracking_representing_mental_states.html.html    /writing/tracking_representing_mental_states.html/    301
/writing/two_kinds_of_purposive_action.html.html    /writing/two_kinds_of_purposive_action.html/    301
/writing/two_systems.html.html    /writing/two_systems.html/    301
/writing/two_systems_goal_ascription.html.html    /writing/two_systems_goal_ascription.html/    301
/writing/what_does_knowledge_explain.html.html    /writing/what_does_knowledge_explain.html/    301
</file>

<file path="scripts/generate-llms.mjs">
import fs from 'fs-extra';
import path from 'path';
import yaml from 'yaml';

// Function to parse frontmatter and content from markdown files
function parseFrontmatter(content) {
  const frontmatterRegex = /^---\s*\n([\s\S]*?)\n---\s*\n([\s\S]*)$/;
  const match = content.match(frontmatterRegex);
  
  if (!match) {
    return { data: {}, body: content };
  }
  
  const [, frontmatterStr, body] = match;
  const data = yaml.parse(frontmatterStr);
  
  return { data, body };
}

// Function to recursively read all markdown files from a directory
async function readMarkdownFiles(dir) {
  const items = [];
  const entries = await fs.readdir(dir, { withFileTypes: true });
  
  for (const entry of entries) {
    const fullPath = path.join(dir, entry.name);
    if (entry.isDirectory()) {
      // Recursively read subdirectories
      const subItems = await readMarkdownFiles(fullPath);
      items.push(...subItems);
    } else if (entry.name.endsWith('.md')) {
      const content = await fs.readFile(fullPath, 'utf-8');
      const { data, body } = parseFrontmatter(content);
      items.push({ data, body, type: path.basename(dir) === 'writing' ? 'writing' : 'talk' });
    }
  }
  
  return items;
}

// Read all writing content files
const writingDir = path.join(process.cwd(), 'src/content/writing');
const writingItems = await readMarkdownFiles(writingDir);

// Read all talks content files
const talksDir = path.join(process.cwd(), 'src/content/talks');
const talkItems = await readMarkdownFiles(talksDir);

// Combine all items
const allItems = [...writingItems, ...talkItems];

let fullText = "Site: Stephen Butterfill's Personal Academic Website\nAuthor: Stephen Butterfill\n\n---\n\n";

for (const item of allItems) {
  // Use the raw markdown body
  const cleanText = item.body;

  fullText += `Title: ${item.data.title || 'Untitled'}\n`;
  fullText += `Authors: ${item.data.authors || 'Unknown'}\n`;
  
  // Handle different date/year formats for writing vs talks
  if (item.type === 'talk' && item.data.pubDate) {
    const year = new Date(item.data.pubDate).getFullYear();
    fullText += `Year: ${year}\n`;
    if (item.data.event) {
      fullText += `Event: ${item.data.event}\n`;
    }
  } else if (item.data.year) {
    fullText += `Year: ${item.data.year}\n`;
    if (item.data.journal) {
      fullText += `Journal: ${item.data.journal}\n`;
    }
  } else {
    fullText += `Year: Unknown\n`;
  }
  
  fullText += `Type: ${item.type === 'talk' ? 'Talk' : 'Publication'}\n\n`;
  fullText += `${cleanText}\n\n---\n\n`;
}

await fs.writeFile('public/llms.txt', fullText);
console.log('Successfully generated llms.txt');
</file>

<file path="src/components/CommandPalette.svelte">
<script lang="ts">
  import { onMount } from 'svelte';
  // REMOVED: import { goto } from '$app/navigation'; // This was incorrect.

  import * as Command from '$lib/components/ui/command';

  interface ContextualAction {
    label: string;
    action: string;
    url?: string;
  }

  // Props passed from Astro
  export let allPages: { url: string; title: string }[] = [];
  export let contextualActions: ContextualAction[] = [];

  // Internal state for the component
  let open = false;
  let inputValue = '';

  // Reactive statement to filter pages based on user input
  $: filteredPages = inputValue === ''
    ? allPages
    : allPages.filter(page =>
        page.title.toLowerCase().includes(inputValue.toLowerCase())
      );

  // Keyboard shortcut listener (Cmd/Ctrl + K) and custom event listener
  onMount(() => {
    const down = (e: KeyboardEvent) => {
      if (e.key === 'k' && (e.metaKey || e.ctrlKey)) {
        e.preventDefault();
        open = !open;
      }
    };

    const openFromTrigger = () => {
      open = true;
    };

    document.addEventListener('keydown', down);
    window.addEventListener('open-command-palette', openFromTrigger);
    
    return () => {
      document.removeEventListener('keydown', down);
      window.removeEventListener('open-command-palette', openFromTrigger);
    };
  });

  // Function to handle selecting a page
  function handlePageSelect(url: string) {
    // CORRECTED: Use the standard browser API for navigation.
    window.location.href = url;
    open = false;
  }

  // Function to execute a contextual action
  function handleActionSelect(action: ContextualAction) {
    switch (action.action) {
      case 'openUrl':
        if (action.url) {
          window.open(action.url, '_blank');
        }
        break;
      // Add more action types here as needed
      default:
        console.warn('Unknown action type:', action.action);
    }
    open = false;
  }
</script>

<Command.Dialog bind:open>
  <Command.Input placeholder="Type a command or search..." bind:value={inputValue} />
  <Command.List>
    <Command.Empty>No results found.</Command.Empty>

    <!-- Contextual Actions Group -->
    {#if contextualActions.length > 0}
      <Command.Group heading="Page Actions">
        {#each contextualActions as action}
          <Command.Item onSelect={() => handleActionSelect(action)}>
            <span>{action.label}</span>
          </Command.Item>
        {/each}
      </Command.Group>
    {/if}

    <!-- Pages Group -->
    <Command.Group heading="Navigate">
      {#each filteredPages.slice(0, 10) as page (page.url)}
        <Command.Item onSelect={() => handlePageSelect(page.url)}>
          <span>{page.title}</span>
        </Command.Item>
      {/each}
    </Command.Group>
  </Command.List>
</Command.Dialog>
</file>

<file path="src/components/Slides.astro">
---
// src/components/Slides.astro
interface Props {
  images: string[];
}
const { images } = Astro.props;
---
<div class="reveal" style="width: 100%; height: 600px;">
  <div class="slides">
    {images.map(src => (
      <section>
        <img src={src} alt="Slide image" style="max-width: 100%; max-height: 100%; object-fit: contain;" />
      </section>
    ))}
  </div>
</div>

<!-- We need to import the CSS and initialize the library on the client -->
<link rel="stylesheet" href="/reveal.js/dist/reveal.css">
<link rel="stylesheet" href="/reveal.js/dist/theme/black.css">

<style>
  /* Override any conflicting styles */
  .reveal {
    position: relative !important;
    display: block !important;
    width: 100% !important;
    height: 600px !important;
  }
  .reveal .slides {
    display: block !important;
  }
  .reveal .slides section {
    display: none !important;
  }
  .reveal .slides section.present {
    display: block !important;
  }
</style>

<script src="/reveal.js/dist/reveal.js" is:inline></script>
<script is:inline>
  // Wait for DOM to be ready and ensure reveal.js loads
  function initializeReveal() {
    if (window.Reveal) {
      console.log('Initializing Reveal.js...');
      let deck = new window.Reveal(document.querySelector('.reveal'), {
        embedded: true,
        width: '100%',
        height: 600,
        controls: true,
        progress: true,
        center: true,
        transition: 'slide'
      });
      deck.initialize().then(() => {
        console.log('Reveal.js initialized successfully');
      });
    } else {
      console.log('Reveal.js not loaded, retrying...');
      setTimeout(initializeReveal, 100);
    }
  }
  
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initializeReveal);
  } else {
    initializeReveal();
  }
</script>
</file>

<file path="src/content/writing/developing_mind.html.md">
---
title: The Developing Mind
authors: Stephen A. Butterfill
year: 2020
isForthcoming: false
doi: 10.4324/9780203758274
---


<html>
  <head>
    <style type="text/css">
    #page-container { /* PDF container */
      position:absolute; /* required for calculating relative positions of pages in pdf2htmlEX.js */
      top:0;
      left:0px;
      margin:0; 
      padding:0;
      border:0; /* required for lazy page loading in pdf2htmlEX.js (page visibility test) */
    }
    @media screen {
      #page-container {
        /* `bottom' and `right' are required for lazy page loading in pdf2htmlEX.js (page visibility test)
        * alternatively you may set width and height
        */
        bottom:0;
        right:0;
        overflow:auto;
      }
    }
    @media print { 
      @page { margin:0; }
      html { margin:0; }
      body { 
        margin:0; 
        -webkit-print-color-adjust:exact; /* enable printing background images for WebKit */
      }
      #sidebar { display:none; }
      #page-container {
        width:auto;
        height:auto;
        overflow:visible;
        background-color:transparent;
      }
      .d { display:none; }
    }
    /* Part 2: Page Elements: Modify with caution
    * The followings are base classes, some of which are meant to be override by PDF specific classes
    * So do not increase the specificity (e.g. ".classname" -> "#page-container .classname")
    */
    .pf { /* page */
      position:relative;
      background-color:white;
      overflow: hidden;
      margin:0; 
      border:0; /* required by pdf2htmlEX.js for page visibility test */
    }
    .pc { /* content of a page */
      position:absolute;
      border:0;
      padding:0;
      margin:0;
      top:0;
      left:0;
      width:100%;
      height:100%;
      overflow:hidden;
      display:block;
      /* set transform-origin for scaling */
      transform-origin:0% 0%;
      -ms-transform-origin:0% 0%;
      -webkit-transform-origin:0% 0%;
    }
    .pc.opened { /* used by pdf2htmlEX.js, to show/hide pages */
      display:block;
    }
    .bf { /* images that occupies the whole page */
      position:absolute;
      border:0;
      margin:0;
      top:0;
      bottom:0;
      width:100%;
      height:100%;
      -ms-user-select:none;
      -moz-user-select:none;
      -webkit-user-select:none;
      user-select:none;
    }
    .bi { /* images that cover only a part of the page */
      position:absolute;
      border:0;
      margin:0;
      -ms-user-select:none;
      -moz-user-select:none;
      -webkit-user-select:none;
      user-select:none;
    }
    @media print {
      .pf {
        margin:0;
        box-shadow:none;
        page-break-after:always;
        page-break-inside:avoid;
      }
      @-moz-document url-prefix() {
        /* fix page truncation for FireFox */
        .pf {
          overflow:visible;
          border:1px solid #FFFFFF;
        }
        .pc {overflow:visible;}
      }
    }
    .c { /* clip box */
      position:absolute;
      border:0;
      padding:0;
      margin:0;
      overflow:hidden;
      display:block;
    }
    .t { /* text line */
      position:absolute;
      white-space:pre;
      font-size:1px;
      transform-origin:0% 100%;
      -ms-transform-origin:0% 100%;
      -webkit-transform-origin:0% 100%;
      unicode-bidi:bidi-override;/* For rtl languages, e.g. Hebrew, we don't want the default Unicode behaviour */
      -moz-font-feature-settings:"liga" 0;/* We don't want Firefox to recognize ligatures */
    }
    .t:after { /* webkit #35443 */
      content: '';
    }
    .t:before { /* Workaround Blink(up to 41)/Webkit bug of word-spacing with leading spaces (chromium #404444 and pdf2htmlEX #412) */
      content: '';
      display: inline-block;
    }
    .t span { /* text blocks within a line */
      /* Blink(up to 41)/Webkit have bug with negative word-spacing and inline-block (pdf2htmlEX #416), so keep normal span inline. */
      position:relative;
      unicode-bidi:bidi-override; /* For rtl languages, e.g. Hebrew, we don't want the default Unicode behaviour */
    }
    ._ { /* text shift */
      /* Blink(up to 41)/Webkit have bug with inline element, continuous spaces and word-spacing. Workaround by inline-block. */
      display: inline-block;
      color: transparent;
      z-index: -1;
    }
    /* selection background should not be opaque, for fallback mode */
    ::selection{
      background: rgba(127,255,255,0.4);
    }
    ::-moz-selection{
      background: rgba(127,255,255,0.4);
    }
    .pi { /* info for Javascript */
      display:none;
    }
    .l { /* annotation links */
    }
    /* transparent color - WebKit */
    .d { /* css drawing */
      position:absolute;
      transform-origin:0% 100%;
      -ms-transform-origin:0% 100%;
      -webkit-transform-origin:0% 100%;
    }
    /* for the forms */
    .it {
      border: none;
      background-color: rgba(255, 255, 255, 0.0);
    }
    
    .ir:hover {
      cursor: pointer;
    }
    
    /* Base CSS END */
    </style>
    <style type="text/css">
    /* vim: set shiftwidth=2 tabstop=2 autoindent cindent expandtab filetype=css: */
    /*! 
    * Fancy styles for pdf2htmlEX
    * Copyright 2012,2013 Lu Wang <coolwanglu@gmail.com> 
    * https://github.com/coolwanglu/pdf2htmlEX/blob/master/share/LICENSE
    */
    @keyframes fadein { from { opacity:0;} to { opacity:1;} }
    @-webkit-keyframes fadein { from { opacity:0;} to { opacity:1;} }
    @keyframes swing {
      0%  { transform: rotate(0deg); }
      10% { transform: rotate(0deg); }
      90% { transform: rotate(720deg); }
      100%{ transform: rotate(720deg); }
    }
    @-webkit-keyframes swing {
      0%  { -webkit-transform: rotate(0deg); }
      10% { -webkit-transform: rotate(0deg); }
      90% { -webkit-transform: rotate(720deg); }
      100%{ -webkit-transform: rotate(720deg); }
    }
    @media screen { 
      #sidebar {
        background-color:#2f3236;
        /* modified from http://philbit.com/svgpatterns/#crossstripes */
        background-image:url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0IiBoZWlnaHQ9IjQiPgo8cmVjdCB3aWR0aD0iNCIgaGVpZ2h0PSI0IiBmaWxsPSIjNDAzYzNmIj48L3JlY3Q+CjxwYXRoIGQ9Ik0wIDBMNCA0Wk00IDBMMCA0WiIgc3Ryb2tlLXdpZHRoPSIxIiBzdHJva2U9IiMxZTI5MmQiPjwvcGF0aD4KPC9zdmc+");
      }
      #outline {
        font-family:Georgia,Times,"Times New Roman",serif;
        font-size:13px;
        margin:2em 1em;
      }
      #outline ul {
        padding:0;
      }
      #outline li {
        list-style-type:none;
        margin:1em 0;
      }
      #outline li > ul {
        margin-left: 1em;
      }
      #outline a,
      #outline a:visited,
      #outline a:hover,
      #outline a:active {
        line-height:1.2;
        color:#e8e8e8;
        text-overflow:ellipsis;
        white-space:nowrap;
        text-decoration:none;
        display:block;
        overflow:hidden;
        outline:0;
      }
      #outline a:hover {
        color:rgb(0,204,255);
      }
      .pf {
        margin: 13px auto;
        box-shadow: 1px 1px 3px 1px #333;
        /* Needed by IE to make box-shadow works * https://developer.mozilla.org/en-US/docs/Web/CSS/box-shadow */
        border-collapse: separate;
      }
      .pc.opened { /* used by pdf2htmlEX.js, to show/hide pages */
        -webkit-animation: fadein 100ms;
        animation: fadein 100ms; 
      }
      .loading-indicator.active {
        /* 
        * use 0.01s instead of 0s,
        * since YUI Compressor will change 0s to 0,
        * which is not recognized by Firefox
        */
        -webkit-animation: swing 1.5s ease-in-out 0.01s infinite alternate none;
        animation: swing 1.5s ease-in-out 0.01s infinite alternate none;
      }
      .checked {
        background: no-repeat url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABYAAAAWCAYAAADEtGw7AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH3goQDSYgDiGofgAAAslJREFUOMvtlM9LFGEYx7/vvOPM6ywuuyPFihWFBUsdNnA6KLIh+QPx4KWExULdHQ/9A9EfUodYmATDYg/iRewQzklFWxcEBcGgEplDkDtI6sw4PzrIbrOuedBb9MALD7zv+3m+z4/3Bf7bZS2bzQIAcrmcMDExcTeXy10DAFVVAQDksgFUVZ1ljD3yfd+0LOuFpmnvVVW9GHhkZAQcxwkNDQ2FSCQyRMgJxnVdy7KstKZpn7nwha6urqqfTqfPBAJAuVymlNLXoigOhfd5nmeiKL5TVTV+lmIKwAOA7u5u6Lped2BsbOwjY6yf4zgQQkAIAcedaPR9H67r3uYBQFEUFItFtLe332lpaVkUBOHK3t5eRtf1DwAwODiIubk5DA8PM8bYW1EU+wEgCIJqsCAIQAiB7/u253k2BQDDMJBKpa4mEon5eDx+UxAESJL0uK2t7XosFlvSdf0QAEmlUnlRFJ9Waho2Qghc1/U9z3uWz+eX+Wr+lL6SZfleEAQIggA8z6OpqSknimIvYyybSCReMsZ6TislhCAIAti2Dc/zejVNWwCAavN8339j27YbTg0AGGM3WltbP4WhlRWq6Q/btrs1TVsYHx+vNgqKoqBUKn2NRqPFxsbGJzzP05puUlpt0ukyOI6z7zjOwNTU1OLo6CgmJyf/gA3DgKIoWF1d/cIY24/FYgOU0pp0z/Ityzo8Pj5OTk9PbwHA+vp6zWghDC+VSiuRSOQgGo32UErJ38CO42wdHR09LBQK3zKZDDY2NupmFmF4R0cHVlZWlmRZ/iVJUn9FeWWcCCE4ODjYtG27Z2Zm5juAOmgdGAB2d3cBADs7O8uSJN2SZfl+WKlpmpumaT6Yn58vn/fs6XmbhmHMNjc3tzDGFI7jYJrm5vb29sDa2trPC/9aiqJUy5pOp4f6+vqeJ5PJBAB0dnZe/t8NBajx/z37Df5OGX8d13xzAAAAAElFTkSuQmCC);
      }
    }
    /* Fancy CSS END */
    </style>
    <style type="text/css">
    .ff0{font-family:sans-serif;visibility:hidden;}
    @font-face{font-family:ff1;src:url('data:application/font-woff;base64,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')format("woff");}.ff1{font-family:ff1;line-height:1.058000;font-style:normal;font-weight:normal;visibility:visible;}
    @font-face{font-family:ff2;src:url('data:application/font-woff;base64,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')format("woff");}.ff2{font-family:ff2;line-height:0.938000;font-style:normal;font-weight:normal;visibility:visible;}
    @font-face{font-family:ff3;src:url('data:application/font-woff;base64,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')format("woff");}.ff3{font-family:ff3;line-height:0.938000;font-style:normal;font-weight:normal;visibility:visible;}
    @font-face{font-family:ff4;src:url('data:application/font-woff;base64,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')format("woff");}.ff4{font-family:ff4;line-height:1.058000;font-style:normal;font-weight:normal;visibility:visible;}
    .m0{transform:matrix(0.250000,0.000000,0.000000,0.250000,0,0);-ms-transform:matrix(0.250000,0.000000,0.000000,0.250000,0,0);-webkit-transform:matrix(0.250000,0.000000,0.000000,0.250000,0,0);}
    .m1{transform:none;-ms-transform:none;-webkit-transform:none;}
    .v0{vertical-align:0.000000px;}
    .v1{vertical-align:17.352000px;}
    .ls2{letter-spacing:0.000000px;}
    .ls0{letter-spacing:23.910400px;}
    .ls3{letter-spacing:26.166484px;}
    .ls1{letter-spacing:47.151309px;}
    .sc_{text-shadow:none;}
    .sc0{text-shadow:-0.015em 0 transparent,0 0.015em transparent,0.015em 0 transparent,0 -0.015em  transparent;}
    @media screen and (-webkit-min-device-pixel-ratio:0){
    .sc_{-webkit-text-stroke:0px transparent;}
    .sc0{-webkit-text-stroke:0.015em transparent;text-shadow:none;}
    }
    .wse{word-spacing:-15.111373px;}
    .wsb{word-spacing:-7.460045px;}
    .wsc{word-spacing:-5.834061px;}
    .wsf{word-spacing:-5.260288px;}
    .wsa{word-spacing:-4.734061px;}
    .ws6{word-spacing:-4.208230px;}
    .ws7{word-spacing:0.000000px;}
    .ws2{word-spacing:7.125299px;}
    .ws5{word-spacing:11.907379px;}
    .ws4{word-spacing:14.288815px;}
    .ws8{word-spacing:16.402339px;}
    .ws9{word-spacing:17.502403px;}
    .wsd{word-spacing:22.523597px;}
    .ws3{word-spacing:23.862579px;}
    .ws1{word-spacing:47.103488px;}
    .ws10{word-spacing:1088.933965px;}
    .ws11{word-spacing:1211.683430px;}
    .ws0{word-spacing:1270.837760px;}
    ._2{margin-left:-26.253619px;}
    ._1{margin-left:-23.910400px;}
    ._3{margin-left:-12.337766px;}
    ._b{margin-left:-11.285709px;}
    ._19{margin-left:-9.994547px;}
    ._29{margin-left:-6.647091px;}
    ._1f{margin-left:-4.208230px;}
    ._a{margin-left:-2.630144px;}
    ._d{margin-left:-1.099878px;}
    ._11{width:1.090311px;}
    ._4{width:4.016947px;}
    ._23{width:5.164646px;}
    ._3c{width:10.520576px;}
    ._7{width:11.955200px;}
    ._40{width:13.437645px;}
    ._1b{width:14.537523px;}
    ._34{width:15.924326px;}
    ._17{width:17.024205px;}
    ._26{width:18.554470px;}
    ._37{width:20.276019px;}
    ._2c{width:21.375898px;}
    ._33{width:22.858342px;}
    ._22{width:23.910400px;}
    ._15{width:25.201562px;}
    ._24{width:28.453376px;}
    ._27{width:29.983642px;}
    ._8{width:31.753011px;}
    ._e{width:32.900710px;}
    ._1d{width:35.148288px;}
    ._28{width:36.152525px;}
    ._3a{width:37.539328px;}
    ._2b{width:38.543565px;}
    ._16{width:39.691069px;}
    ._1a{width:41.412739px;}
    ._2f{width:42.417126px;}
    ._35{width:43.421680px;}
    ._20{width:45.907968px;}
    ._13{width:47.151309px;}
    ._6{width:54.993920px;}
    ._10{width:57.384800px;}
    ._5{width:62.597360px;}
    ._c{width:63.697360px;}
    ._1e{width:64.988272px;}
    ._f{width:86.507760px;}
    ._9{width:87.607760px;}
    ._25{width:206.872781px;}
    ._14{width:680.633446px;}
    ._36{width:752.221184px;}
    ._30{width:765.283693px;}
    ._31{width:799.542418px;}
    ._39{width:804.202394px;}
    ._2a{width:855.801037px;}
    ._3e{width:875.551027px;}
    ._2d{width:893.388186px;}
    ._1c{width:896.926925px;}
    ._12{width:912.877398px;}
    ._3b{width:947.186586px;}
    ._38{width:965.884518px;}
    ._18{width:992.616346px;}
    ._21{width:996.250726px;}
    ._3d{width:1064.634470px;}
    ._3f{width:1081.419571px;}
    ._2e{width:1093.709517px;}
    ._0{width:1102.460723px;}
    ._32{width:1184.282112px;}
    .fc0{color:rgb(0,0,0);}
    .fs4{font-size:31.880400px;}
    .fs3{font-size:39.850400px;}
    .fs1{font-size:47.820800px;}
    .fs2{font-size:57.384800px;}
    .fs0{font-size:99.148400px;}
    .y1c{bottom:104.882000px;}
    .y1b{bottom:133.228000px;}
    .y42{bottom:133.229000px;}
    .y1a{bottom:148.033000px;}
    .y87{bottom:148.038000px;}
    .y41{bottom:148.106000px;}
    .y63{bottom:148.901000px;}
    .y19{bottom:162.838000px;}
    .y86{bottom:162.847000px;}
    .y40{bottom:162.983000px;}
    .y62{bottom:164.573000px;}
    .y18{bottom:177.643000px;}
    .y85{bottom:177.656000px;}
    .y3f{bottom:177.860000px;}
    .y61{bottom:180.245000px;}
    .y17{bottom:192.448000px;}
    .y84{bottom:192.465000px;}
    .y3e{bottom:192.737000px;}
    .y60{bottom:195.918000px;}
    .y16{bottom:207.252000px;}
    .y83{bottom:207.274000px;}
    .y3d{bottom:207.614000px;}
    .y5f{bottom:211.590000px;}
    .y15{bottom:222.057000px;}
    .y82{bottom:222.083000px;}
    .y3c{bottom:222.491000px;}
    .y5e{bottom:227.262000px;}
    .y81{bottom:236.892000px;}
    .y5d{bottom:242.935000px;}
    .y14{bottom:249.057000px;}
    .y3b{bottom:249.611000px;}
    .y5c{bottom:258.607000px;}
    .y13{bottom:263.861000px;}
    .y80{bottom:263.898000px;}
    .y3a{bottom:264.488000px;}
    .y12{bottom:278.666000px;}
    .y39{bottom:279.365000px;}
    .y5b{bottom:287.052000px;}
    .y11{bottom:293.471000px;}
    .y38{bottom:294.242000px;}
    .y5a{bottom:302.724000px;}
    .y7f{bottom:305.848000px;}
    .y10{bottom:308.276000px;}
    .y37{bottom:309.119000px;}
    .y59{bottom:318.397000px;}
    .y7e{bottom:320.657000px;}
    .yf{bottom:323.081000px;}
    .y58{bottom:334.069000px;}
    .y7d{bottom:335.466000px;}
    .y36{bottom:336.239000px;}
    .y57{bottom:349.741000px;}
    .ye{bottom:350.080000px;}
    .y7c{bottom:350.275000px;}
    .y35{bottom:351.116000px;}
    .yd{bottom:364.885000px;}
    .y7b{bottom:365.084000px;}
    .y56{bottom:365.414000px;}
    .y34{bottom:365.994000px;}
    .yc{bottom:379.690000px;}
    .y7a{bottom:379.893000px;}
    .y33{bottom:380.871000px;}
    .y55{bottom:381.086000px;}
    .yb{bottom:394.494000px;}
    .y32{bottom:395.748000px;}
    .y79{bottom:406.900000px;}
    .ya{bottom:409.299000px;}
    .y54{bottom:409.531000px;}
    .y31{bottom:410.625000px;}
    .y78{bottom:421.709000px;}
    .y53{bottom:425.203000px;}
    .y30{bottom:425.502000px;}
    .y9{bottom:436.298000px;}
    .y77{bottom:436.518000px;}
    .y2f{bottom:440.379000px;}
    .y52{bottom:440.876000px;}
    .y76{bottom:451.327000px;}
    .y2e{bottom:455.256000px;}
    .y51{bottom:456.548000px;}
    .y75{bottom:466.136000px;}
    .y50{bottom:472.220000px;}
    .y8{bottom:478.242000px;}
    .y74{bottom:480.945000px;}
    .y2d{bottom:482.376000px;}
    .y4f{bottom:487.893000px;}
    .y7{bottom:493.047000px;}
    .y73{bottom:495.754000px;}
    .y2c{bottom:497.253000px;}
    .y4e{bottom:503.565000px;}
    .y6{bottom:507.851000px;}
    .y72{bottom:510.563000px;}
    .y2b{bottom:512.130000px;}
    .y4d{bottom:519.237000px;}
    .y5{bottom:522.656000px;}
    .y71{bottom:525.372000px;}
    .y2a{bottom:527.007000px;}
    .y4{bottom:537.461000px;}
    .y70{bottom:540.181000px;}
    .y29{bottom:541.884000px;}
    .y4c{bottom:547.682000px;}
    .y3{bottom:552.266000px;}
    .y28{bottom:556.762000px;}
    .y4b{bottom:563.355000px;}
    .y2{bottom:567.071000px;}
    .y6f{bottom:567.187000px;}
    .y27{bottom:571.639000px;}
    .y4a{bottom:579.027000px;}
    .y6e{bottom:581.996000px;}
    .y26{bottom:586.516000px;}
    .y1{bottom:594.070000px;}
    .y49{bottom:594.699000px;}
    .y6d{bottom:596.805000px;}
    .y25{bottom:601.393000px;}
    .y48{bottom:610.372000px;}
    .y6c{bottom:611.614000px;}
    .y24{bottom:616.270000px;}
    .y47{bottom:626.044000px;}
    .y6b{bottom:626.423000px;}
    .y6a{bottom:641.232000px;}
    .y46{bottom:641.716000px;}
    .y23{bottom:643.390000px;}
    .y69{bottom:656.041000px;}
    .y45{bottom:657.389000px;}
    .y22{bottom:658.267000px;}
    .y0{bottom:664.895000px;}
    .y68{bottom:670.850000px;}
    .y44{bottom:673.061000px;}
    .y21{bottom:673.144000px;}
    .y67{bottom:685.659000px;}
    .y20{bottom:688.021000px;}
    .y66{bottom:700.468000px;}
    .y43{bottom:701.506000px;}
    .y1f{bottom:702.898000px;}
    .y89{bottom:704.048000px;}
    .y65{bottom:715.277000px;}
    .y1e{bottom:717.775000px;}
    .y64{bottom:730.086000px;}
    .y88{bottom:730.449000px;}
    .y1d{bottom:744.895000px;}
    .h5{height:32.677328px;}
    .h2{height:33.474560px;}
    .h3{height:39.213056px;}
    .h4{height:40.169360px;}
    .h6{height:43.493928px;}
    .h1{height:81.301688px;}
    .h0{height:841.890000px;}
    .w0{width:595.276000px;}
    .x0{left:113.386000px;}
    .x1{left:131.319000px;}
    .x5{left:290.988000px;}
    .x4{left:292.338000px;}
    .x3{left:293.688000px;}
    .x2{left:295.038000px;}
    </style>
    <script>
    /*
    Copyright 2012 Mozilla Foundation 
    Copyright 2013 Lu Wang <coolwanglu@gmail.com>
    Apachine License Version 2.0 
    */
    (function(){function b(a,b,e,f){var c=(a.className||"").split(/\s+/g);""===c[0]&&c.shift();var d=c.indexOf(b);0>d&&e&&c.push(b);0<=d&&f&&c.splice(d,1);a.className=c.join(" ");return 0<=d}if(!("classList"in document.createElement("div"))){var e={add:function(a){b(this.element,a,!0,!1)},contains:function(a){return b(this.element,a,!1,!1)},remove:function(a){b(this.element,a,!1,!0)},toggle:function(a){b(this.element,a,!0,!0)}};Object.defineProperty(HTMLElement.prototype,"classList",{get:function(){if(this._classList)return this._classList;
    var a=Object.create(e,{element:{value:this,writable:!1,enumerable:!0}});Object.defineProperty(this,"_classList",{value:a,writable:!1,enumerable:!1});return a},enumerable:!0})}})();
    </script>
    <script>
    /* vim: set shiftwidth=2 tabstop=2 autoindent cindent expandtab filetype=javascript : */
    /** 
    * @license pdf2htmlEX.js: Core UI functions for pdf2htmlEX 
    * Copyright 2012,2013 Lu Wang <coolwanglu@gmail.com> and other contributors 
    * https://github.com/coolwanglu/pdf2htmlEX/blob/master/share/LICENSE 
    */
    
    /*
    * Attention:
    * This files is to be optimized by closure-compiler, 
    * so pay attention to the forms of property names:
    *
    * string/bracket form is safe, won't be optimized:
    * var obj={ 'a':'b' }; obj['a'] = 'b';
    * name/dot form will be optimized, the name is likely to be modified:
    * var obj={ a:'b' }; obj.a = 'b';
    *
    * Either form can be used for internal objects, 
    * but must be consistent for each one respectively.
    *
    * string/bracket form must be used for external objects
    * e.g. DEFAULT_CONFIG, object stored in page-data
    * property names are part of the `protocol` in these cases.
    *
    */
    
    'use strict';
    
    var pdf2htmlEX = window['pdf2htmlEX'] = window['pdf2htmlEX'] || {};
    
    /** 
    * @const 
    * @struct
    */
    var CSS_CLASS_NAMES = {
      page_frame       : 'pf',
      page_content_box : 'pc',
      page_data        : 'pi',
      background_image : 'bi',
      link             : 'l',
      input_radio      : 'ir',
      __dummy__        : 'no comma'
    };
    
    /** 
    * configurations of Viewer
    * @const 
    * @dict
    */
    var DEFAULT_CONFIG = {
      // id of the element to put the pages in
      'container_id' : 'page-container',
      // id of the element for sidebar (to open and close)
      'sidebar_id' : 'sidebar',
      // id of the element for outline
      'outline_id' : 'outline',
      // class for the loading indicator
      'loading_indicator_cls' : 'loading-indicator',
      // How many page shall we preload that are below the last visible page
      'preload_pages' : 3,
      // how many ms should we wait before actually rendering the pages and after a scroll event
      'render_timeout' : 100,
      // zoom ratio step for each zoom in/out event
      'scale_step' : 0.9,
      // register global key handler, allowing navigation by keyboard
      'key_handler' : true,
      // register hashchange handler, navigate to the location specified by the hash
      'hashchange_handler' : true,
      // register view history handler, allowing going back to the previous location
      'view_history_handler' : true,
    
      '__dummy__'        : 'no comma'
    };
    
    /** @const */
    var EPS = 1e-6;
    
    /************************************/
    /* utility function */
    /**
    * @param{Array.<number>} ctm
    */
    function invert(ctm) {
      var det = ctm[0] * ctm[3] - ctm[1] * ctm[2];
      return [ ctm[3] / det
              ,-ctm[1] / det
              ,-ctm[2] / det
              ,ctm[0] / det
              ,(ctm[2] * ctm[5] - ctm[3] * ctm[4]) / det
              ,(ctm[1] * ctm[4] - ctm[0] * ctm[5]) / det
            ];
    };
    /**
    * @param{Array.<number>} ctm
    * @param{Array.<number>} pos
    */
    function transform(ctm, pos) {
      return [ctm[0] * pos[0] + ctm[2] * pos[1] + ctm[4]
            ,ctm[1] * pos[0] + ctm[3] * pos[1] + ctm[5]];
    };
    
    /**
    * @param{Element} ele
    */
    function get_page_number(ele) {
      return parseInt(ele.getAttribute('data-page-no'), 16);
    };
    
    /**
    * @param{NodeList} eles
    */
    function disable_dragstart(eles) {
      for (var i = 0, l = eles.length; i < l; ++i) {
        eles[i].addEventListener('dragstart', function() {
          return false;
        }, false);
      }
    };
    
    /**
    * @param{...Object} var_args
    */
    function clone_and_extend_objs(var_args) {
      var result_obj = {};
      for (var i = 0, l = arguments.length; i < l; ++i) {
        var cur_obj = arguments[i];
        for (var k in cur_obj) {
          if (cur_obj.hasOwnProperty(k)) {
            result_obj[k] = cur_obj[k];
          }
        }
      }
      return result_obj;
    };
    
    /** 
    * @constructor 
    * @param{Element} page The element for the page
    */
    function Page(page) {
      if (!page) return;
    
      this.loaded = false;
      this.shown = false;
      this.page = page; // page frame element
    
      this.num = get_page_number(page);
    
      // page size
      // Need to make rescale work when page_content_box is not loaded, yet
      this.original_height = page.clientHeight;     
      this.original_width = page.clientWidth;
    
      // content box
      var content_box = page.getElementsByClassName(CSS_CLASS_NAMES.page_content_box)[0];
    
      // if page is loaded
      if (content_box) {
        this.content_box = content_box;
        /*
        * scale ratios
        *
        * original_scale : the first one
        * cur_scale : currently using
        */
        this.original_scale = this.cur_scale = this.original_height / content_box.clientHeight;
        this.page_data = JSON.parse(page.getElementsByClassName(CSS_CLASS_NAMES.page_data)[0].getAttribute('data-data'));
    
        this.ctm = this.page_data['ctm'];
        this.ictm = invert(this.ctm);
    
        this.loaded = true;
      }
    };
    Page.prototype = {
      /* hide & show are for contents, the page frame is still there */
      hide : function(){
        if (this.loaded && this.shown) {
          this.content_box.classList.remove('opened');
          this.shown = false;
        }
      },
      show : function(){
        if (this.loaded && !this.shown) {
          this.content_box.classList.add('opened');
          this.shown = true;
        }
      },
      /**
      * @param{number} ratio
      */
      rescale : function(ratio) {
        if (ratio === 0) {
          // reset scale
          this.cur_scale = this.original_scale;
        } else {
          this.cur_scale = ratio;
        }
    
        // scale the content box
        if (this.loaded) {
          var cbs = this.content_box.style;
          cbs.msTransform = cbs.webkitTransform = cbs.transform = 'scale('+this.cur_scale.toFixed(3)+')';
        }
    
        // stretch the page frame to hold the place
        {
          var ps = this.page.style;
          ps.height = (this.original_height * this.cur_scale) + 'px';
          ps.width = (this.original_width * this.cur_scale) + 'px';
        }
      },
      /*
      * return the coordinate of the top-left corner of container
      * in our coordinate system
      * assuming that p.parentNode === p.offsetParent
      */
      view_position : function () {
        var p = this.page;
        var c = p.parentNode;
        return [c.scrollLeft - p.offsetLeft - p.clientLeft
              ,c.scrollTop - p.offsetTop - p.clientTop];
      },
      height : function () {
        return this.page.clientHeight;
      },
      width : function () {
        return this.page.clientWidth;
      }
    };
    
    /** 
    * @constructor
    * @param{Object=} config
    */
    function Viewer(config) {
      this.config = clone_and_extend_objs(DEFAULT_CONFIG, (arguments.length > 0 ? config : {}));
      this.pages_loading = [];
      this.init_before_loading_content();
    
      var self = this;
      document.addEventListener('DOMContentLoaded', function(){
        self.init_after_loading_content();
      }, false);
    };
    
    Viewer.prototype = {
      scale : 1,
      /* 
      * index of the active page (the one with largest visible area)
      * which estimates the page currently being viewed
      */
      cur_page_idx : 0,
    
      /*
      * index of the first visible page
      * used when determining current view
      */
      first_page_idx : 0,
    
      init_before_loading_content : function() {
        /* hide all pages before loading, will reveal only visible ones later */
        this.pre_hide_pages();
      },
    
      initialize_radio_button : function() {
        var elements = document.getElementsByClassName(CSS_CLASS_NAMES.input_radio);
        
        for(var i = 0; i < elements.length; i++) {
          var r = elements[i];
    
          r.addEventListener('click', function() {
            this.classList.toggle("checked");
          });
        }
      },
    
      init_after_loading_content : function() {
        this.sidebar = document.getElementById(this.config['sidebar_id']);
        this.outline = document.getElementById(this.config['outline_id']);
        this.container = document.getElementById(this.config['container_id']);
        this.loading_indicator = document.getElementsByClassName(this.config['loading_indicator_cls'])[0];
    
        
        {
          // Open the outline if nonempty
          var empty = true;
          var nodes = this.outline.childNodes;
          for (var i = 0, l = nodes.length; i < l; ++i) {
            var cur_node = nodes[i];
            if (cur_node.nodeName.toLowerCase() === 'ul') {
              empty = false;
              break;
            }
          }
          if (!empty)
            this.sidebar.classList.add('opened');
        }
    
        this.find_pages();
        // do nothing if there's nothing
        if(this.pages.length == 0) return;
    
        // disable dragging of background images
        disable_dragstart(document.getElementsByClassName(CSS_CLASS_NAMES.background_image));
    
        if (this.config['key_handler'])
          this.register_key_handler();
    
        var self = this;
    
        if (this.config['hashchange_handler']) {
          window.addEventListener('hashchange', function(e) {
            self.navigate_to_dest(document.location.hash.substring(1));
          }, false);
        }
    
        if (this.config['view_history_handler']) {
          window.addEventListener('popstate', function(e) {
            if(e.state) self.navigate_to_dest(e.state);
          }, false);
        }
    
        // register schedule rendering
        // renew old schedules since scroll() may be called frequently
        this.container.addEventListener('scroll', function() {
          self.update_page_idx();
          self.schedule_render(true);
        }, false);
    
        // handle links
        [this.container, this.outline].forEach(function(ele) {
          ele.addEventListener('click', self.link_handler.bind(self), false);
        });
    
        this.initialize_radio_button();
        this.render();
      },
    
      /*
      * set up this.pages and this.page_map
      * pages is an array holding all the Page objects
      * page-Map maps an original page number (in PDF) to the corresponding index in page
      */
      find_pages : function() {
        var new_pages = [];
        var new_page_map = {};
        var nodes = this.container.childNodes;
        for (var i = 0, l = nodes.length; i < l; ++i) {
          var cur_node = nodes[i];
          if ((cur_node.nodeType === Node.ELEMENT_NODE)
              && cur_node.classList.contains(CSS_CLASS_NAMES.page_frame)) {
            var p = new Page(cur_node);
            new_pages.push(p);
            new_page_map[p.num] = new_pages.length - 1;
          }
        }
        this.pages = new_pages;
        this.page_map = new_page_map;
      },
    
      /**
      * @param{number} idx
      * @param{number=} pages_to_preload
      * @param{function(Page)=} callback
      *
      * TODO: remove callback -> promise ?
      */
      load_page : function(idx, pages_to_preload, callback) {
        var pages = this.pages;
        if (idx >= pages.length)
          return;  // Page does not exist
    
        var cur_page = pages[idx];
        if (cur_page.loaded)
          return;  // Page is loaded
    
        if (this.pages_loading[idx])
          return;  // Page is already loading
    
        var cur_page_ele = cur_page.page;
        var url = cur_page_ele.getAttribute('data-page-url');
        if (url) {
          this.pages_loading[idx] = true;       // set semaphore
    
          // add a copy of the loading indicator if not already present
          var new_loading_indicator = cur_page_ele.getElementsByClassName(this.config['loading_indicator_cls'])[0];
          if (typeof new_loading_indicator === 'undefined'){
            new_loading_indicator = this.loading_indicator.cloneNode(true);
            new_loading_indicator.classList.add('active');
            cur_page_ele.appendChild(new_loading_indicator);
          }
    
          // load data
          {
            var self = this;
            var _idx = idx;
            var xhr = new XMLHttpRequest();
            xhr.open('GET', url, true);
            xhr.onload = function(){
              if (xhr.status === 200 || xhr.status === 0) {
                // find the page element in the data
                var div = document.createElement('div');
                div.innerHTML = xhr.responseText;
    
                var new_page = null;
                var nodes = div.childNodes;
                for (var i = 0, l = nodes.length; i < l; ++i) {
                  var cur_node = nodes[i];
                  if ((cur_node.nodeType === Node.ELEMENT_NODE)
                      && cur_node.classList.contains(CSS_CLASS_NAMES.page_frame)) {
                    new_page = cur_node;
                    break;
                  }
                }
    
                // replace the old page with loaded data
                // the loading indicator on this page should also be destroyed
                var p = self.pages[_idx];
                self.container.replaceChild(new_page, p.page);
                p = new Page(new_page);
                self.pages[_idx] = p;
    
                p.hide();
                p.rescale(self.scale);
    
                // disable background image dragging
                disable_dragstart(new_page.getElementsByClassName(CSS_CLASS_NAMES.background_image));
    
                self.schedule_render(false);
    
                if (callback){ callback(p); }
              }
    
              // Reset loading token
              delete self.pages_loading[_idx];
            };
            xhr.send(null);
          }
        }
        // Concurrent prefetch of the next pages
        if (pages_to_preload === undefined)
          pages_to_preload = this.config['preload_pages'];
    
        if (--pages_to_preload > 0) {
          var self = this;
          setTimeout(function() {
            self.load_page(idx+1, pages_to_preload);
          },0);
        }
      },
    
      /*
      * Hide all pages that have no 'opened' class
      * The 'opened' class will be added to visible pages by JavaScript
      * We cannot add this in the default CSS because JavaScript may be disabled
      */
      pre_hide_pages : function() {
        /* pages might have not been loaded yet, so add a CSS rule */
        var s = '@media screen{.'+CSS_CLASS_NAMES.page_content_box+'{display:none;}}';
        var n = document.createElement('style');
        if (n.styleSheet) {
          n.styleSheet.cssText = s;
        } else {
          n.appendChild(document.createTextNode(s));
        }
        document.head.appendChild(n);
      },
    
      /*
      * show visible pages and hide invisible pages
      */
      render : function () {
        var container = this.container;
        /* 
        * show the pages that are 'nearly' visible -- it's right above or below the container
        *
        * all the y values are in the all-page element's coordinate system
        */
        var container_min_y = container.scrollTop;
        var container_height = container.clientHeight;
        var container_max_y = container_min_y + container_height;
        var visible_min_y = container_min_y - container_height;
        var visible_max_y = container_max_y + container_height;
    
        var cur_page_fully_visible = false;
        var cur_page_idx = this.cur_page_idx;
        var max_visible_page_idx = cur_page_idx;
        var max_visible_ratio = 0.0;
    
        var pl = this.pages;
        for (var i = 0, l = pl.length; i < l; ++i) {
          var cur_page = pl[i];
          var cur_page_ele = cur_page.page;
          var page_min_y = cur_page_ele.offsetTop + cur_page_ele.clientTop;
          var page_height = cur_page_ele.clientHeight;
          var page_max_y = page_min_y + page_height;
          if ((page_min_y <= visible_max_y) && (page_max_y >= visible_min_y))
          {
            // cur_page is 'nearly' visible, show it or load it
            if (cur_page.loaded) {
              cur_page.show();
            } else {
              this.load_page(i);
            }
          } else {
            cur_page.hide();
          }
        }
      },
      /*
      * update cur_page_idx and first_page_idx
      * normally called upon scrolling
      */
      update_page_idx: function () {
        var pages = this.pages;
        var pages_len = pages.length;
        // there is no chance that cur_page_idx or first_page_idx is modified
        if (pages_len < 2) return;
      
        var container = this.container;
        var container_min_y = container.scrollTop;
        var container_max_y = container_min_y + container.clientHeight;
    
        // binary search for the first page
        // whose bottom border is below the top border of the container
        var first_idx = -1;
        var last_idx = pages_len;
        var rest_len = last_idx - first_idx;
        // TODO: use current first_page_idx as a hint?
        while(rest_len > 1) {
          var idx = first_idx + Math.floor(rest_len / 2);
          var cur_page_ele = pages[idx].page;
          if (cur_page_ele.offsetTop + cur_page_ele.clientTop + cur_page_ele.clientHeight >= container_min_y) {
            last_idx = idx;
          } else {
            first_idx = idx;
          }
          rest_len = last_idx - first_idx;
        }
        
        /*
        * with malformed settings it is possible that no page is visible, e.g.
        * - the container is to thin, which lies in the margin between two pages
        * - all pages are completely above or below the container
        * but we just assume that they won't happen.
        */
        this.first_page_idx = last_idx;
    
        // find the page with largest visible area
        var cur_page_idx = this.cur_page_idx;
        var max_visible_page_idx = cur_page_idx;
        var max_visible_ratio = 0.0;
    
        for(var i = last_idx; i < pages_len; ++i) {
          var cur_page_ele = pages[i].page;
          var page_min_y = cur_page_ele.offsetTop + cur_page_ele.clientTop;
          var page_height = cur_page_ele.clientHeight;
          var page_max_y = page_min_y + page_height;
          if (page_min_y > container_max_y) break;
    
          // check the visible fraction of the page
          var page_visible_ratio = ( Math.min(container_max_y, page_max_y) 
                                    - Math.max(container_min_y, page_min_y)
                                  ) / page_height;
    
          // stay with the current page if it is still fully visible
          if ((i === cur_page_idx) && (Math.abs(page_visible_ratio - 1.0) <= EPS)) {
            max_visible_page_idx = cur_page_idx;
            break;
          }
    
          if (page_visible_ratio > max_visible_ratio) {
            max_visible_ratio = page_visible_ratio;
            max_visible_page_idx = i;
          }
        }
    
        this.cur_page_idx = max_visible_page_idx;
      },
    
      /**
      * @param{boolean} renew renew the existing schedule instead of using the old one
      */
      schedule_render : function(renew) {
        if (this.render_timer !== undefined) {
          if (!renew) return;
          clearTimeout(this.render_timer);
        }
    
        var self = this;
        this.render_timer = setTimeout(function () {
          /*
          * render() may trigger load_page(), which may in turn trigger another render()
          * so delete render_timer first
          */
          delete self.render_timer;
          self.render();
        }, this.config['render_timeout']);
      },
    
      /*
      * Handling key events, zooming, scrolling etc.
      */
      register_key_handler: function () {
        /* 
        * When user try to zoom in/out using ctrl + +/- or mouse wheel
        * handle this and prevent the default behaviours
        *
        * Code credit to PDF.js
        */
        var self = this;
    
        // Firefox specific event, so that we can prevent browser from zooming
        window.addEventListener('DOMMouseScroll', function(e) {
          if (e.ctrlKey) {
            e.preventDefault();
            var container = self.container;
            var rect = container.getBoundingClientRect();
            var fixed_point = [e.clientX - rect['left'] - container.clientLeft
                              ,e.clientY - rect['top'] - container.clientTop];
            self.rescale(Math.pow(self.config['scale_step'], e.detail), true, fixed_point);
          }
        }, false);
    
        window.addEventListener('keydown', function(e) {
          var handled = false;
          /*
          var cmd = (e.ctrlKey ? 1 : 0)
                    | (e.altKey ? 2 : 0)
                    | (e.shiftKey ? 4 : 0)
                    | (e.metaKey ? 8 : 0)
                    ;
                    */
          var with_ctrl = e.ctrlKey || e.metaKey;
          var with_alt = e.altKey;
          switch (e.keyCode) {
            case 61: // FF/Mac '='
            case 107: // FF '+' and '='
            case 187: // Chrome '+'
              if (with_ctrl){
                self.rescale(1.0 / self.config['scale_step'], true);
                handled = true;
              }
              break;
            case 173: // FF/Mac '-'
            case 109: // FF '-'
            case 189: // Chrome '-'
              if (with_ctrl){
                self.rescale(self.config['scale_step'], true);
                handled = true;
              }
              break;
            case 48: // '0'
              if (with_ctrl){
                self.rescale(0, false);
                handled = true;
              }
              break;
            case 33: // Page UP:
              if (with_alt) { // alt-pageup    -> scroll one page up
                self.scroll_to(self.cur_page_idx - 1);
              } else { // pageup        -> scroll one screen up
                self.container.scrollTop -= self.container.clientHeight;
              }
              handled = true;
              break;
            case 34: // Page DOWN
              if (with_alt) { // alt-pagedown  -> scroll one page down
                self.scroll_to(self.cur_page_idx + 1);
              } else { // pagedown      -> scroll one screen down
                self.container.scrollTop += self.container.clientHeight;
              }
              handled = true;
              break;
            case 35: // End
              self.container.scrollTop = self.container.scrollHeight;
              handled = true;
              break;
            case 36: // Home
              self.container.scrollTop = 0;
              handled = true;
              break;
          }
          if (handled) {
            e.preventDefault();
            return;
          }
        }, false);
      },
    
      /**
      * @param{number} ratio
      * @param{boolean} is_relative
      * @param{Array.<number>=} fixed_point preserve the position (relative to the top-left corner of the viewer) after rescaling
      */
      rescale : function (ratio, is_relative, fixed_point) {
        var old_scale = this.scale;
        var new_scale = old_scale;
        // set new scale
        if (ratio === 0) {
          new_scale = 1;
          is_relative = false;
        } else if (is_relative)
          new_scale *= ratio;
        else
          new_scale = ratio;
    
        this.scale = new_scale;
    
        if (!fixed_point)
          fixed_point = [0,0];
    
        // translate fixed_point to the coordinate system of all pages
        var container = this.container;
        fixed_point[0] += container.scrollLeft;
        fixed_point[1] += container.scrollTop;
    
        // find the visible page that contains the fixed point
        // if the fixed point lies between two pages (including their borders), it's contained in the first one
        var pl = this.pages;
        var pl_len = pl.length;
        for (var i = this.first_page_idx; i < pl_len; ++i) {
          var p = pl[i].page;
          if (p.offsetTop + p.clientTop >= fixed_point[1])
            break;
        }
        var fixed_point_page_idx = i - 1;
    
        // determine the new scroll position
        // each-value consists of two parts, one inside the page, which is affected by rescaling,
        // the other is outside, (e.g. borders and margins), which is not affected
    
        // if the fixed_point is above the first page, use the first page as the reference
        if (fixed_point_page_idx < 0) 
          fixed_point_page_idx = 0;
    
        var fp_p = pl[fixed_point_page_idx].page;
        var fp_p_width = fp_p.clientWidth;
        var fp_p_height = fp_p.clientHeight;
    
        var fp_x_ref = fp_p.offsetLeft + fp_p.clientLeft;
        var fp_x_inside = fixed_point[0] - fp_x_ref;
        if (fp_x_inside < 0)
          fp_x_inside = 0;
        else if (fp_x_inside > fp_p_width)
          fp_x_inside = fp_p_width;
    
        var fp_y_ref = fp_p.offsetTop + fp_p.clientTop;
        var fp_y_inside = fixed_point[1] - fp_y_ref;
        if (fp_y_inside < 0)
          fp_y_inside = 0;
        else if (fp_y_inside > fp_p_height)
          fp_y_inside = fp_p_height;
    
        // Rescale pages
        for (var i = 0; i < pl_len; ++i) 
            pl[i].rescale(new_scale);  
    
        // Correct container scroll to keep view aligned while zooming
        container.scrollLeft += fp_x_inside / old_scale * new_scale + fp_p.offsetLeft + fp_p.clientLeft - fp_x_inside - fp_x_ref;
        container.scrollTop += fp_y_inside / old_scale * new_scale + fp_p.offsetTop + fp_p.clientTop - fp_y_inside - fp_y_ref;
    
        // some pages' visibility may be toggled, wait for next render()
        // renew old schedules since rescale() may be called frequently
        this.schedule_render(true);
      },
    
      fit_width : function () {
        var page_idx = this.cur_page_idx;
        this.rescale(this.container.clientWidth / this.pages[page_idx].width(), true);
        this.scroll_to(page_idx);
      },
    
      fit_height : function () {
        var page_idx = this.cur_page_idx;
        this.rescale(this.container.clientHeight / this.pages[page_idx].height(), true);
        this.scroll_to(page_idx);
      },
      /**
      * @param{Node} ele
      */
      get_containing_page : function(ele) {
        /* get the page obj containing obj */
        while(ele) {
          if ((ele.nodeType === Node.ELEMENT_NODE)
              && ele.classList.contains(CSS_CLASS_NAMES.page_frame)) {
            /*
            * Get original page number and map it to index of pages
            * TODO: store the index on the dom element
            */
            var pn = get_page_number(/** @type{Element} */(ele));
            var pm = this.page_map;
            return (pn in pm) ? this.pages[pm[pn]] : null;
          }
          ele = ele.parentNode;
        }
        return null;
      },
    
      /**
      * @param{Event} e
      */
      link_handler : function (e) {
        var target = /** @type{Node} */(e.target);
        var detail_str = /** @type{string} */ (target.getAttribute('data-dest-detail'));
        if (!detail_str) return;
    
        if (this.config['view_history_handler']) {
          try {
            var cur_hash = this.get_current_view_hash();
            window.history.replaceState(cur_hash, '', '#' + cur_hash);
            window.history.pushState(detail_str, '', '#' + detail_str);
          } catch(ex) { }
        }
        this.navigate_to_dest(detail_str, this.get_containing_page(target));
        e.preventDefault();
      },
    
      /**
      * @param{string} detail_str may come from user provided hashtag, need sanitzing
      * @param{Page=} src_page page containing the source event (e.g. link)
      */
      navigate_to_dest : function(detail_str, src_page) {
        try {
          var detail = JSON.parse(detail_str);
        } catch(e) {
          return;
        }
    
        if(!(detail instanceof Array)) return;
    
        var target_page_no = detail[0];
        var page_map = this.page_map;
        if (!(target_page_no in page_map)) return;
        var target_page_idx = page_map[target_page_no];
        var target_page = this.pages[target_page_idx];
    
        for (var i = 2, l = detail.length; i < l; ++i) {
          var d = detail[i];
          if(!((d === null) || (typeof d === 'number')))
            return;
        }
    
        while(detail.length < 6)
          detail.push(null);
    
        // cur_page might be undefined, e.g. from Outline
        var cur_page = src_page || this.pages[this.cur_page_idx];
    
        var cur_pos = cur_page.view_position();
        cur_pos = transform(cur_page.ictm, [cur_pos[0], cur_page.height()-cur_pos[1]]);
    
        var zoom = this.scale;
        var pos = [0,0];
        var upside_down = true;
        var ok = false;
    
        // position specified in `detail` are in the raw coordinate system of the page (unscaled)
        var scale = this.scale;
        // TODO: fitb*
        // TODO: BBox
        switch(detail[1]) {
          case 'XYZ':
            pos = [ (detail[2] === null) ? cur_pos[0] : detail[2] * scale
                  , (detail[3] === null) ? cur_pos[1] : detail[3] * scale ];
            zoom = detail[4];
            if ((zoom === null) || (zoom === 0))
              zoom = this.scale;
            ok = true;
            break;
          case 'Fit':
          case 'FitB':
            pos = [0,0];
            ok = true;
            break;
          case 'FitH':
          case 'FitBH':
            pos = [0, (detail[2] === null) ? cur_pos[1] : detail[2] * scale];
            ok = true;
            break;
          case 'FitV':
          case 'FitBV':
            pos = [(detail[2] === null) ? cur_pos[0] : detail[2] * scale, 0];
            ok = true;
            break;
          case 'FitR':
            /* locate the top-left corner of the rectangle */
            // TODO
            pos = [detail[2] * scale, detail[5] * scale];
            upside_down = false;
            ok = true;
            break;
          default:
            break;
        }
    
        if (!ok) return;
    
        this.rescale(zoom, false);
    
        var self = this;
        /**
        * page should of type Page 
        * @param{Page} page 
        */
        var transform_and_scroll = function(page) {
          pos = transform(page.ctm, pos);
          if (upside_down) {
            pos[1] = page.height() - pos[1];
          }
          self.scroll_to(target_page_idx, pos);
        };
    
        if (target_page.loaded) {
          transform_and_scroll(target_page);
        } else {
          // TODO: scroll_to may finish before load_page
    
          // Scroll to the exact position once loaded.
          this.load_page(target_page_idx, undefined, transform_and_scroll);
    
          // In the meantime page gets loaded, scroll approximately position for maximum responsiveness.
          this.scroll_to(target_page_idx);
        }
      }, 
    
      /**
      * @param{number} page_idx
      * @param{Array.<number>=} pos [x,y] where (0,0) is the top-left corner
      */
      scroll_to : function(page_idx, pos) {
        var pl = this.pages;
        if ((page_idx < 0) || (page_idx >= pl.length)) return;
        var target_page = pl[page_idx];
        var cur_target_pos = target_page.view_position();
    
        if (pos === undefined)
          pos = [0,0];
    
        var container = this.container;
        container.scrollLeft += pos[0] - cur_target_pos[0];
        container.scrollTop += pos[1] - cur_target_pos[1];
      },
    
      /**
      * generate the hash for the current view
      */
      get_current_view_hash : function() {
        var detail = [];
        var cur_page = this.pages[this.cur_page_idx];
    
        detail.push(cur_page.num);
        detail.push('XYZ');
    
        var cur_pos = cur_page.view_position();
        cur_pos = transform(cur_page.ictm, [cur_pos[0], cur_page.height()-cur_pos[1]]);
        detail.push(cur_pos[0] / this.scale);
        detail.push(cur_pos[1] / this.scale);
        
        detail.push(this.scale);
    
        return JSON.stringify(detail);
      }
    };
    
    // export pdf2htmlEX.Viewer
    pdf2htmlEX['Viewer'] = Viewer;
    </script>
    <script>
    try{
    pdf2htmlEX.defaultViewer = new pdf2htmlEX.Viewer({});
    }catch(e){}
    </script>
    
  </head>
  <body>
    <div class="row">
      <div class="span12">
        <h3><a href="/">Home</a> 
          >> <a href="/#writing">Writing</a> 
          >> 
        </h3>
        <h1>The Developing Mind: A Philosophical Introduction (2020)</h1>
        <p>by Stephen A. Butterfill<br/>--- London: Routledge <a href="https://www.routledge.com/The-Developing-Mind-A-Philosophical-Introduction/Butterfill/p/book/9780415566230" target="_blank">[publisher's page]</a><br/>--- links: <a href="/pdf/developing_mind_contents.pdf">contents [pdf]</a>; <a href="/pdf/developing_mind_introduction.pdf">introduction [pdf]</a></p>
        <div style="position:relative">
          <div id="pf1" class="pf w0 h0" data-page-no="1"><div class="pc pc1 w0 h0"><div class="t m0 x0 h1 y0 ff1 fs0 fc0 sc0 ls2">Contents</div><div class="t m0 x0 h2 y1 ff2 fs1 fc0 sc0 ls2 ws0">Preface xi</div><div class="t m0 x0 h2 y2 ff2 fs1 fc0 sc0 ls2 ws1">1 Introduction<span class="_ _0"> </span>1</div><div class="t m0 x1 h3 y3 ff1 fs1 fc0 sc0 ls0 ws2">1<span class="_ _1"></span>.<span class="_ _1"></span>1 T<span class="_ _2"></span>w<span class="_ _1"></span>o<span class="_ _3"></span>B<span class="_ _1"></span>r<span class="_ _1"></span>e<span class="_ _1"></span>a<span class="_ _1"></span>k<span class="_ _1"></span>t<span class="_ _1"></span>h<span class="_ _1"></span>r<span class="_ _1"></span>o<span class="_ _1"></span>u<span class="_ _1"></span>g<span class="_ _1"></span>h<span class="_ _1"></span>s<span class="_ _4"></span>.......................<span class="_ _5"> </span>2<span class="_ _1"></span></div><div class="t m0 x1 h3 y4 ff1 fs1 fc0 sc0 ls0 ws2">1<span class="_ _1"></span>.<span class="_ _1"></span>2 K<span class="_ _1"></span>n<span class="_ _1"></span>o<span class="_ _1"></span>w<span class="_ _1"></span>l<span class="_ _1"></span>e<span class="_ _1"></span>d<span class="_ _1"></span>g<span class="_ _1"></span>e............................<span class="_ _5"> </span>3<span class="_ _1"></span></div><div class="t m0 x1 h3 y5 ff1 fs1 fc0 sc0 ls2 ws3">1.3<span class="_ _6"> </span>A<span class="_ _7"> </span>Crude<span class="_ _7"> </span>Picture<span class="_ _7"> </span>of<span class="_ _7"> </span>the<span class="_ _7"> </span>Mind<span class="_ _8"> </span>. . . . . . . . . . . . . . . . . .<span class="_ _9"> </span>4</div><div class="t m0 x1 h3 y6 ff1 fs1 fc0 sc0 ls0 ws2">1<span class="_ _1"></span>.<span class="_ _1"></span>4 C<span class="_ _1"></span>o<span class="_ _1"></span>r<span class="_ _1"></span>e<span class="_ _3"></span>K<span class="_ _1"></span>n<span class="_ _1"></span>o<span class="_ _1"></span>w<span class="_ _1"></span>l<span class="_ _1"></span>e<span class="_ _1"></span>d<span class="_ _1"></span>g<span class="_ _1"></span>e<span class="_ _a"></span>.........................<span class="_ _5"> </span>5<span class="_ _1"></span></div><div class="t m0 x1 h3 y7 ff1 fs1 fc0 sc0 ls0 ws2">1<span class="_ _1"></span>.<span class="_ _1"></span>5 T<span class="_ _2"></span>w<span class="_ _1"></span>o<span class="_ _3"></span>S<span class="_ _1"></span>t<span class="_ _1"></span>o<span class="_ _1"></span>r<span class="_ _1"></span>i<span class="_ _1"></span>e<span class="_ _1"></span>s<span class="_ _b"></span>............................<span class="_ _c"> </span>7<span class="_ _1"></span></div><div class="t m0 x1 h3 y8 ff1 fs1 fc0 sc0 ls2 ws3">1.6<span class="_ _6"> </span>Development<span class="_ _7"> </span>Is<span class="_ _7"> </span>Rediscov<span class="_ _d"></span>ery<span class="_ _e"> </span>. . . . . . . . . . . . . . . . . .<span class="_ _f"> </span>8</div><div class="t m0 x0 h4 y9 ff2 fs2 fc0 sc0 ls2 ws4">I<span class="_ _10"> </span>Physical Obje<span class="_ _11"></span>cts<span class="_ _12"> </span>11</div><div class="t m0 x0 h2 ya ff2 fs1 fc0 sc0 ls2 ws5">2<span class="_ _13"> </span>Principles of Object Perception<span class="_ _14"> </span>13</div><div class="t m0 x1 h3 yb ff1 fs1 fc0 sc0 ls2 ws3">2.1<span class="_ _6"> </span>Knowledge<span class="_ _7"> </span>of<span class="_ _7"> </span>Objects<span class="_ _7"> </span>Involves<span class="_ _7"> </span>Three<span class="_ _7"> </span>Abilities<span class="_ _15"> </span>. . . . . . . .<span class="_ _c"> </span>13</div><div class="t m0 x1 h3 yc ff1 fs1 fc0 sc0 ls0 ws6">2<span class="_ _1"></span>.<span class="_ _1"></span>2<span class="_ _8"> </span>S<span class="_ _1"></span>e<span class="_ _1"></span>g<span class="_ _1"></span>m<span class="_ _1"></span>e<span class="_ _1"></span>n<span class="_ _1"></span>t<span class="_ _1"></span>a<span class="_ _1"></span>t<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n ..........................<span class="_ _16"> </span>1<span class="_ _1"></span>5<span class="_ _1"></span></div><div class="t m0 x1 h3 yd ff1 fs1 fc0 sc0 ls2 ws3">2.3<span class="_ _6"> </span>Principles<span class="_ _7"> </span>of<span class="_ _7"> </span>Object<span class="_ _7"> </span>Perception<span class="_ _17"> </span>. . . . . . . . . . . . . . . . .<span class="_ _c"> </span>20</div><div class="t m0 x1 h3 ye ff1 fs1 fc0 sc0 ls0 ws2">2<span class="_ _1"></span>.<span class="_ _1"></span>4 C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _d"></span>............................<span class="_ _16"> </span>2<span class="_ _1"></span>3<span class="_ _1"></span></div><div class="t m0 x0 h2 yf ff2 fs1 fc0 sc0 ls1">3<span class="ff3 ls2">�<span class="ff2 ws5">e Simple View<span class="_ _18"> </span>25</span></span></div><div class="t m0 x1 h3 y10 ff1 fs1 fc0 sc0 ls0 ws2">3<span class="_ _1"></span>.<span class="_ _1"></span>1 T<span class="_ _1"></span>h<span class="_ _1"></span>e<span class="_ _3"></span>S<span class="_ _1"></span>i<span class="_ _1"></span>m<span class="_ _1"></span>p<span class="_ _1"></span>l<span class="_ _1"></span>e<span class="_ _3"></span>V<span class="_ _1"></span>i<span class="_ _1"></span>e<span class="_ _1"></span>w<span class="_ _19"></span>.........................<span class="_ _16"> </span>2<span class="_ _1"></span>5<span class="_ _1"></span></div><div class="t m0 x1 h3 y11 ff1 fs1 fc0 sc0 ls0 ws2">3<span class="_ _1"></span>.<span class="_ _1"></span>2 P<span class="_ _1"></span>e<span class="_ _1"></span>r<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>s<span class="_ _1"></span>t<span class="_ _1"></span>e<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>e............................<span class="_ _1a"> </span>2<span class="_ _1"></span>7<span class="_ _1"></span></div><div class="t m0 x1 h3 y12 ff1 fs1 fc0 sc0 ls2 ws3">3.3<span class="_ _6"> </span>Extending<span class="_ _7"> </span>the<span class="_ _7"> </span>Simple<span class="_ _7"> </span>View<span class="_ _7"> </span>to<span class="_ _7"> </span>Persistence<span class="_ _1b"> </span>. . . . . . . . . . .<span class="_ _c"> </span>32</div><div class="t m0 x1 h3 y13 ff1 fs1 fc0 sc0 ls2 ws3">3.4<span class="_ _6"> </span>Causal<span class="_ _7"> </span>Interactions<span class="_ _16"> </span>. . . . . . . . . . . . . . . . . . . . . . .<span class="_ _c"> </span>34</div><div class="t m0 x1 h3 y14 ff1 fs1 fc0 sc0 ls2 ws3">3.5<span class="_ _6"> </span>The<span class="_ _7"> </span>Case<span class="_ _7"> </span>for<span class="_ _7"> </span>the<span class="_ _7"> </span>Simple<span class="_ _7"> </span>View<span class="_ _1a"> </span>. . . . . . . . . . . . . . . . .<span class="_ _c"> </span>36</div><div class="t m0 x0 h2 y15 ff2 fs1 fc0 sc0 ls1">4<span class="ff3 ls2">�<span class="ff2 ws5">e Linking Problem<span class="_ _1c"> </span>41</span></span></div><div class="t m0 x1 h3 y16 ff1 fs1 fc0 sc0 ls2 ws3">4.1<span class="_ _6"> </span>Against<span class="_ _7"> </span>the<span class="_ _7"> </span>Simple<span class="_ _7"> </span>View<span class="_ _1d"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _c"> </span>42</div><div class="t m0 x1 h3 y17 ff1 fs1 fc0 sc0 ls2 ws3">4.2<span class="_ _6"> </span>Further<span class="_ _7"> </span>Evidence<span class="_ _7"> </span>Against<span class="_ _7"> </span>the<span class="_ _7"> </span>Simple<span class="_ _7"> </span>View<span class="_ _8"> </span>. . . . . . . . . .<span class="_ _1e"> </span>46</div><div class="t m0 x1 h3 y18 ff1 fs1 fc0 sc0 ls2 ws3">4.3<span class="_ _6"> </span>Things<span class="_ _7"> </span>Get<span class="_ _7"> </span>Even<span class="_ _7"> </span>W<span class="_ _1f"></span>orse<span class="_ _7"> </span>for<span class="_ _7"> </span>the<span class="_ _7"> </span>Simple<span class="_ _7"> </span>View<span class="_ _1d"> </span>. . . . . . . . .<span class="_ _c"> </span>48</div><div class="t m0 x1 h3 y19 ff1 fs1 fc0 sc0 ls2 ws3">4.4<span class="_ _6"> </span>The<span class="_ _7"> </span>Linking<span class="_ _7"> </span>Problem<span class="_ _1d"> </span>. . . . . . . . . . . . . . . . . . . . . .<span class="_ _1e"> </span>50</div><div class="t m0 x1 h3 y1a ff1 fs1 fc0 sc0 ls2 ws3">4.5<span class="_ _6"> </span>Representation<span class="_ _7"> </span>Not<span class="_ _7"> </span>Knowledge<span class="_ _20"> </span>. . . . . . . . . . . . . . . .<span class="_ _1e"> </span>51</div><div class="t m0 x1 h3 y1b ff1 fs1 fc0 sc0 ls2 ws3">4.6<span class="_ _6"> </span>Graded<span class="_ _7"> </span>Representations?<span class="_ _8"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _5"> </span>53</div><div class="t m0 x2 h5 y1c ff1 fs3 fc0 sc0 ls2">v</div><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:593.008000px;width:42.221000px;height:10.516000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:565.895000px;width:89.038000px;height:10.630000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:548.472000px;width:126.207000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:533.667000px;width:84.412000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:521.516000px;width:168.385000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:504.058000px;width:110.845000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:491.907000px;width:87.185000px;height:10.032000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:474.448000px;width:168.373000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:432.024000px;width:127.908000px;height:15.421000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:405.481000px;width:188.038000px;height:13.273000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:390.701000px;width:256.076000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:375.896000px;width:96.642000px;height:12.686000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:361.091000px;width:180.758000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:348.940000px;width:84.747000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:319.263000px;width:109.971000px;height:13.272000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:304.482000px;width:112.793000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:292.331000px;width:84.376000px;height:10.033000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:274.872000px;width:232.955000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:262.722000px;width:123.661000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:245.263000px;width:174.685000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:218.240000px;width:134.192000px;height:13.272000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:203.459000px;width:150.189000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:188.654000px;width:236.936000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:173.849000px;width:245.017000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:159.044000px;width:132.914000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:144.239000px;width:182.061000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:129.435000px;width:151.480000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
          <div id="pf2" class="pf w0 h0" data-page-no="2"><div class="pc pc2 w0 h0"><div class="t m0 x1 h3 y1d ff1 fs1 fc0 sc0 ls0 ws2">4<span class="_ _1"></span>.<span class="_ _1"></span>7 C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _d"></span>............................<span class="_ _16"> </span>5<span class="_ _1"></span>5<span class="_ _1"></span></div><div class="t m0 x0 h2 y1e ff2 fs1 fc0 sc0 ls2 ws5">5<span class="_ _13"> </span>Core Knowledge<span class="_ _21"> </span>57</div><div class="t m0 x1 h3 y1f ff1 fs1 fc0 sc0 ls2 ws3">5.1<span class="_ _6"> </span>What<span class="_ _7"> </span>Is<span class="_ _7"> </span>Core<span class="_ _7"> </span>Knowledge?<span class="_ _1a"> </span>. . . . . . . . . . . . . . . . . . .<span class="_ _c"> </span>58</div><div class="t m0 x1 h3 y20 ff1 fs1 fc0 sc0 ls2 ws5">5.2<span class="_ _6"> </span>Can Core Knowledge Solve the Linking Pr<span class="_ _d"></span>oblem?<span class="_ _1d"> </span>.<span class="_ _22"> </span>.<span class="_ _22"> </span>.<span class="_ _22"> </span>.<span class="_ _22"> </span>.<span class="_ _22"> </span>.<span class="_ _c"> </span>60</div><div class="t m0 x1 h3 y21 ff1 fs1 fc0 sc0 ls2 ws5">5.3<span class="_ _6"> </span>How Not to De<span class="ff4"></span><span class="ls0 ws8">n<span class="_ _1"></span>e<span class="_ _3"></span>S<span class="_ _1"></span>o<span class="_ _1"></span>m<span class="_ _1"></span>e<span class="_ _1"></span>t<span class="_ _1"></span>h<span class="_ _1"></span>i<span class="_ _1"></span>n<span class="_ _1"></span>g<span class="_ _23"></span>................. 6<span class="_ _1"></span>2<span class="_ _1"></span></span></div><div class="t m0 x1 h3 y22 ff1 fs1 fc0 sc0 ls2 ws3">5.4<span class="_ _6"> </span>Will<span class="_ _7"> </span>Invoking<span class="_ _7"> </span>Modularity<span class="_ _7"> </span>Help?<span class="_ _24"> </span>. . . . . . . . . . . . . . . .<span class="_ _c"> </span>63</div><div class="t m0 x1 h3 y23 ff1 fs1 fc0 sc0 ls0 ws2">5<span class="_ _1"></span>.<span class="_ _1"></span>5 C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _d"></span>............................<span class="_ _1a"> </span>6<span class="_ _1"></span>4<span class="_ _1"></span></div><div class="t m0 x0 h2 y24 ff2 fs1 fc0 sc0 ls2 ws5">6<span class="_ _13"> </span>Object Indexes and Motor Representations of Objects<span class="_ _25"> </span>67</div><div class="t m0 x1 h3 y25 ff1 fs1 fc0 sc0 ls2 ws3">6.1<span class="_ _6"> </span>Object<span class="_ _7"> </span>Indexes<span class="_ _7"> </span>in<span class="_ _7"> </span>Adult<span class="_ _7"> </span>Humans<span class="_ _26"> </span>. . . . . . . . . . . . . . . .<span class="_ _c"> </span>68</div><div class="t m0 x1 h3 y26 ff1 fs1 fc0 sc0 ls2 ws5">6.2<span class="_ _6"> </span>Object Indexes and the Principles of Object Perception<span class="_ _1a"> </span>.<span class="_ _22"> </span>.<span class="_ _22"> </span>.<span class="_ _c"> </span>70</div><div class="t m0 x1 h3 y27 ff1 fs1 fc0 sc0 ls2 ws3">6.3<span class="_ _6"> </span>The<span class="_ _7"> </span>CLSTX<span class="_ _7"> </span>Conjecture<span class="_ _27"> </span>. . . . . . . . . . . . . . . . . . . . .<span class="_ _1e"> </span>74</div><div class="t m0 x1 h3 y28 ff1 fs1 fc0 sc0 ls0 ws2">6<span class="_ _1"></span>.<span class="_ _1"></span>4 S<span class="_ _1"></span>i<span class="_ _1"></span>g<span class="_ _1"></span>n<span class="_ _1"></span>a<span class="_ _1"></span>t<span class="_ _1"></span>u<span class="_ _1"></span>r<span class="_ _1"></span>e<span class="_ _3"></span>L<span class="_ _1"></span>i<span class="_ _1"></span>m<span class="_ _1"></span>i<span class="_ _1"></span>t<span class="_ _1"></span>s<span class="_ _11"></span>.........................<span class="_ _16"> </span>7<span class="_ _1"></span>5<span class="_ _1"></span></div><div class="t m0 x1 h3 y29 ff1 fs1 fc0 sc0 ls2 ws3">6.5<span class="_ _6"> </span>Knowledge<span class="_ _7"> </span>or<span class="_ _7"> </span>Core<span class="_ _7"> </span>Knowledge<span class="_ _7"> </span>or<span class="_ _7"> </span>…?<span class="_ _15"> </span>. . . . . . . . . . . . .<span class="_ _1e"> </span>79</div><div class="t m0 x1 h3 y2a ff1 fs1 fc0 sc0 ls2 ws3">6.6<span class="_ _6"> </span>Against<span class="_ _7"> </span>the<span class="_ _7"> </span>CLSTX<span class="_ _7"> </span>Conjecture<span class="_ _17"> </span>. . . . . . . . . . . . . . . . .<span class="_ _1e"> </span>80</div><div class="t m0 x1 h3 y2b ff1 fs1 fc0 sc0 ls2 ws3">6.7<span class="_ _6"> </span>Motor<span class="_ _7"> </span>Representations<span class="_ _7"> </span>of<span class="_ _7"> </span>Objects<span class="_ _28"> </span>. . . . . . . . . . . . . . .<span class="_ _1e"> </span>81</div><div class="t m0 x1 h3 y2c ff1 fs1 fc0 sc0 ls0 ws2">6<span class="_ _1"></span>.<span class="_ _1"></span>8 C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>j<span class="_ _1"></span>e<span class="_ _1"></span>c<span class="_ _1"></span>t<span class="_ _1"></span>u<span class="_ _1"></span>r<span class="_ _1"></span>e<span class="_ _3"></span>O<span class="_ _29"></span>...........................<span class="_ _16"> </span>8<span class="_ _1"></span>3<span class="_ _1"></span></div><div class="t m0 x1 h3 y2d ff1 fs1 fc0 sc0 ls2 ws3">6.9<span class="_ _6"> </span>Conclusion:<span class="_ _1b"> </span>Paradox<span class="_ _7"> </span>Lost<span class="_ _17"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _1e"> </span>86</div><div class="t m0 x0 h2 y2e ff2 fs1 fc0 sc0 ls2 ws5">7<span class="_ _13"> </span>Metacognitive Feelings<span class="_ _2a"> </span>89</div><div class="t m0 x1 h3 y2f ff1 fs1 fc0 sc0 ls2 ws3">7.1<span class="_ _6"> </span>Objection<span class="_ _7"> </span>to<span class="_ _7"> </span>Conjecture<span class="_ _7"> </span>O<span class="_ _16"> </span>. . . . . . . . . . . . . . . . . . .<span class="_ _c"> </span>89</div><div class="t m0 x1 h3 y30 ff1 fs1 fc0 sc0 ls2 ws3">7.2<span class="_ _6"> </span>Metacognitive<span class="_ _7"> </span>Feelings:<span class="_ _1b"> </span>A<span class="_ _7"> </span>First<span class="_ _7"> </span>Example<span class="_ _2b"> </span>. . . . . . . . . . .<span class="_ _1e"> </span>91</div><div class="t m0 x1 h3 y31 ff1 fs1 fc0 sc0 ls2 ws3">7.3<span class="_ _6"> </span>More<span class="_ _7"> </span>Metacognitive<span class="_ _7"> </span>Feelings<span class="_ _2c"> </span>. . . . . . . . . . . . . . . . . .<span class="_ _c"> </span>92</div><div class="t m0 x1 h3 y32 ff1 fs1 fc0 sc0 ls2 ws3">7.4<span class="_ _6"> </span>What<span class="_ _7"> </span>Is<span class="_ _7"> </span>a<span class="_ _7"> </span>Metacognitive<span class="_ _7"> </span>Feeling?<span class="_ _2b"> </span>. . . . . . . . . . . . . . .<span class="_ _1e"> </span>94</div><div class="t m0 x1 h3 y33 ff1 fs1 fc0 sc0 ls2 ws3">7.5<span class="_ _6"> </span>A<span class="_ _7"> </span>Metacognitive<span class="_ _7"> </span>Feeling<span class="_ _7"> </span>of<span class="_ _7"> </span>Surprise?<span class="_ _28"> </span>. . . . . . . . . . . . .<span class="_ _1e"> </span>96</div><div class="t m0 x1 h6 y34 ff1 fs1 fc0 sc0 ls2 ws5">7.6<span class="_ _6"> </span>Conjecture O<span class="fs4 ls3 v1">m</span><span class="ls0 ws9">.......................... </span>97</div><div class="t m0 x1 h3 y35 ff1 fs1 fc0 sc0 ls2 ws3">7.7<span class="_ _6"> </span>Metacognitive<span class="_ _7"> </span>Feelings<span class="_ _7"> </span>are<span class="_ _7"> </span>Intentional<span class="_ _7"> </span>Isolators<span class="_ _28"> </span>. . . . . . .<span class="_ _c"> </span>99</div><div class="t m0 x1 h3 y36 ff1 fs1 fc0 sc0 ls0 wsa">7<span class="_ _1"></span>.<span class="_ _1"></span>8<span class="_ _8"> </span>C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _a"></span>............................ 1<span class="_ _1"></span>0<span class="_ _1"></span>1<span class="_ _1"></span></div><div class="t m0 x0 h2 y37 ff2 fs1 fc0 sc0 ls2 ws5">8<span class="_ _13"> </span>Conclusion to Part I<span class="_ _2d"> </span>103</div><div class="t m0 x1 h3 y38 ff1 fs1 fc0 sc0 ls2 ws3">8.1<span class="_ _6"> </span>What<span class="_ _7"> </span>Is<span class="_ _7"> </span>an<span class="_ _7"> </span>Expectation?<span class="_ _1a"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>103</div><div class="t m0 x1 h3 y39 ff1 fs1 fc0 sc0 ls2 ws3">8.2<span class="_ _6"> </span>Core<span class="_ _7"> </span>Knowledge:<span class="_ _1b"> </span>A<span class="_ _7"> </span>Lighter<span class="_ _7"> </span>A<span class="_ _d"></span>ccount<span class="_ _20"> </span>. . . . . . . . . . . . .<span class="_ _1a"> </span>105</div><div class="t m0 x1 h3 y3a ff1 fs1 fc0 sc0 ls2 ws3">8.3<span class="_ _6"> </span>Development<span class="_ _7"> </span>Is<span class="_ _7"> </span>Rediscov<span class="_ _d"></span>ery<span class="_ _e"> </span>. . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>106</div><div class="t m0 x1 h3 y3b ff1 fs1 fc0 sc0 ls2 ws3">8.4<span class="_ _6"> </span>How<span class="_ _7"> </span>Does<span class="_ _7"> </span>Rediscovery<span class="_ _7"> </span>Occur?<span class="_ _26"> </span>. . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>108</div><div class="t m0 x0 h2 y3c ff2 fs1 fc0 sc0 ls2 ws1">9 Innateness<span class="_ _2e"> </span>113</div><div class="t m0 x1 h3 y3d ff1 fs1 fc0 sc0 ls0 wsb">9<span class="_ _1"></span>.<span class="_ _1"></span>1<span class="_ _8"> </span>S<span class="_ _1"></span>y<span class="_ _1"></span>n<span class="_ _1"></span>t<span class="_ _1"></span>a<span class="_ _1"></span>x ..............................<span class="_ _26"> </span>1<span class="_ _1"></span>1<span class="_ _1"></span>4<span class="_ _1"></span></div><div class="t m0 x1 h3 y3e ff1 fs1 fc0 sc0 ls2 ws3">9.2<span class="_ _6"> </span>A<span class="_ _7"> </span>Poverty<span class="_ _7"> </span>of<span class="_ _7"> </span>Stimulus<span class="_ _7"> </span>Argument<span class="_ _7"> </span>. . . . . . . . . . . . . . . .<span class="_ _2f"> </span>115</div><div class="t m0 x1 h3 y3f ff1 fs1 fc0 sc0 ls2 ws3">9.3<span class="_ _6"> </span>The<span class="_ _7"> </span>Poverty<span class="_ _7"> </span>of<span class="_ _7"> </span>Po<span class="_ _d"></span>verty<span class="_ _7"> </span>of<span class="_ _7"> </span>Stimulus<span class="_ _7"> </span>Arguments<span class="_ _7"> </span>. . . . . . . .<span class="_ _2f"> </span>118</div><div class="t m0 x1 h3 y40 ff1 fs1 fc0 sc0 ls2 ws3">9.4<span class="_ _6"> </span>Is<span class="_ _7"> </span>Core<span class="_ _7"> </span>Knowledge<span class="_ _7"> </span>Innate?<span class="_ _15"> </span>. . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>119</div><div class="t m0 x1 h3 y41 ff1 fs1 fc0 sc0 ls2 ws3">9.5<span class="_ _6"> </span>Syntax<span class="_ _7"> </span>and<span class="_ _7"> </span>Rediscovery<span class="_ _17"> </span>. . . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>120</div><div class="t m0 x1 h3 y42 ff1 fs1 fc0 sc0 ls0 wsc">9<span class="_ _1"></span>.<span class="_ _1"></span>6<span class="_ _8"> </span>C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _a"></span>............................ 1<span class="_ _1"></span>2<span class="_ _1"></span>2<span class="_ _1"></span></div><div class="t m0 x3 h5 y1c ff1 fs3 fc0 sc0 ls2">vi</div><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:743.756000px;width:84.747000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:713.958000px;width:109.361000px;height:13.272000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:699.105000px;width:157.278000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:684.227000px;width:270.912000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:669.350000px;width:177.770000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:654.473000px;width:186.509000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:642.250000px;width:84.747000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:612.452000px;width:306.789000px;height:13.273000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:597.707000px;width:188.852000px;height:13.003000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:582.722000px;width:295.325000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:567.952000px;width:143.088000px;height:13.004000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:552.968000px;width:109.936000px;height:12.686000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:538.091000px;width:212.822000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:523.214000px;width:180.483000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:508.336000px;width:193.192000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:493.567000px;width:94.717000px;height:12.579000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:481.236000px;width:154.513000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:451.438000px;width:144.246000px;height:13.273000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:436.693000px;width:158.103000px;height:13.003000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:421.708000px;width:226.878000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:406.831000px;width:170.967000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:391.954000px;width:192.450000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:377.077000px;width:210.264000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:362.307000px;width:101.512000px;height:12.604000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:347.323000px;width:261.898000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:335.099000px;width:84.747000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:308.057000px;width:128.717000px;height:10.517000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:290.448000px;width:148.970000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:275.571000px;width:208.001000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:260.694000px;width:168.373000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:245.817000px;width:180.603000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:221.429000px;width:78.637000px;height:10.003000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:203.820000px;width:63.024000px;height:12.687000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:188.943000px;width:190.370000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:174.066000px;width:259.328000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:159.189000px;width:161.331000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:144.312000px;width:146.423000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:132.089000px;width:84.747000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
          <div id="pf3" class="pf w0 h0" data-page-no="3"><div class="pc pc3 w0 h0"><div class="t m0 x0 h4 y1d ff2 fs2 fc0 sc0 ls2 ws4">Interlude on Innateness<span class="_ _30"> </span>113</div><div class="t m0 x0 h4 y43 ff2 fs2 fc0 sc0 ls2 ws4">II<span class="_ _10"> </span>Minds and Actions<span class="_ _31"> </span>125</div><div class="t m0 x0 h2 y44 ff2 fs1 fc0 sc0 ls2 wsd">10 Action<span class="_ _32"> </span>127</div><div class="t m0 x1 h3 y45 ff1 fs1 fc0 sc0 ls2 ws3">10.1<span class="_ _e"> </span>T<span class="_ _d"></span>racking<span class="_ _7"> </span>vs<span class="_ _7"> </span>Knowing<span class="_ _27"> </span>. . . . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>127</div><div class="t m0 x1 h3 y46 ff1 fs1 fc0 sc0 ls2 ws3">10.2<span class="_ _e"> </span>Three-month-olds<span class="_ _7"> </span>T<span class="_ _d"></span>rack<span class="_ _7"> </span>the<span class="_ _7"> </span>Goals<span class="_ _7"> </span>of<span class="_ _7"> </span>Actions<span class="_ _2f"> </span>. . . . . . . .<span class="_ _2f"> </span>128</div><div class="t m0 x1 h3 y47 ff1 fs1 fc0 sc0 ls2 ws3">10.3<span class="_ _e"> </span>Pure<span class="_ _7"> </span>Goal<span class="_ _7"> </span>T<span class="_ _d"></span>racking<span class="_ _1d"> </span>. . . . . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>131</div><div class="t m0 x1 h3 y48 ff1 fs1 fc0 sc0 ls2 ws3">10.4<span class="_ _e"> </span>The<span class="_ _7"> </span>T<span class="_ _a"></span>eleological<span class="_ _7"> </span>Stance<span class="_ _33"> </span>. . . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>134</div><div class="t m0 x1 h3 y49 ff1 fs1 fc0 sc0 ls2 ws3">10.5<span class="_ _e"> </span>Statistical<span class="_ _7"> </span>Regularities<span class="_ _34"> </span>. . . . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>137</div><div class="t m0 x1 h3 y4a ff1 fs1 fc0 sc0 ls2 ws3">10.6<span class="_ _e"> </span>A<span class="_ _7"> </span>Methodological<span class="_ _7"> </span>Explanation?<span class="_ _16"> </span>. . . . . . . . . . . . . . . .<span class="_ _35"> </span>141</div><div class="t m0 x1 h3 y4b ff1 fs1 fc0 sc0 ls2 ws3">10.7<span class="_ _e"> </span>A<span class="_ _7"> </span>Second<span class="_ _7"> </span>Puzzle:<span class="_ _1b"> </span>Acting<span class="_ _7"> </span>and<span class="_ _7"> </span>T<span class="_ _a"></span>racking<span class="_ _20"> </span>. . . . . . . . . . . .<span class="_ _1a"> </span>142</div><div class="t m0 x1 h3 y4c ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>0<span class="_ _1"></span>.<span class="_ _1"></span>8 C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _d"></span>............................<span class="_ _17"> </span>1<span class="_ _1"></span>4<span class="_ _1"></span>5<span class="_ _1"></span></div><div class="t m0 x0 h2 y4d ff2 fs1 fc0 sc0 ls2 ws5">11<span class="_ _33"> </span>A <span class="ff3">�</span>eory of Goal Tracking<span class="_ _36"> </span>147</div><div class="t m0 x1 h3 y4e ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>1<span class="_ _1"></span>.<span class="_ _1"></span>1 T<span class="_ _1"></span>h<span class="_ _1"></span>e<span class="_ _3"></span>S<span class="_ _1"></span>i<span class="_ _1"></span>m<span class="_ _1"></span>p<span class="_ _1"></span>l<span class="_ _1"></span>e<span class="_ _3"></span>V<span class="_ _1"></span>i<span class="_ _1"></span>e<span class="_ _1"></span>w<span class="_ _19"></span>.........................<span class="_ _26"> </span>1<span class="_ _1"></span>4<span class="_ _1"></span>7<span class="_ _1"></span></div><div class="t m0 x1 h3 y4f ff1 fs1 fc0 sc0 ls2 ws3">11.2<span class="_ _e"> </span>The<span class="_ _7"> </span>Motor<span class="_ _7"> </span>Theory<span class="_ _7"> </span>of<span class="_ _7"> </span>Goal<span class="_ _7"> </span>Tracking<span class="_ _37"> </span>. . . . . . . . . . . . . .<span class="_ _1a"> </span>148</div><div class="t m0 x1 h3 y50 ff1 fs1 fc0 sc0 ls2 ws3">11.3<span class="_ _e"> </span>The<span class="_ _7"> </span>Motor<span class="_ _7"> </span>Theory<span class="_ _7"> </span>and<span class="_ _7"> </span>the<span class="_ _7"> </span>T<span class="_ _a"></span>eleological<span class="_ _7"> </span>Stance<span class="_ _15"> </span>. . . . . . . .<span class="_ _2f"> </span>151</div><div class="t m0 x1 h3 y51 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>1<span class="_ _1"></span>.<span class="_ _1"></span>4 T<span class="_ _2"></span>a<span class="_ _1"></span>r<span class="_ _1"></span>g<span class="_ _1"></span>e<span class="_ _1"></span>t<span class="_ _3"></span>v<span class="_ _1"></span>s<span class="_ _3"></span>G<span class="_ _1"></span>o<span class="_ _1"></span>a<span class="_ _1"></span>l<span class="_ _23"></span>..........................<span class="_ _26"> </span>1<span class="_ _1"></span>5<span class="_ _1"></span>3<span class="_ _1"></span></div><div class="t m0 x1 h3 y52 ff1 fs1 fc0 sc0 ls2 ws3">11.5<span class="_ _e"> </span>A<span class="_ _7"> </span>Dual<span class="_ _7"> </span>Process<span class="_ _7"> </span>Theory<span class="_ _7"> </span>of<span class="_ _7"> </span>Goal<span class="_ _7"> </span>Tracking<span class="_ _1d"> </span>. . . . . . . . . . .<span class="_ _1a"> </span>155</div><div class="t m0 x1 h3 y53 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>1<span class="_ _1"></span>.<span class="_ _1"></span>6 P<span class="_ _1"></span>u<span class="_ _1"></span>z<span class="_ _1"></span>z<span class="_ _1"></span>l<span class="_ _1"></span>e<span class="_ _1"></span>s<span class="_ _3"></span>S<span class="_ _1"></span>o<span class="_ _1"></span>l<span class="_ _1"></span>v<span class="_ _1"></span>e<span class="_ _1"></span>d<span class="_ _1"></span>?<span class="_ _34"> </span>.........................<span class="_ _37"> </span>1<span class="_ _1"></span>5<span class="_ _1"></span>7<span class="_ _1"></span></div><div class="t m0 x1 h3 y54 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>1<span class="_ _1"></span>.<span class="_ _1"></span>7 C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _d"></span>............................<span class="_ _17"> </span>1<span class="_ _1"></span>5<span class="_ _1"></span>8<span class="_ _1"></span></div><div class="t m0 x0 h2 y55 ff2 fs1 fc0 sc0 ls2 ws5">12<span class="_ _33"> </span>Mind:<span class="_ _1b"> </span>the Puzzle<span class="_ _38"> </span>161</div><div class="t m0 x1 h3 y56 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>2<span class="_ _1"></span>.<span class="_ _1"></span>1 A<span class="_ _1"></span>l<span class="_ _1"></span>l<span class="_ _3"></span>A<span class="_ _1"></span>b<span class="_ _1"></span>o<span class="_ _1"></span>u<span class="_ _1"></span>t<span class="_ _b"></span>M<span class="_ _1"></span>a<span class="_ _1"></span>x<span class="_ _1"></span>i<span class="_ _37"> </span>.........................<span class="_ _26"> </span>1<span class="_ _1"></span>6<span class="_ _1"></span>2<span class="_ _1"></span></div><div class="t m0 x1 h3 y57 ff1 fs1 fc0 sc0 ls2 ws3">12.2<span class="_ _e"> </span>Infants<span class="_ _7"> </span>track<span class="_ _7"> </span>false<span class="_ _7"> </span>beliefs<span class="_ _8"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>166</div><div class="t m0 x1 h3 y58 ff1 fs1 fc0 sc0 ls2 ws3">12.3<span class="_ _e"> </span>A<span class="_ _7"> </span>Replication<span class="_ _7"> </span>Challenge<span class="_ _20"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>169</div><div class="t m0 x1 h3 y59 ff1 fs1 fc0 sc0 ls2 ws5">12.4<span class="_ _e"> </span>Methodological Defects or Truly Contradictory Responses?<span class="_ _34"> </span>.<span class="_ _2f"> </span>170</div><div class="t m0 x1 h3 y5a ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>2<span class="_ _1"></span>.<span class="_ _1"></span>5 M<span class="_ _1"></span>o<span class="_ _1"></span>d<span class="_ _1"></span>e<span class="_ _1"></span>l<span class="_ _1"></span>s ..............................<span class="_ _26"> </span>1<span class="_ _1"></span>7<span class="_ _1"></span>3<span class="_ _1"></span></div><div class="t m0 x1 h3 y5b ff1 fs1 fc0 sc0 ls2 ws3">12.6<span class="_ _e"> </span>The<span class="_ _7"> </span>Mindreading<span class="_ _7"> </span>Puzzle<span class="_ _1a"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>175</div><div class="t m0 x0 h2 y5c ff2 fs1 fc0 sc0 ls2 wsd">13 <span class="ff3">�</span><span class="ws5">ree Levels of Analysis<span class="_ _39"> </span>177</span></div><div class="t m0 x1 h3 y5d ff1 fs1 fc0 sc0 ls2 ws3">13.1<span class="_ _e"> </span>T<span class="_ _d"></span>racking<span class="_ _7"> </span>Beliefs<span class="_ _7"> </span>without<span class="_ _7"> </span>Representing<span class="_ _7"> </span>Them?<span class="_ _e"> </span>. . . . . . . .<span class="_ _35"> </span>177</div><div class="t m0 x1 h3 y5e ff1 fs1 fc0 sc0 ls2 ws3">13.2<span class="_ _e"> </span>Altercentric<span class="_ _7"> </span>Interference<span class="_ _1d"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>178</div><div class="t m0 x1 h3 y5f ff1 fs1 fc0 sc0 ls2 ws3">13.3<span class="_ _e"> </span>Mirroring<span class="_ _7"> </span>beliefs?<span class="_ _27"> </span>. . . . . . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>180</div><div class="t m0 x1 h3 y60 ff1 fs1 fc0 sc0 ls2 ws3">13.4<span class="_ _e"> </span>Three<span class="_ _7"> </span>Levels<span class="_ _7"> </span>of<span class="_ _7"> </span>Analysis<span class="_ _3a"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>182</div><div class="t m0 x1 h3 y61 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>3<span class="_ _1"></span>.<span class="_ _1"></span>5 T<span class="_ _2"></span>a<span class="_ _1"></span>s<span class="_ _1"></span>k<span class="_ _3"></span>A<span class="_ _1"></span>n<span class="_ _1"></span>a<span class="_ _1"></span>l<span class="_ _1"></span>y<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>s<span class="_ _17"> </span>..........................<span class="_ _26"> </span>1<span class="_ _1"></span>8<span class="_ _1"></span>4<span class="_ _1"></span></div><div class="t m0 x1 h3 y62 ff1 fs1 fc0 sc0 ls2 ws3">13.6<span class="_ _e"> </span>Selection<span class="_ _7"> </span>and<span class="_ _7"> </span>Inhibition<span class="_ _17"> </span>. . . . . . . . . . . . . . . . . . . . .<span class="_ _35"> </span>187</div><div class="t m0 x1 h3 y63 ff1 fs1 fc0 sc0 ls2 ws3">13.7<span class="_ _e"> </span>T<span class="_ _a"></span>oo<span class="_ _7"> </span>Much<span class="_ _7"> </span>Mindreading?<span class="_ _28"> </span>. . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>192</div><div class="t m0 x1 h3 y1b ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>3<span class="_ _1"></span>.<span class="_ _1"></span>8 W<span class="_ _1"></span>h<span class="_ _1"></span>a<span class="_ _1"></span>t<span class="_ _3"></span>N<span class="_ _1"></span>o<span class="_ _1"></span>w<span class="_ _1"></span>?<span class="_ _37"> </span>...........................<span class="_ _26"> </span>1<span class="_ _1"></span>9<span class="_ _1"></span>7<span class="_ _1"></span></div><div class="t m0 x4 h5 y1c ff1 fs3 fc0 sc0 ls2">vii</div><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:743.684000px;width:157.060000px;height:12.357000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:700.295000px;width:148.323000px;height:12.357000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:671.885000px;width:56.161000px;height:10.116000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:653.595000px;width:134.480000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:640.577000px;width:251.557000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:622.250000px;width:124.557000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:606.578000px;width:144.833000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:590.906000px;width:137.971000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:575.233000px;width:183.687000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:559.561000px;width:216.907000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:546.543000px;width:84.747000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:515.420000px;width:164.271000px;height:13.272000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:499.771000px;width:112.793000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:484.099000px;width:205.769000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:468.427000px;width:256.446000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:452.754000px;width:100.468000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:437.082000px;width:227.910000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:424.064000px;width:106.098000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:408.391000px;width:84.747000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:379.910000px;width:110.685000px;height:10.630000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:364.274000px;width:105.154000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:348.602000px;width:151.301000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:330.275000px;width:147.918000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:314.603000px;width:318.828000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:301.585000px;width:64.841000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:283.258000px;width:148.539000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:254.879000px;width:151.133000px;height:13.182000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:239.141000px;width:253.984000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:226.123000px;width:150.225000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:207.796000px;width:117.384000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:192.124000px;width:149.591000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:176.452000px;width:97.407000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:163.433000px;width:146.184000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:145.107000px;width:149.818000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:132.089000px;width:87.950000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
          <div id="pf4" class="pf w0 h0" data-page-no="4"><div class="pc pc4 w0 h0"><div class="t m0 x0 h2 y1d ff2 fs1 fc0 sc0 ls2 ws5">14<span class="_ _33"> </span>Mind:<span class="_ _1b"> </span>a Solution?<span class="_ _3b"> </span>199</div><div class="t m0 x1 h3 y64 ff1 fs1 fc0 sc0 ls2 ws3">14.1<span class="_ _e"> </span>Mindreading<span class="_ _7"> </span>Is<span class="_ _7"> </span>Sometimes<span class="_ _7"> </span>A<span class="_ _a"></span>utomatic . . . . . . . . . . . . .<span class="_ _1a"> </span>200</div><div class="t m0 x1 h3 y65 ff1 fs1 fc0 sc0 ls2 ws3">14.2<span class="_ _e"> </span>Mindreading<span class="_ _7"> </span>Is<span class="_ _7"> </span>Not<span class="_ _7"> </span>Always<span class="_ _7"> </span>A<span class="_ _a"></span>utomatic<span class="_ _16"> </span>. . . . . . . . . . . .<span class="_ _2f"> </span>201</div><div class="t m0 x1 h3 y66 ff1 fs1 fc0 sc0 ls2 ws3">14.3<span class="_ _e"> </span>A<span class="_ _7"> </span>Dual<span class="_ _7"> </span>Process<span class="_ _7"> </span>Theory<span class="_ _7"> </span>of<span class="_ _7"> </span>Mindreading<span class="_ _24"> </span>. . . . . . . . . . . .<span class="_ _1a"> </span>202</div><div class="t m0 x1 h3 y67 ff1 fs1 fc0 sc0 ls2 ws5">14.4<span class="_ _e"> </span>Speed–A<span class="_ _d"></span>ccuracy T<span class="_ _d"></span>rade-O<span class="ff4"></span><span class="ls0 wsf">s .................. 2<span class="_ _1"></span>0<span class="_ _1"></span>4<span class="_ _1"></span></span></div><div class="t m0 x1 h3 y68 ff1 fs1 fc0 sc0 ls2 ws3">14.5<span class="_ _e"> </span>What<span class="_ _7"> </span>Is<span class="_ _7"> </span>a<span class="_ _7"> </span>Model<span class="_ _7"> </span>of<span class="_ _7"> </span>Minds<span class="_ _7"> </span>and<span class="_ _7"> </span>Actions?<span class="_ _37"> </span>. . . . . . . . . . . .<span class="_ _2f"> </span>205</div><div class="t m0 x1 h3 y69 ff1 fs1 fc0 sc0 ls2 ws3">14.6<span class="_ _e"> </span>Minimal<span class="_ _7"> </span>Models<span class="_ _7"> </span>of<span class="_ _7"> </span>the<span class="_ _7"> </span>Mental<span class="_ _27"> </span>. . . . . . . . . . . . . . . . .<span class="_ _35"> </span>207</div><div class="t m0 x1 h3 y6a ff1 fs1 fc0 sc0 ls2 ws3">14.7<span class="_ _e"> </span>Signature<span class="_ _7"> </span>Limits<span class="_ _7"> </span>in<span class="_ _7"> </span>Mindreading<span class="_ _37"> </span>. . . . . . . . . . . . . . . .<span class="_ _1a"> </span>210</div><div class="t m0 x1 h3 y6b ff1 fs1 fc0 sc0 ls2 ws3">14.8<span class="_ _e"> </span>A<span class="_ _7"> </span>Developmental<span class="_ _7"> </span>Theory<span class="_ _7"> </span>of<span class="_ _7"> </span>Mindreading<span class="_ _26"> </span>. . . . . . . . . . .<span class="_ _2f"> </span>213</div><div class="t m0 x1 h3 y6c ff1 fs1 fc0 sc0 ls2 ws3">14.9<span class="_ _e"> </span>How<span class="_ _7"> </span>to<span class="_ _7"> </span>Solve<span class="_ _7"> </span>the<span class="_ _7"> </span>Mindr<span class="_ _d"></span>eading<span class="_ _7"> </span>Puzzle<span class="_ _33"> </span>. . . . . . . . . . . . .<span class="_ _1a"> </span>216</div><div class="t m0 x1 h3 y6d ff1 fs1 fc0 sc0 ls2 ws3">14.10<span class="_ _3c"> </span>T<span class="_ _a"></span>ask<span class="_ _7"> </span>Analysis<span class="_ _7"> </span>Revisited<span class="_ _33"> </span>. . . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>218</div><div class="t m0 x1 h3 y6e ff1 fs1 fc0 sc0 ls2 ws3">14.11<span class="_ _3c"> </span>Is<span class="_ _7"> </span>There<span class="_ _7"> </span>Core<span class="_ _3c"> </span>Knowledge<span class="_ _7"> </span>of<span class="_ _7"> </span>Minds? . . . . . . . . . . . . . .<span class="_ _1a"> </span>219</div><div class="t m0 x1 h3 y6f ff1 fs1 fc0 sc0 ls2 ws3">14.12<span class="_ _3c"> </span>Origins<span class="_ _7"> </span>of<span class="_ _7"> </span>Knowledge<span class="_ _7"> </span>of<span class="_ _7"> </span>Mind:<span class="_ _1b"> </span>Rediscovery<span class="_ _16"> </span>. . . . . . . . .<span class="_ _2f"> </span>219</div><div class="t m0 x0 h2 y70 ff2 fs1 fc0 sc0 ls2 ws5">15<span class="_ _33"> </span>Joint Action<span class="_ _3d"> </span>223</div><div class="t m0 x1 h3 y71 ff1 fs1 fc0 sc0 ls2 ws5">15.1<span class="_ _e"> </span>Joint Action vs Parallel but Mer<span class="_ _d"></span>ely Individual Actions<span class="_ _2c"> </span>.<span class="_ _22"> </span>.<span class="_ _22"> </span>.<span class="_ _22"> </span>.<span class="_ _35"> </span>224</div><div class="t m0 x1 h3 y72 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>5<span class="_ _1"></span>.<span class="_ _1"></span>2 S<span class="_ _1"></span>h<span class="_ _1"></span>a<span class="_ _1"></span>r<span class="_ _1"></span>e<span class="_ _1"></span>d<span class="_ _3"></span>I<span class="_ _1"></span>n<span class="_ _1"></span>t<span class="_ _1"></span>e<span class="_ _1"></span>n<span class="_ _1"></span>t<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1f"></span>.........................<span class="_ _26"> </span>2<span class="_ _1"></span>2<span class="_ _1"></span>6<span class="_ _1"></span></div><div class="t m0 x1 h3 y73 ff1 fs1 fc0 sc0 ls2 ws3">15.3<span class="_ _e"> </span>Bratman<span class="_ _7"> </span>on<span class="_ _7"> </span>Shared<span class="_ _7"> </span>Intention<span class="_ _26"> </span>. . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>227</div><div class="t m0 x1 h3 y74 ff1 fs1 fc0 sc0 ls2 ws3">15.4<span class="_ _e"> </span>An<span class="_ _7"> </span>Inconsistent<span class="_ _7"> </span>T<span class="_ _d"></span>riad . . . . . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>228</div><div class="t m0 x1 h3 y75 ff1 fs1 fc0 sc0 ls2 ws3">15.5<span class="_ _e"> </span>Coordinating<span class="_ _7"> </span>Planning<span class="_ _3a"> </span>. . . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>231</div><div class="t m0 x1 h3 y76 ff1 fs1 fc0 sc0 ls2 ws3">15.6<span class="_ _e"> </span>Joint<span class="_ _7"> </span>Action<span class="_ _3c"> </span>in<span class="_ _7"> </span>the<span class="_ _7"> </span>First<span class="_ _7"> </span>Y<span class="_ _a"></span>ears<span class="_ _7"> </span>of<span class="_ _7"> </span>Life<span class="_ _20"> </span>. . . . . . . . . . . . .<span class="_ _1a"> </span>235</div><div class="t m0 x1 h3 y77 ff1 fs1 fc0 sc0 ls2 ws3">15.7<span class="_ _e"> </span>Collective<span class="_ _7"> </span>Goals<span class="_ _7"> </span>vs<span class="_ _7"> </span>Shared<span class="_ _7"> </span>Intentions<span class="_ _27"> </span>. . . . . . . . . . . . .<span class="_ _2f"> </span>239</div><div class="t m0 x1 h3 y78 ff1 fs1 fc0 sc0 ls2 ws3">15.8<span class="_ _e"> </span>Expectations<span class="_ _7"> </span>ab<span class="_ _11"></span>out<span class="_ _7"> </span>Collective<span class="_ _7"> </span>Goals<span class="_ _33"> </span>. . . . . . . . . . . . . .<span class="_ _1a"> </span>242</div><div class="t m0 x1 h3 y79 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>5<span class="_ _1"></span>.<span class="_ _1"></span>9 C<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _1"></span>c<span class="_ _1"></span>l<span class="_ _1"></span>u<span class="_ _1"></span>s<span class="_ _1"></span>i<span class="_ _1"></span>o<span class="_ _1"></span>n<span class="_ _d"></span>............................<span class="_ _17"> </span>2<span class="_ _1"></span>4<span class="_ _1"></span>5<span class="_ _1"></span></div><div class="t m0 x0 h2 y7a ff2 fs1 fc0 sc0 ls2 ws5">16<span class="_ _33"> </span>Conclusion to Part II<span class="_ _3e"> </span>249</div><div class="t m0 x1 h3 y7b ff1 fs1 fc0 sc0 ls2 ws3">16.1<span class="_ _e"> </span>Dual<span class="_ _7"> </span>Process<span class="_ _7"> </span>Theories<span class="_ _1b"> </span>. . . . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>250</div><div class="t m0 x1 h3 y7c ff1 fs1 fc0 sc0 ls2 ws3">16.2<span class="_ _e"> </span>Pluralism<span class="_ _7"> </span>about<span class="_ _7"> </span>Mo<span class="_ _11"></span>dels<span class="_ _33"> </span>. . . . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>251</div><div class="t m0 x1 h3 y7d ff1 fs1 fc0 sc0 ls2 ws3">16.3<span class="_ _e"> </span>Goal<span class="_ _7"> </span>T<span class="_ _d"></span>racking<span class="_ _7"> </span>Is<span class="_ _7"> </span>the<span class="_ _7"> </span>Foundation . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>252</div><div class="t m0 x1 h3 y7e ff1 fs1 fc0 sc0 ls2 ws3">16.4<span class="_ _e"> </span>When<span class="_ _7"> </span>Joint<span class="_ _7"> </span>Action<span class="_ _3c"> </span>Enables<span class="_ _7"> </span>Goal<span class="_ _7"> </span>Tracking<span class="_ _2f"> </span>. . . . . . . . . .<span class="_ _2f"> </span>253</div><div class="t m0 x1 h3 y7f ff1 fs1 fc0 sc0 ls2 ws5">16.5<span class="_ _e"> </span>Joint Action and the De<span class="_ _d"></span>velopmental Emergence of Knowledge 255</div><div class="t m0 x0 h4 y80 ff2 fs2 fc0 sc0 ls2 ws10">Conclusion 259</div><div class="t m0 x0 h2 y81 ff2 fs1 fc0 sc0 ls2 wsd">17 Conclusion<span class="_ _3f"> </span>259</div><div class="t m0 x1 h3 y82 ff1 fs1 fc0 sc0 ls2 ws3">17.1<span class="_ _e"> </span>Infants<span class="_ _7"> </span>Rely<span class="_ _7"> </span>on<span class="_ _7"> </span>Minimal<span class="_ _7"> </span>Models<span class="_ _7"> </span>…<span class="_ _34"> </span>. . . . . . . . . . . . . . .<span class="_ _2f"> </span>260</div><div class="t m0 x1 h3 y83 ff1 fs1 fc0 sc0 ls2 ws3">17.2<span class="_ _e"> </span>…<span class="_ _7"> </span>As<span class="_ _3c"> </span>Do<span class="_ _7"> </span>Adults,<span class="_ _7"> </span>Sometimes<span class="_ _35"> </span>. . . . . . . . . . . . . . . . . .<span class="_ _35"> </span>261</div><div class="t m0 x1 h3 y84 ff1 fs1 fc0 sc0 ls0 wse">1<span class="_ _1"></span>7<span class="_ _1"></span>.<span class="_ _1"></span>3 P<span class="_ _1"></span>u<span class="_ _1"></span>z<span class="_ _1"></span>z<span class="_ _1"></span>l<span class="_ _1"></span>e<span class="_ _1"></span>s<span class="_ _3"></span>M<span class="_ _1"></span>a<span class="_ _1"></span>t<span class="_ _1"></span>t<span class="_ _1"></span>e<span class="_ _1"></span>r..........................<span class="_ _26"> </span>2<span class="_ _1"></span>6<span class="_ _1"></span>2<span class="_ _1"></span></div><div class="t m0 x1 h3 y85 ff1 fs1 fc0 sc0 ls2 ws3">17.4<span class="_ _e"> </span>Linking<span class="_ _7"> </span>Problems<span class="_ _3c"> </span>Ab<span class="_ _11"></span>ound<span class="_ _16"> </span>. . . . . . . . . . . . . . . . . . .<span class="_ _1a"> </span>263</div><div class="t m0 x1 h3 y86 ff1 fs1 fc0 sc0 ls2 ws3">17.5<span class="_ _e"> </span>Core<span class="_ _3c"> </span>Knowledge<span class="_ _7"> </span>Isn’t<span class="_ _7"> </span>What<span class="_ _7"> </span>Y<span class="_ _a"></span>ou<span class="_ _7"> </span>Think<span class="_ _7"> </span>It<span class="_ _7"> </span>Is<span class="_ _17"> </span>. . . . . . . . . .<span class="_ _35"> </span>264</div><div class="t m0 x1 h3 y87 ff1 fs1 fc0 sc0 ls2 ws3">17.6<span class="_ _e"> </span>How<span class="_ _3c"> </span>to<span class="_ _7"> </span>Solve<span class="_ _7"> </span>Linking<span class="_ _7"> </span>Problems<span class="_ _3a"> </span>. . . . . . . . . . . . . . . .<span class="_ _1a"> </span>265</div><div class="t m0 x1 h3 y42 ff1 fs1 fc0 sc0 ls2 ws3">17.7<span class="_ _e"> </span>Representation:<span class="_ _40"> </span>Handle<span class="_ _7"> </span>with<span class="_ _7"> </span>Care<span class="_ _15"> </span>. . . . . . . . . . . . . . .<span class="_ _1a"> </span>266</div><div class="t m0 x5 h5 y1c ff1 fs3 fc0 sc0 ls2">viii</div><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:743.720000px;width:115.048000px;height:10.630000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:726.292000px;width:213.551000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:711.483000px;width:218.298000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:696.674000px;width:221.047000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:681.865000px;width:165.767000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:669.710000px;width:222.673000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:654.901000px;width:177.387000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:637.438000px;width:188.338000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:622.629000px;width:231.879000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:607.820000px;width:213.719000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:593.011000px;width:145.012000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:578.202000px;width:205.015000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:563.393000px;width:243.806000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:536.901000px;width:85.690000px;height:12.220000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:521.578000px;width:291.236000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:509.423000px;width:111.155000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:494.614000px;width:171.744000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:479.805000px;width:135.950000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:462.342000px;width:141.282000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:448.268000px;width:208.064000px;height:12.376000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:435.378000px;width:211.698000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:417.915000px;width:205.171000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:405.760000px;width:84.747000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:378.831000px;width:133.104000px;height:10.517000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:363.944000px;width:138.246000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:349.135000px;width:144.977000px;height:10.457000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:331.672000px;width:187.477000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:316.863000px;width:234.473000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:302.054000px;width:332.828000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:262.823000px;width:76.133000px;height:12.221000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:235.716000px;width:81.709000px;height:10.630000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:218.289000px;width:198.093000px;height:13.111000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:204.681000px;width:165.504000px;height:11.910000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:191.325000px;width:101.436000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:173.862000px;width:157.888000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:159.053000px;width:240.845000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:144.244000px;width:184.213000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:129.435000px;width:195.770000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
          <div id="pf5" class="pf w0 h0" data-page-no="5"><div class="pc pc5 w0 h0"><div class="t m0 x1 h3 y1d ff1 fs1 fc0 sc0 ls2 ws3">17.8<span class="_ _e"> </span>Inferential<span class="_ _3c"> </span>and<span class="_ _7"> </span>Intentional<span class="_ _7"> </span>Isolation<span class="_ _27"> </span>. . . . . . . . . . . . . .<span class="_ _35"> </span>267</div><div class="t m0 x1 h3 y88 ff1 fs1 fc0 sc0 ls2 ws3">17.9<span class="_ _e"> </span>Rediscovery<span class="_ _7"> </span>Is<span class="_ _7"> </span>Joint<span class="_ _7"> </span>Action<span class="_ _2c"> </span>. . . . . . . . . . . . . . . . . . .<span class="_ _2f"> </span>268</div><div class="t m0 x0 h2 y89 ff2 fs1 fc0 sc0 ls2 ws11">Glossary 271</div><div class="t m0 x3 h5 y1c ff1 fs3 fc0 sc0 ls2">ix</div><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:743.756000px;width:203.497000px;height:10.456000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" '><div class="d m1" style="border-style:none;position:absolute;left:130.322000px;bottom:726.656000px;width:162.276000px;height:13.110000px;background-color:rgba(255,255,255,0.000001);"></div></span ><span class="l" ><div class="d m1" style="border-style:none;position:absolute;left:112.390000px;bottom:700.320000px;width:49.442000px;height:13.183000px;background-color:rgba(255,255,255,0.000001);"></div></span ></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
        </div>
      </div>
    </div>
  </body>
</html>
</file>

<file path="src/content/writing/intention_motor.md">
---
title: Intention and Motor Representation in Purposive Action
authors: Stephen A. Butterfill and Corrado Sinigaglia
year: 2014
isForthcoming: false
journal: Philosophy and Phenomenological Research
volume: "88"
number: "1"
pages: 119-145
doi: 10.1111/j.1933-1592.2012.00604.x
pdfUrl: /pdf/intention_motor.pdf
---

## Abstract

Are there distinct roles for intention and motor representation in explaining the purposiveness of action? Standard accounts of action assign a role to intention but are silent on motor representation. The temptation is to suppose that nothing need be said here because motor representation is either only an enabling condition for purposive action or else merely a variety of intention. This paper provides reasons for resisting that temptation. Some motor representations, like intentions, coordinate actions in virtue of representing outcomes; but, unlike intentions, motor representations cannot feature as premises or conclusions in practical reasoning. This implies that motor representation has a distinctive role in explaining the purposiveness of action. It also gives rise to a problem: were the roles of intention and motor representation entirely independent, this would impair effective action. It is therefore necessary to explain how intentions interlock with motor representations. The solution, we argue, is to recognise that the contents of intentions can be partially determined by the contents of motor representations. Understanding this content-determining relation enables better understanding how intentions relate to actions.
</file>

<file path="src/content/writing/interacting_mindreaders.md">
---
title: Interacting Mindreaders
authors: Stephen A. Butterfill
year: 2012
isForthcoming: false
journal: Philosophical Studies
volume: "165"
number: "3"
pages: 841-863
doi: 10.1007/s11098-012-9980-x
pdfUrl: /pdf/interacting_mindreaders.pdf
---

## Abstract

Could interacting mindreaders be in a position to know things which they would be unable to know if they were manifestly passive observers? This paper argues that they could. Mindreading is sometimes reciprocal: the mindreader's target reciprocates by taking the mindreader as a target for mindreading. The paper explains how such reciprocity can significantly narrow the range of possible interpretations of behaviour where mindreaders are, or appear to be, in a position to interact. A consequence is that revisions and extensions are needed to standard theories of the evidential basis of mindreading. The view also has consequences for understanding how abilities to interact combined with comparatively simple forms of mindreading may explain the emergence, in evolution or development, of sophisticated forms of social cognition.
</file>

<file path="src/content/writing/joint_action_development.md">
---
title: Joint Action and Development
authors: Stephen A. Butterfill
year: 2011
isForthcoming: false
journal: Philosophical Quarterly
volume: "62"
number: "246"
pages: 23-47
doi: 10.1111/j.1467-9213.2011.00005.x
pdfUrl: /pdf/joint_action_development.pdf
---

## Abstract

Given the premise that joint action plays some role in explaining how humans come to understand minds, what could joint action be? Not what a leading account, Michael Bratman&#39;s, says it is. For on that account engaging in joint action involves sharing intentions and sharing intentions requires much of the understanding of minds whose development is supposed to be explained by appeal to joint action. This paper therefore offers an account of a different kind of joint action, an account compatible with the premise about development. The new account is no replacement for the leading account; rather the accounts characterise two kinds of joint action. Where the kind of joint characterised by the leading account involves shared intentions, the new account characterises a kind of joint action involving shared goals.

<h3 class="likesectionHead"><a id="x1-1000" name="x1-1000"></a>Contents</h3>

      <div class="tableofcontents">
        <span class="sectionToc">1 <a href="#x1-20001" id="QQ2-1-2" name="QQ2-1-2">The
        Question</a></span><br />
        <span class="sectionToc">2 <a href="#x1-30002" id="QQ2-1-3" name="QQ2-1-3">What
        is joint action? The leading account</a></span><br />
        <span class="sectionToc">3 <a href="#x1-40003" id="QQ2-1-4" name="QQ2-1-4">Why
        shared intentional activity could not significantly foster an understanding of
        minds</a></span><br />
        <span class="sectionToc">4 <a href="#x1-50004" id="QQ2-1-5" name=
        "QQ2-1-5">Joint action, shared intention and coordinated
        planning</a></span><br />
        <span class="sectionToc">5 <a href="#x1-60005" id="QQ2-1-6" name=
        "QQ2-1-6">Plural activities</a></span><br />
        <span class="sectionToc">6 <a href="#x1-70006" id="QQ2-1-7" name="QQ2-1-7">What
        is the function of shared goals?</a></span><br />
        <span class="sectionToc">7 <a href="#x1-80007" id="QQ2-1-8" name=
        "QQ2-1-8">Which states could realise shared goals?</a></span><br />
        <span class="sectionToc">8 <a href="#x1-90008" id="QQ2-1-9" name=
        "QQ2-1-9">Shared goals characterise one form of joint action</a></span><br />
        <span class="sectionToc">9 <a href="#x1-100009" id="QQ2-1-10" name=
        "QQ2-1-10">Conclusion</a></span>
      </div><!--l. 49-->

      <p class="indent">&nbsp; <a id="x1-1001r1" name="x1-1001r1"></a></p>

      <h3 class="sectionHead"><span class="titlemark">1.</span> <a id="x1-20001" name=
      "x1-20001"></a>The Question</h3><!--l. 54-->

      <p class="noindent">On the assumption that joint action plays some role in
      explaining how humans develop an understanding of minds, what could joint action
      be? This question needs a little background. It is quite widely agreed that human
      adults reflections on thoughts and actions, their own and others, involve a range
      of commonsense psychological concepts including belief, desire, intention,
      knowledge and perception. Childrens abilities to deploy these concepts improve in
      fluency and sophistication over more than three years (e.g.&nbsp;<a href=
      "#Xen_610">Bartsch &amp; Wellman</a>&nbsp;<a href="#Xen_610">1995</a>). Several
      psychologists have claimed that children first engage in joint action from around
      their first birthday and that engaging in joint action facilitates these early
      improvements (<a href="#Xen_1198">Moll &amp; Tomasello</a>&nbsp;<a href=
      "#Xen_1198">2007</a>;&nbsp;<a href="#Xen_1421">Tomasello &amp;
      Carpenter</a>&nbsp;<a href="#Xen_1421">2007</a>;&nbsp;<a href=
      "#Xen_1090">Tomasello et&nbsp;al.</a>&nbsp;<a href=
      "#Xen_1090">2005</a>;&nbsp;<a href="#Xen_557">Tomasello &amp;
      Rakoczy</a>&nbsp;<a href="#Xen_557">2003</a>). Joint actions that young children
      engage in include tidying up the toys together (<a href="#Xen_1204">Behne
      et&nbsp;al.</a>&nbsp;<a href="#Xen_1204">2005</a>), cooperatively pulling handles
      in sequence to make a dog-puppet sing (<a href="#Xen_1679">Brownell
      et&nbsp;al.</a>&nbsp;<a href="#Xen_1679">2006</a>), bouncing a ball on a large
      trampoline together (<a href="#Xen_1421">Tomasello &amp;
      Carpenter</a>&nbsp;<a href="#Xen_1421">2007</a>) and pretending to row a boat
      together. The psychologists claim that engaging in joint actions like these plays
      some role in the early development of abilities to use concepts like belief,
      desire, intention, knowledge and perception, and in the development of higher
      forms of cognition more generally. My question is what joint action could be
      given that some version of this claim is true.</p><!--l. 56-->

      <p class="indent">The question arises because a leading account of joint action,
      Michael Bratman's, is incompatible with the premise. To anticipate what is
      explained in detail below, on the leading account engaging in joint action
      requires sharing intentions, and sharing intentions requires abilities to engage
      in reasoning about propositional attitudesreasoning of just the sort whose
      development was supposed to be explained by engaging in joint action. So if the
      leading account were the whole truth about joint action, engaging in joint action
      would presuppose, and therefore could not explain, much of the development of
      reasoning about others mental states. Given that the premise is true, the leading
      account cannot be the whole truth about joint action. We need a further account
      of joint action, one that is compatible with the premise that joint action plays
      a role in explaining how humans develop abilities to think about minds and
      actions. Sections 4 to 8 provide such an account. Before that, Section 2 outlines the
      leading account of joint action and Section 3 explains why this account is
      incompatible with the premise about development. <a id="x1-2001r2" name=
      "x1-2001r2"></a></p>

      <h3 class="sectionHead"><span class="titlemark">2.</span> <a id="x1-30002" name=
      "x1-30002"></a>What is joint action? The leading account</h3><!--l. 61-->

      <p class="noindent">Philosophers paradigm cases of joint action include painting
      the house together (Michael Bratman), lifting a heavy sofa together (David
      Velleman), preparing a hollandaise sauce together (John Searle), going to Chicago
      together (Christopher Kutz), and walking together (Margaret Gilbert). One aim of
      an account of joint action is to identify features of some or all of these cases
      in virtue of which they count as joint actions. In this paper I focus on Michael
      Bratman's account because, despite the clarity of its presentation, no decisive
      objection to the parts of his account outlined below has yet been published, and
      also because this account has been most influential in psychology.<span class=
      "footnote-mark"><a href="#fn1x0" id="fn1x0-bk" name="fn1x0-bk"><sup class=
      "textsuperscript">1</sup></a></span><a id="x1-3001f1" name="x1-3001f1"></a></p>
      <!--l. 71-->

      <p class="indent">Bratman characterises a kind of joint action he calls shared
      intentional activity, which is activity explainable by shared
      intention.<span class="footnote-mark"><a href="#fn2x0" id="fn2x0-bk" name=
      "fn2x0-bk"><sup class="textsuperscript">2</sup></a></span><a id="x1-3002f2" name=
      "x1-3002f2"></a> This immediately leads to the question of what shared intentions
      are. Bratman's answer has two parts, a specification of the functional role shared
      intentions play and a substantial account of what shared intentions could be. On
      the first part, Bratman stipulates that the functional role of shared intentions
      is to:</p>

      <div class="quote">
        <!--l. 74-->

        <p class="noindent">(i) coordinate activities; (ii) coordinate planning; and
        (iii) provide a framework to structure bargaining (<a href=
        "#Xen_1356">Bratman</a>&nbsp;<a href="#Xen_1356">1993</a>, p.&nbsp;99).</p>
      </div><!--l. 77-->

      <p class="noindent">To illustrate: if we share an intention that we cook dinner,
      this shared intention will (iii) structure bargaining insofar as we may need to
      decide what to cook or how to cook it on the assumption that we are cooking it
      together; the shared intention will also require us to (ii) coordinate our
      planning by each bringing complementary ingredients and tools, and to (i)
      coordinate our activities by preparing the ingredients in the right order.</p>
      <!--l. 79-->

      <p class="indent">Given this claim about what shared intentions are for, Bratman
      argues that the following three conditions are collectively
      sufficient<span class="footnote-mark"><a href="#fn3x0" id="fn3x0-bk" name=
      "fn3x0-bk"><sup class="textsuperscript">3</sup></a></span><a id="x1-3003f3" name=
      "x1-3003f3"></a> for you and I to have a shared intention that we J. This is his
      substantial account of what shared intentions could be:</p>

      <div class="quote">
        <!--l. 83-->

        <p class="noindent">1. (a) I intend that we J and (b) you intend that we J</p>
        <!--l. 85-->

        <p class="noindent">2. I intend that we J in accordance with and because of la,
        lb, and meshing subplans of la and lb; you intend that we J in accordance with
        and because of la, lb, and meshing subplans of la and lb</p><!--l. 87-->

        <p class="noindent">3. 1 and 2 are common knowledge between us (<a href=
        "#Xen_1356">Bratman</a>&nbsp;<a href="#Xen_1356">1993</a>, p.&nbsp;View 4)</p>
      </div><!--l. 90-->

      <p class="noindent">In arguing that these are collectively sufficient conditions
      for shared intention, Bratman combines two strategies. He argues that these
      conditions collectively suffice to rule out certain cases where, intuitively,
      there is no shared intention (such as the case where we each intend to paint the
      house, I yellow and you blue). And he argues that the attitudes specified in
      these conditions are collectively capable of playing the three roles shared
      intentions are supposed to play. <a id="x1-3004r3" name="x1-3004r3"></a></p>

      <h3 class="sectionHead"><span class="titlemark">3.</span> <a id="x1-40003" name=
      "x1-40003"></a>Why shared intentional activity could not significantly foster an
      understanding of minds</h3><!--l. 96-->

      <p class="noindent">Suppose that joint action plays a role in explaining the
      early development of childrens abilities to think about minds. Is Michael
      Bratman's shared intentional activity a notion of joint action which could play
      this role? Several psychologists have suggested that it is. Thus Moll and
      Tomasello explicate their hypothesis that the unique aspects of human cognition
      were driven by, or even constituted by social cooperation (<a href=
      "#Xen_1198">2007</a>, p.&nbsp;3) by appeal to a modified version of Bratman's
      (1992) definition of shared cooperative activities [on which] the participants in
      the cooperative activity share a joint goal (<a href="#Xen_1198">2007</a>,
      p.&nbsp;3); in this context share a joint goal means possess a shared intention.
      Similarly, Carpenter, in a discussion of joint action in infancy, writes:</p>

      <div class="quote">
        <!--l. 99-->

        <p class="noindent">I will adopt Bratman's (1992) influential formulation of
        joint action [F]or an activity to be considered shared or joint each partner
        needs to intend to perform the joint action together in accordance with and
        because of meshing subplans (p. 338) and this needs to be common knowledge
        between the participants. (<a href="#Xen_1682">Carpenter</a>&nbsp;<a href=
        "#Xen_1682">2009</a>, p.&nbsp;281)</p>
      </div><!--l. 102-->

      <p class="noindent">Others who appeal to Bratman's notion of shared intentional
      activity in characterising childrens first joint actions and their role in
      development include <a href="#Xen_1090">Tomasello et&nbsp;al.</a>&nbsp;(<a href=
      "#Xen_1090">2005</a>, p.&nbsp;680) and <a href=
      "#Xen_1365">Gr&acirc;&euro;&deg;fenhain et&nbsp;al.</a>&nbsp;(<a href=
      "#Xen_1365">2009</a>, p.&nbsp;1430).</p><!--l. 104-->

      <p class="indent">Recall that shared intentional activity requires shared
      intentions. On Bratman's substantial account, sharing intentions requires having
      intentions about intentions and even intentions about subplans of intentions (see
      Condition 2 in the quote on the preceding page). Bratman emphasises this feature
      of the account:</p>

      <div class="quote">
        <!--l. 107-->

        <p class="noindent">each agent does not just intend that the group perform the
        [] joint action. Rather, each agent intends as well that the group perform this
        joint action in accordance with subplans (of the intentions in favor of the
        joint action) that mesh (<a href="#Xen_1197">Bratman</a>&nbsp;<a href=
        "#Xen_1197">1992</a>, p.&nbsp;332).</p>
      </div><!--l. 110-->

      <p class="noindent">A natural thought at this point is that joint action might
      require only plans which <span class="cmti-12">in fact</span> mesh rather than
      <span class="cmti-12">intentions about</span> the meshing of plans. Bratman
      considers this option and explains why this weakening of his account is not
      coherent (<a href="#Xen_1197">Bratman</a>&nbsp;<a href="#Xen_1197">1992</a>,
      pp.&nbsp;331-3), so I shall not pursue this thought.</p><!--l. 112-->

      <p class="indent">The fact that shared intentions require intentions about
      intentions suggests a potential objection to the view that shared intentional
      activity explains early developments in childrens abilities to think about minds.
      For it seems unlikely that 2- and 3-year-olds, who according to many findings are
      years away from being able to ascribe any propositional attitudes at
      all,<span class="footnote-mark"><a href="#fn4x0" id="fn4x0-bk" name=
      "fn4x0-bk"><sup class="textsuperscript">4</sup></a></span><a id="x1-4001f4" name=
      "x1-4001f4"></a> can form intentions about others intentions.<span class=
      "footnote-mark"><a href="#fn5x0" id="fn5x0-bk" name="fn5x0-bk"><sup class=
      "textsuperscript">5</sup></a></span><a id="x1-4002f5" name="x1-4002f5"></a> This
      would mean they cannot meet the sufficient conditions Bratman lays out for
      sharing intentions.</p><!--l. 123-->

      <p class="indent">This potential objection is weak because it depends on
      controversial empirical claims about the absolute time in development at which
      abilities to ascribe, and to form intentions about, intentions might emerge. A
      more promising objection avoids this dependence. The ability to form intentions
      about intentions involves a sophisticated kind of propositional attitude
      ascription (as explained below). This ability is required for sharing intentions
      in accordance with Bratman's substantial account. So meeting the sufficient
      conditions for joint action given by this account could not significantly
      <span class="cmti-12">explain</span> the development of an understanding of minds
      because it already <span class="cmti-12">presupposes</span> too much
      sophistication in the use of psychological concepts.<span class=
      "footnote-mark"><a href="#fn6x0" id="fn6x0-bk" name="fn6x0-bk"><sup class=
      "textsuperscript">6</sup></a></span><a id="x1-4003f6" name="x1-4003f6"></a></p>
      <!--l. 125-->

      <p class="indent">Note that, as it stands, this objection does not establish
      much. It concerns conditions imposed by the substantial account of shared
      intention which are sufficient but not necessary conditions.<span class=
      "footnote-mark"><a href="#fn7x0" id="fn7x0-bk" name="fn7x0-bk"><sup class=
      "textsuperscript">7</sup></a></span><a id="x1-4004f7" name="x1-4004f7"></a> The
      substantial account is supposed to characterise oneperhaps one among manyways in
      which the functional role of shared intentions can be realised. So the objection
      serves only to raise a question. Are there in fact alternative sufficient
      conditions for shared intention, conditions that can be met without already
      having abilities to use psychological concepts whose development was supposed to
      be explained by joint action?</p><!--l. 131-->

      <p class="indent">The answer to this question is not entirely straightforward. We
      must begin with the functional roles of shared intention, for these provide
      necessary conditions. One of the roles of shared intentions is to coordinate
      planning. What does coordinating planning involve? Intuitively the idea is that
      just as individual intentions serve to coordinate an individuals planning over
      time, so shared intentions coordinate planning between agents. (I use the terms
      individual intention and individual goal to refer to intentions and goals
      explanatory of individual actions; an individual action is an action performed by
      just one agent such as that described by the sentence Ayesha repaired the
      puncture all by herself.) A second role for shared intentions is to structure
      bargaining concerning plans. To understand these roles it is essential to
      understand what planning means in this context. The term planning is sometimes
      used quite broadly to encompass processes involved in low-level control over the
      execution of sequences of movements, as is often required for manipulating
      objects manually (e.g.&nbsp;<a href="#Xen_1535">Haggard</a>&nbsp;<a href=
      "#Xen_1535">1998</a>), as well as processes controlling the movements of a limb
      on a single trajectory (e.g.&nbsp;<a href="#Xen_1681">Bizzu</a>&nbsp;<a href=
      "#Xen_1681">2001</a>). In Bratman's account and this paper, the term planning is
      used in a narrower sense. Planning in this narrow sense exists to coordinate an
      agents various activities over relatively long intervals of time; it involves
      practical reasoning and forming intentions which may themselves require further
      planning, generating a hierachy of plans and subplans. Paradigm cases include
      planning a birthday party or planning to move house.</p><!--l. 133-->

      <p class="indent">Given the functional roles of shared intention, when (if ever)
      must the states which realise shared intentions include intentions about others
      intentions? Coordinating plans with others does not seem always or in principle
      to require specific intentions about others intentions. It is plausible that in
      everyday life some of our plans are coordinated largely thanks to a background of
      shared preferences, habits and conventions. Consider, for example, people who
      often meet in a set place at a fixed time of day to discuss research over lunch.
      These people can coordinate their lunch plans merely by setting a date and
      following established routine; providing nothing unexpected happens, they seem
      not to need intentions about each others intentions. Within limits, then,
      coordinating plans may not always require intentions about intentions. The same
      may hold for structuring bargaining. But when the background of shared
      preferences, habits and conventions is not sufficient to ensure that our plans
      will be coordinated, it is necessary to monitor or manipulate others plans. And
      since intentions are the basic elements of plans (in the special sense of plan in
      terms of which Bratman defined shared intention), this means monitoring or
      manipulating others intentions. The background which makes for effortlessly
      coordinated planning is absent when our aims are sufficiently novel, when the
      circumstances sufficiently unusual (as in many emergencies), and when our
      co-actors are sufficiently unfamiliar. In all of these cases, coordinating plans
      and structuring bargaining will involve monitoring or manipulating others
      intentions. Now this does not necessarily involve forming intentions about their
      intentions because, in principle, monitoring and manipulating others intentions
      could (within limits) be achieved by representing states which serve as proxies
      for intentions rather than by representing intentions as such, much as one can
      (within limits) monitor and manipulate others visual perceptions by representing
      their lines of sight. But possession of general abilities to monitor and
      manipulate others intentions does require being able to form intentions about
      others intentions.</p><!--l. 135-->

      <p class="indent">The question was whether there are sufficient conditions for
      shared intention which do not presuppose abilities to use psychological concepts
      whose development is supposed to be explained by joint action. As promised, the
      answer is not straightforward. In a limited range of cases, coordinating plans
      and perhaps structuring bargaining does not appear to require insights into other
      minds. But in other cases, particularly cases involving novel aims or agents
      unfamiliar with each other, intentions about others intentions are generally
      required.</p><!--l. 137-->

      <p class="indent">The main question for this section was whether Bratman's account
      captures a notion of joint action suitable for explaining the early development
      of childrens abilities to think about minds. Some of the joint actions which
      young children engage in involve novel aims, and some involve unfamiliar
      partners. So if these joint actions did involve coordinating planning and
      structuring bargaining, they could not rest on a shared background but would
      require abilities to form intentions about others intentions. It follows that
      joint action would presuppose much of the sophistication in the use of
      psychological concepts whose development it was supposed to explain. So given the
      premise that joint action plays a role in explaining early developments in
      understanding minds, it cannot be the case that the joint actions children engage
      in as soon as they engage in any joint actions involve shared intentions as
      characterised by Bratman.</p><!--l. 139-->

      <p class="indent">This conclusion rests on the assumption that having intentions
      about intentions involves some of the psychological sophistication whose
      development is supposed to be explained by appeal to joint action. One might
      object that the ability for form intentions about intentions is somehow less
      sophisticated than abilities to form other kinds of representation of other kinds
      of mental states. To answer the objection it is sufficient to clarify what
      intention means in this context. For the term intention, like planning, is used
      to mean different things by different researchers. Sometimes intention and goal
      are used interchangeably in describing behaviour which is somewhat flexibly
      organised around some outcome (e.g.&nbsp;<a href=
      "#Xen_1433">Premack</a>&nbsp;<a href="#Xen_1433">1990</a>, p.&nbsp;14). But in
      this context we need a different notion of intention, one on which intentions are
      elements of plans. Such intentions play a role in coordinating an agents
      activities over time. Their role is characterised in part by normative
      constraints expressed in terms of the propositional contents of intentions. For
      instance, one norm characteristic of the role of intentions in plans requires an
      agent to avoid ways of realising one intention that will make it impossible for
      her to realise other intentions she has (all things being equal). This norm
      requires someone who intends both to visit an aunt and to buy some shoes in a
      single evening to limit time spent on each activity to allow for the other.
      Another norm characteristic of intention concerns the compatibility of having
      multiple intentions simultaneously: it is not rational to have multiple
      intentions unless it is rational to have a single intention agglomerating them
      all (<a href="#Xen_1543">Bratman</a>&nbsp;<a href="#Xen_1543">1987</a>). Norms
      such as these, together with the role of intentions in practical reasoning, are
      what characterise the role of intentions in planning. Given that intentions are
      characterised in this way as elements in plans, it seems necessary that
      understanding intentions will involve some grasp both of the role that intentions
      as propositional attitudes play in practical reasoning and also of the norms
      relating intentions to planning. Of course this does not mean that individuals
      who understand intentions as elements in plans can articulate or list the
      relevant roles or norms. But it does mean that they should sometimes be sensitive
      to some of the requirements these norms impose and also that they would be able
      to recognise some of the norms as correct in optimal conditions. This is why
      requiring intentions about intentions presupposes significant psychological
      sophistication.</p><!--l. 141-->

      <p class="indent">Given that joint action facilitates the development of mental
      understanding, and that (as just argued) Bratman's notion of shared intentional
      activity is not a kind of joint action which could play this role, does it follow
      that Bratman's account is incorrect?<span class="footnote-mark"><a href="#fn8x0"
      id="fn8x0-bk" name="fn8x0-bk"><sup class=
      "textsuperscript">8</sup></a></span><a id="x1-4005f8" name="x1-4005f8"></a>
      Drawing this conclusion would require the further assumption that there is just
      one kind of joint action. This is not obviously true. Compare individual action.
      It is sometimes accepted that there is a distinction between intentional action
      and other kinds of action such as response behaviours and merely purposive
      activities (<a href="#Xen_496">Dickinson &amp; Balleine</a>&nbsp;<a href=
      "#Xen_496">2000</a>;&nbsp;<a href="#Xen_194">Frankfurt</a>&nbsp;<a href=
      "#Xen_194">1971</a>). Because there may be an analogous distinction between kinds
      of joint action, the developmental considerations do not directly bear on the
      correctness of Bratman's account. Perhaps what we need is not a modified version
      of Bratman's account but an account of a different kind of joint action.</p>
      <!--l. 143-->

      <p class="indent">To sum up, Bratman's account does not characterise a kind of
      joint action which could play a role in explaining how children come to
      understand minds. In the next section I consider whether this undermines the
      claim that the interactions highlighted by developmental psychologists are really
      joint actions before offering a new account of joint action in the following
      sections. <a id="x1-4006r4" name="x1-4006r4"></a></p>

      <h3 class="sectionHead"><span class="titlemark">4.</span> <a id="x1-50004" name=
      "x1-50004"></a>Joint action, shared intention and coordinated planning</h3>
      <!--l. 149-->

      <p class="noindent">The argument of the previous section establishes that not all
      of the following claims are true:</p>

      <div class="quote">
        <!--l. 152-->

        <p class="noindent">(1) joint action fosters an understanding of minds;</p>
        <!--l. 154-->

        <p class="noindent">(2) all joint action involves shared intention; and</p>
        <!--l. 156-->

        <p class="noindent">(3) a function of shared intention is to coordinate two or
        more agents plans.</p>
      </div><!--l. 159-->

      <p class="noindent">These claims are inconsistent because if the second and third
      were both true, abilities to engage in joint action would presuppose, and so
      could not significantly foster, an understanding of minds. For all that has been
      said so far, any of these claims could be rejected. In what follows I
      characterise a form of joint action which involves what I call shared goals and
      no shared intentions. The aim is to show by construction that there are forms of
      joint action which require minimal cognitive sophistication. In doing this I
      shall provide grounds for rejecting the claim that joint action always involves
      shared intention, (2), strengthening the case for accepting the premise about
      development, (1), while remaining neutral on whether shared intentions function
      to coordinate planning, (3).</p><!--l. 161-->

      <p class="indent">Some researchers assert that all joint actions involve shared
      intentions. For instance, Tomasello writes that [t]he sine qua non of
      collaborative action is a joint goal and a joint commitment (<a href=
      "#Xen_1828">2008</a>, p.&nbsp;181). Here joint goal refers to shared intention in
      Bratman's sense and collaborative action includes joint actions early in
      development. Similarly, Gilbert writes I take collective action to involve a
      collective intention (<a href="#Xen_1287">2006</a>, p.&nbsp;5). Perhaps, then, a
      better strategy than the one I propose would be to reject the first claim, (1),
      above and conjecture that joint action cannot significantly foster an
      understanding of minds (although some lesser form of interaction may do so). But
      it is striking that none of the researchers who assert that all joint action
      involves shared intention provide an argument, and narrowly semantic
      considerations provide no support for this assertion (<a href=
      "#Xen_2394">Ludwig</a>&nbsp;<a href="#Xen_2394">2007</a>;&nbsp;<a href=
      "#Xen_2395">Smith</a>&nbsp;<a href="#Xen_2395">2011</a>, p.&nbsp;367). In fact
      other researchers have assumed without argument that not all joint actions
      involve shared intention (<a href="#Xen_1197">Bratman</a>&nbsp;<a href=
      "#Xen_1197">1992</a>, p.&nbsp;330; <a href="#Xen_1860">Schmidt
      et&nbsp;al.</a>&nbsp;<a href="#Xen_1860">2010</a>, p.&nbsp;2010; <a href=
      "#Xen_1778">Vesper et&nbsp;al.</a>&nbsp;<a href="#Xen_1778">2010</a>). On what
      grounds could we accept this assumption or its negation?</p><!--l. 165-->

      <p class="indent">Let us step back. What features other than shared intention
      indicate that the actions of two or more agents constitute a joint action as
      opposed to any other kind of interaction? Here are several indicators of joint
      action:</p>

      <div class="quote">
        <!--l. 168-->

        <p class="noindent">i. this case seems to fit with paradigm examples of joint
        action such as walking, cooking or playing the piano together (<a href=
        "#Xen_1861">Gilbert</a>&nbsp;<a href="#Xen_1861">1990</a>;&nbsp;<a href=
        "#Xen_2392">Goebl &amp; Palmer</a>&nbsp;<a href=
        "#Xen_2392">2009</a>;&nbsp;<a href="#Xen_25">Velleman</a>&nbsp;<a href=
        "#Xen_25">2000</a>);</p><!--l. 170-->

        <p class="noindent">ii. the candidate joint action differs from a case in which
        the agents perform the same type of activity (such as walking or cooking) in
        parallel rather than together (<a href="#Xen_1768">Bratman</a>&nbsp;<a href=
        "#Xen_1768">2009</a>;&nbsp;<a href="#Xen_1861">Gilbert</a>&nbsp;<a href=
        "#Xen_1861">1990</a>;&nbsp;<a href="#Xen_1365">Gr&acirc;&euro;&deg;fenhain
        et&nbsp;al.</a>&nbsp;<a href="#Xen_1365">2009</a>, p.&nbsp;150);</p>
        <!--l. 172-->

        <p class="noindent">iii. for each agent, acting together rather than
        individually is voluntary in this sense: in so far as they control which means
        they adopt in pursuing a goal, such as whether to move an object by lifting it
        or by dragging it, they can also control whether their actions are individual
        or joint;</p><!--l. 174-->

        <p class="noindent">iv. there is a sense in which all of the agents actions
        taken together are directed to a single goal, and this is not just a matter of
        each agents action being individually directed to that goal;</p><!--l. 176-->

        <p class="noindent">v. there is a description of the interaction with a plural
        subject and an action verb, such as they are bouncing the ball on the
        trampoline (<a href="#Xen_1290">Kutz</a>&nbsp;<a href="#Xen_1290">2000</a>
        emphasises this indicator);</p><!--l. 178-->

        <p class="noindent">vi. each agent is disposed to modify her actions in
        accordance with what is needed to achieve the goal given how the other agents
        actions are unfolding (<a href="#Xen_1197">Bratman</a>&nbsp;<a href=
        "#Xen_1197">1992</a>, p.&nbsp;328).</p>
      </div><!--l. 181-->

      <p class="noindent">I am not suggesting that any of these features is necessary
      for joint action, nor that they are collectively sufficient. My claim is just
      these features are relevant to deciding whether an interaction is a joint action,
      and that where an interaction has many or all of these features we have
      (defeasible) grounds to infer that it is a joint action. In short, I propose
      that, in the absence of a deeper analysis, we should take these features as a
      rough and provisional explication of one theoretically significant way of using
      the term joint action.<span class="footnote-mark"><a href="#fn9x0" id="fn9x0-bk"
      name="fn9x0-bk"><sup class="textsuperscript">9</sup></a></span><a id="x1-5001f9"
      name="x1-5001f9"></a></p><!--l. 183-->

      <p class="indent">So what about the objection that all joint action involves
      shared intention? The following will provide (defeasible) grounds for rejecting
      it. For I shall show that some interactions have the features listed above,
      (i)-(vi), but do not involve shared intention. <a id="x1-5002r5" name=
      "x1-5002r5"></a></p>

      <h3 class="sectionHead"><span class="titlemark">5.</span> <a id="x1-60005" name=
      "x1-60005"></a>Plural activities</h3><!--l. 189-->

      <p class="noindent">Our question is what joint action could be on the assumption
      that it fosters an understanding of minds. We have seen (in Section 3) that the
      answer cannot involve appeal to shared intention. Joint actions involving shared
      intention presuppose, and so cannot significantly foster the development of,
      sophisticated uses of psychological concepts. What we need, then, is to identify
      a form of joint action that requires as little psychological sophistication as
      possible; by presupposing less we make it possible to explain more.</p>
      <!--l. 191-->

      <p class="indent">I shall approach this task indirectly by first considering
      forms of action involving multiple agents more basic than any kind of joint
      action. Some ants harvest plant hair and fungus in order to build traps to
      capture large insects; once captured, many worker ants sting the large insects,
      transport them and carve them up (<a href="#Xen_1292">Dejean
      et&nbsp;al.</a>&nbsp;<a href="#Xen_1292">2005</a>). The ants behaviours have an
      interesting feature distinct from their being coordinated: each ants behaviours
      are individually organised around an outcomethe flys deathwhich occurs as a
      common effect of many ants behaviours. We can say that there is a single
      activitykilling a flywhich several ants performed. In general, a plural activity
      is one involving two or more agents. As I shall use the term plural activity, for
      agents to be engaged in a plural activity it is sufficient that each agents
      activities are individually organised around a single outcome which occurs as a
      common effect of all the agents activities.</p><!--l. 193-->

      <p class="indent">Note that nothing controversial is assumed in stating these
      sufficient conditions for plural activity. The first ingredient is the notion
      that an individuals behaviours can be organised around an outcome. This is
      shorthand for an open-ended disjunction of cases; it means that there is an
      intention, habit, biological function or other behaviour-organizing circumstance
      connecting the individuals behaviours to the outcome. The second ingredient is
      the notion of a common effect, which is not specific to action. The flys capture
      is a common effect of the ants individual behaviours in just the sense that the
      flys death is a common effect of the multiple doses of poison it received: none
      of the doses was individually deadly, each had its murderous effect only in
      concert with some of the others. As characterised here, the notion of a plural
      activity depends on nothing more controversial than the cogency of these two
      ingredients.</p><!--l. 195-->

      <p class="indent">My use of the term plural activity is different from other
      natural uses of this term and may be too broad to pick out an intuitive category.
      If Helen and Ayesha individually aim to smooth a section of pavement by shuffling
      their feet when they walk over it and if it does become smooth partly as a
      consequence of both their efforts, then they are engaged in the plural activity
      of smoothing the pavement. This is true even in the absence of any intention to
      act together. It is true even if their lives do not overlap at all, so that there
      may be no intuitive sense in which their smoothing the pavement is a joint
      action. Allowing this case to count as a plural activity does no harm for present
      purposes and makes it possible to characterise a useful notion of plural activity
      in uncontroversial terms.</p><!--l. 197-->

      <p class="indent">Another potentially unnatural feature of this definition of
      plural activity is that it requires success. By definition, where two or more
      agents actions constitute a plural activity the outcome to which they are
      directed must occur. This simplifies the definition in ways that will shortly be
      useful.</p><!--l. 199-->

      <p class="indent">Humans sometimes perform activities that are plural in the
      minimal sense that some ants behaviours are. In a particularly sulky mood Thomas
      pulls on one end of a large boat in order to move it; he does not realise that
      Illaria is pushing the other end and that without her contribution the boat would
      not move. He succeeds in moving the boat, as does Illaria. So there is a single
      activitymoving the boatwhich they both perform. Even though Thomas action is
      goal-directed (its goal is to move the boat to the sea), his activity is plural
      only in the minimal sense that the ants fly-trapping behaviour is: it is
      organised around an outcome which occurs as a common effect. The plural nature of
      such activities need not show up in intentions, desires or beliefs. That an
      activity is performed by two individuals does not require that they intend,
      believe or desire this to be so.</p><!--l. 201-->

      <p class="indent">What most philosophers mean by joint action is not, or not
      only, this minimal notion of plural activity. Certainly this minimal notion is
      inadequate for our present purpose, which is to understand what joint action
      could be on the assumption that it plays some role in explaining how children
      come to understand minds. Nevertheless, the notion of plural activity is a useful
      starting point for understanding a kind of joint action relevant to explaining
      development. <a id="x1-6001r6" name="x1-6001r6"></a></p>

      <h3 class="sectionHead"><span class="titlemark">6.</span> <a id="x1-70006" name=
      "x1-70006"></a>What is the function of shared goals?</h3><!--l. 206-->

      <p class="noindent">In giving an account of one kind of joint action I shall
      first identify something I call shared goals. Before starting it will be helpful
      to fix terminology. As I use the term goal it refers to an outcome, actual or
      possible, and not to a state. I make no direct use of the notion that agents can
      have goals (as in Sams goal was to topple the president) and focus on relations
      between goals and actions (as in the goal of Marvins action was to upset Ayesha).
      An action is <span class="cmti-12">goal-directed</span> where it makes sense to
      ask which of its possible and actual outcomes are goals to which the action was
      directed. One paradigm case of goal-directed action involves intention: where an
      agent acts on an intention, the intentions content specifies a goal to which her
      action is directed. In addition, an action can arguably be directed to a goal
      which is not specified in the content of any of the agents intentions (<a href=
      "#Xen_1359">Bratman</a>&nbsp;<a href="#Xen_1359">1984</a>). There may also be
      forms of action which are goal-directed but do not involve intention at
      all.<span class="footnote-mark"><a href="#fn10x0" id="fn10x0-bk" name=
      "fn10x0-bk"><sup class="textsuperscript">10</sup></a></span><a id="x1-7001f10"
      name="x1-7001f10"></a> Certainly there are ways of representing actions as
      goal-directed which do not involve representing intentions or any other
      propositional attitudes of agents.</p><!--l. 208-->

      <p class="indent">The term shared is used loosely. Just as, on most accounts,
      shared intentions are neither literally intentions nor literally shared (no
      single intention is mine and yours), so shared goals are not goals (they are
      complexes of states and relations) and do not by definition alone involve
      anything which is literally shared. I have used the otherwise infelicitous label
      shared goal in order to highlight the basic intuition behind the positive account
      of joint action I shall offer: some cases of joint action involve structures
      which bind not the agents intentions but the goals to which their activities are
      directed.<span class="footnote-mark"><a href="#fn11x0" id="fn11x0-bk" name=
      "fn11x0-bk"><sup class="textsuperscript">11</sup></a></span><a id="x1-7002f11"
      name="x1-7002f11"></a></p><!--l. 210-->

      <p class="indent">Having fixed terminology we can now turn to the primary issue,
      which is to identify shared goals. Following the model provided by Bratman's
      account of shared intentions, my account of shared goals has two parts: a
      specification of their function role and a substantial description of states that
      could realise them. This section is about the functional role of shared goals,
      the following section concerns their realisation. To emphasise, questions about
      the mental states involved in sharing goals will be deferred until the next
      section; in this section the question is only what shared goals are for.</p>
      <!--l. 212-->

      <p class="indent">Successful plural activity generally requires coordination. How
      is this coordination achieved? In the case of ants such coordination may be
      achieved hormonally. In humans, who can voluntarily engage in plural activities
      with novel outcomes, coordination can usually only be achieved psychologically.
      This is what shared goals are for. Shared goals coordinate multiple agents
      goal-directed activities around an outcome to be achieved as a common effect of
      their efforts. That is, their function is to coordinate plural activities.</p>
      <!--l. 214-->

      <p class="indent">An illustration may help to clarify what performing this
      function amounts to. There is a fallen tree lying across the road. Several people
      each want it moved, but none of them can move it by themselves and none of them
      can control the others actions. The trees movement can only be secured as common
      effect of several peoples actions. In this situation, there is a need for several
      people to coordinate their goal-directed activities. They need to lift in
      complementary directions and at suitably related times; and if lifting doesnt
      work, they need to change strategy and try pushing or something else that might
      achieve the outcome. The role of shared goals is to coordinate these
      goal-directed activities.</p><!--l. 216-->

      <p class="indent">In the above illustration, plural activity is necessary to
      achieve an outcome. Shared goals also play a role in situations where plural
      activity occurs even though it is not necessary. For instance, Amin and Bertram
      each individually aim to put a large barrel into a boat. Either of them could
      move the barrel into the boat alone or their doing this could be a plural
      activity; the choice is theirs. The sequence of activities Amin would need to
      perform to put the barrel in the boat differs depending on whether he is acting
      alone or with Bertram. Acting alone, Amin would position himself so that the
      barrel and boat are in front of him, throw his arms around the middle of barrel,
      raise it, tilt back and then push up and forwards. If he chose to act with
      Bertram, Amin would need to take an entirely different approach. It is this need
      that shared goals answer.</p><!--l. 218-->

      <p class="indent">The notion that a function of shared goals is to coordinate
      goal-directed activities needs qualifying because all action involves
      coordination at several levels. Any goal-directed action, individual or joint,
      will be realised by a collection of simple object-directed actions such as
      pushing, pulling and tearing; and these in turn will be realized by some kind of
      motor actions, and so on until at some point we reach continuous bodily
      movements. Plainly most tasks require coordination at several of these levels;
      for instance, passing an object from one hand to another requires precise timing
      of releases and grasps as well as appropriate positioning in space. At some
      levels, coordination is largely independent of which goals agents actions are
      directed to (for example, it can be hard to use ones hands in an uncoordinated
      way even when doing so would be advantageous). This is true even for coordination
      of multiple agents activities. We may coordinate with others without being aware
      of how we are coordinating or even that we are coordinating (<a href=
      "#Xen_1693">Richardson et&nbsp;al.</a>&nbsp;<a href="#Xen_1693">2009</a>). In
      fact there seem to be several forms of <span class="cmti-12">emergent
      coordination</span>, that is, coordination which is independent of the goals of
      an agents actions (<a href="#Xen_1812">Knoblich et&nbsp;al.</a>&nbsp;<a href=
      "#Xen_1812">2010</a>). Clearly, then, shared goals are not the only factor in
      coordinating plural activity. The role of shared goals is limited to coordinating
      goal-directed actions and not their non-purposive components, and it may be that
      shared goals can play this role only thanks to the existence of other mechanisms
      of emergent coordination.</p><!--l. 220-->

      <p class="indent">Shared goals resemble shared intentions insofar as both exist
      to coordinate activities. They differ in that structuring bargaining and
      coordinating planning are not functions of shared goals. On some views, the
      distinction I have drawn between shared intentions and shared goals parallels a
      distinction between individual intentions and more primitive states connecting
      individuals activities with the goals to which they are directed. For individual
      intentions are sometimes held to be intrinsically elements in agents plans and
      therefore absent from the lives of any agents incapable of planning (<a href=
      "#Xen_1694">Bratman</a>&nbsp;<a href="#Xen_1694">2007</a>). Such agents (if there
      are any) may need to act when faced with equally desirable alternatives and to
      coordinate their activities around goals despite fluctuations in desire. This
      need might be met by states which resemble intentions in that they exist in part
      to coordinate a single agents activities and in that they connect the agents
      activities to a goal, but differ from intentions in lacking planning functions.
      Given this distinction, shared intentions would stand to shared goals roughly as
      individual intentions stand to their more primitive counterparts.</p>
      <!--l. 222-->

      <p class="indent">The limited function of shared goals makes them better suited
      than shared intentions for characterising the cases studied in developmental
      research. Many, perhaps all, of these cases would not normally require
      coordinated planning. As mentioned above, these cases include tidying up the toys
      together and cooperatively pulling handles in sequence in order to make a puppet
      sing. As coordinated planning is not needed in such cases, nor are shared
      intentions. What is needed, though, is for the agents goal-directed actions to be
      coordinated. This is what shared goals are for. <a id="x1-7003r7" name=
      "x1-7003r7"></a></p>

      <h3 class="sectionHead"><span class="titlemark">7.</span> <a id="x1-80007" name=
      "x1-80007"></a>Which states could realise shared goals?</h3><!--l. 227-->

      <p class="noindent">In the previous section I identified shared goals in terms of
      their function, which is to coordinate plural activities. The next step is to
      characterise states capable of realising this function.</p><!--l. 229-->

      <p class="indent">To start with an illustration, suppose that a goal of Amins
      actions in the near future will be move a large barrel into a boat. Amin
      anticipates that some of Bertrams future actions will have the same goal, and
      Amin expects the barrels moving into the boat to occur as a common effect of his
      own goal-directed actions and Bertrams. For his part, moving the barrel into the
      boat will also be a goal of Bertrams actions and Bertram has expectations
      mirroring Amins. In favourable circumstances their goal-directed contributions to
      a plural activity of moving the barrel into the boat could be coordinated in
      virtue of this pattern of goal-relations and expectations. Accordingly, the
      existence of such goal-relations and expectations are sufficient for Amin and
      Bertram to share the goal of getting the barrel into the boat.</p><!--l. 231-->

      <p class="indent">Here are the key features of this case expressed in general
      terms:</p>

      <div class="quote">
        <!--l. 234-->

        <p class="noindent">(a) <span class="cmti-12">one goal, two or more
        agents</span></p><!--l. 236-->

        <p class="noindent">there is a single goal, G, to which each agents actions
        are, or will be, individually directed;</p><!--l. 238-->

        <p class="noindent">(b) <span class="cmti-12">identification</span></p>
        <!--l. 240-->

        <p class="noindent">each agent can identify each of the other agents in a way
        that doesnt depend on knowledge of the goal or actions directed to it;</p>
        <!--l. 242-->

        <p class="noindent">(c) <span class="cmti-12">expectations about goal-directed
        actions</span></p><!--l. 245-->

        <p class="noindent">on balance<span class="footnote-mark"><a href="#fn12x0" id=
        "fn12x0-bk" name="fn12x0-bk"><sup class=
        "textsuperscript">12</sup></a></span><a id="x1-8001f12" name="x1-8001f12"></a>
        each agent expects each of the other agents she can identify to perform an
        action directed to the goal; and</p><!--l. 247-->

        <p class="noindent">(d) <span class="cmti-12">expectations about a common
        effect</span></p><!--l. 250-->

        <p class="noindent">on balance each agent expects this goal to occur as a
        common effect of all of their actions directed to the goal, her own and the
        others.</p>
      </div><!--l. 253-->

      <p class="noindent">In favourable circumstances and in concert with emergent
      coordination, these goal-relations and expectations could serve to coordinate the
      goal-directed plural activities of two or more agents (I shall say which
      circumstances are favourable below). Since shared goal was defined in terms of
      this coordinating function, (a)-(d) are collectively sufficient for possessing a
      shared goal.</p><!--l. 255-->

      <p class="indent">Let us consider these four features in turn. The first feature,
      (a), is required just because we are concerned with plural activities; by
      definition, actions comprising a plural activity are directed to a single goal
      such as moving a particular barrel onto a certain boat (see Section 4). The
      second feature, (b), was not explicit in the description of Amin and Bertrams
      barrel moving. It excludes the following sort of case. Mia and Sobani are in a
      crowded space. Each intends to move a table and, thanks to her background
      knowledge, expects that exactly one other agent intends the same. But neither Mia
      nor Sobani can identify who else she expects to be involved in moving the table,
      except trivially as the other table-mover. In this case, the pattern of
      goal-relations and expectations in (a), (c) and (d) could have at most a limited
      effect on Mia and Sobanis ability to coordinate their efforts. So the
      identification requirement, (b), is included because the coordinating effect of
      (a), (c) and (d) seem to depend on it. Identification may not feature whenever
      agents have a shared goal; (a)-(d) collectively provide only sufficient
      conditions.</p><!--l. 257-->

      <p class="indent">The third and fourth features, (c) and (d), involve
      expectations. Knowledge states and beliefs both count as expectations but it is
      not necessary to have either. In developmental research, looking times and eye
      movements are regularly used as measures of infants expectations concerning goals
      (for example&nbsp;<a href="#Xen_1434">Csibra et&nbsp;al.</a>&nbsp;<a href=
      "#Xen_1434">2003</a>;&nbsp;<a href="#Xen_1436">Gergely &amp;
      Csibra</a>&nbsp;<a href="#Xen_1436">2003</a>;&nbsp;<a href="#Xen_1207">Gergely
      et&nbsp;al.</a>&nbsp;<a href="#Xen_1207">1995</a>;&nbsp;<a href="#Xen_1437">Luo
      &amp; Baillargeon</a>&nbsp;<a href="#Xen_1437">2005</a>;&nbsp;<a href=
      "#Xen_1439">Woodward &amp; Sommerville</a>&nbsp;<a href="#Xen_1439">2000</a>). It
      is an open question whether these sorts of expectations are beliefs. Where such
      expectations do not merely control looking times and eye movements but also
      inform a range of goal-directed actions in ways that are rational given their
      contents, then expectations of this type are sufficient for sharing goals whether
      or not they amount to beliefs. This marks one contrast between requirements on
      sharing goals and sharing intentions. Given that intentions function to
      coordinate planning in Bratman's intellectual sense of planning and given some
      plausible norms governing the rationality of planning (<a href=
      "#Xen_1774">Hawthorne</a>&nbsp;<a href="#Xen_1774">2004</a>, pp.&nbsp;29-31),
      sharing an intention will require knowledge of others intentions and their
      relations to ones own. By contrast, sharing a goal does not require knowledge.
      because several kinds of expectation which fall short of knowledge are sufficient
      for coordinating plural activities.</p><!--l. 259-->

      <p class="indent">The third feature, (c), concerns expectations about other
      agents goal-directed actions. This is a minimal counterpart of the requirement
      that agents who share an intention represent each others intentions. Possessing a
      shared goal requires representing only goal-directed actions. It is possible to
      represent an action as goal-directed without representing (or even being able to
      represent) intentions or any other propositional attitudes. For instance, an
      agent might represent goals as functions of actions.<span class=
      "footnote-mark"><a href="#fn13x0" id="fn13x0-bk" name="fn13x0-bk"><sup class=
      "textsuperscript">13</sup></a></span><a id="x1-8002f13" name="x1-8002f13"></a>
      There are strong theoretical and empirical grounds to hold that representing
      goal-directed actions requires less conceptual sophistication, and may be less
      cognitively demanding, than representing intentions as such.</p><!--l. 265-->

      <p class="indent">The fourth feature, (d), concerns the agents expectations that
      the goal to which their actions are directed will occur as a common effect of
      their efforts. This and the third feature are jointly equivalent to requiring
      that the each agent expects that she and the other agents are engaged in a plural
      activity with goal G. (The agents may not actually be engaged in a single plural
      activity because, as noted earlier, plural activities are by definition
      successful whereas it is possible to possesses a shared goal without succeeding.)
      The claim that features (a)-(d) are sufficient for agents to possess a shared
      goal is the claim that this combination of features could function to coordinate
      plural activities. In essence, the claim is this: an expectation, on the part of
      each agent concerned, that she is or will be involved in a plural activity with
      the others, will, in favourable circumstances and in concert with emergent
      coordination, normally enable them to coordinate their actions.</p><!--l. 267-->

      <p class="indent">Shared intention is sometimes thought to involve common
      knowledge in such a way that agents who share an intention can know that they
      share an intention.<span class="footnote-mark"><a href="#fn14x0" id="fn14x0-bk"
      name="fn14x0-bk"><sup class="textsuperscript">14</sup></a></span><a id=
      "x1-8003f14" name="x1-8003f14"></a> By contrast, it is possible to have a shared
      goal without knowing that one does. Agents can have, and act on (see Section 8
      below), a shared goal without understanding their actions as comprising anything
      more than a plural activity.</p><!--l. 273-->

      <p class="indent">The above pattern of goal-relations and expectations, (a)-(d),
      can play its coordinating role only in favourable circumstances. What makes
      circumstances <span class="cmti-12">un</span>favourable? One factor is a lack of
      freedom. To illustrate, suppose that Hendrik and Arch are instructed to tidy the
      toys away. Arch would not normally obey this instruction but Hendrik convincingly
      threatens reprisals unless Arch tidies all the toys away. For the goal of tidying
      the toys away, (a)-(d) above could all obtain in this case (Hendrik fulfils (a)
      by the act of threatening). But any coordinating effect this pattern of
      goal-relations and expectations might have had is trumped by Hendriks control
      over Archs actions. Archs lack of freedom is an unfavourable circumstance, that
      is, one in which the coordinating role of shared goals may be blocked. Another
      unfavourable circumstance is antagonism to plural activity. Suppose that Ella and
      Cohen have been tasked with wiping a table clean. Ella is desperate to clean the
      table without Cohen, and Cohen is desperate to clean it without Ella. Neither
      thinks this will be possible, and they satisfy (a)-(d) above. But because they
      are desperate to act alone, each tries to sabotage the others efforts. Any
      coordinating effect the shared goal might have had is overridden by the agents
      antagonism to plural activity. In short, then, favourable circumstances are those
      in which factors that would defeat the coordinating tendency of the pattern of
      goal-relations and expectations in (a)-(d) are absent; paradigm defeating factors
      are a lack of freedom and antagonism towards plural activity. <a id="x1-8004r8"
      name="x1-8004r8"></a></p>

      <h3 class="sectionHead"><span class="titlemark">8.</span> <a id="x1-90008" name=
      "x1-90008"></a>Shared goals characterise one form of joint action</h3>
      <!--l. 279-->

      <p class="noindent">So far I have stipulated that the function of shared goals is
      to coordinate plural activities and argued that this function could be realised
      by a certain pattern of goal-relations and expectations. Finally I shall use this
      to characterise a form of joint action which, lacking the cognitive and
      conceptual demands associated with shared intentional action, could be used to
      explicate the premise that engaging in joint action fosters an understanding of
      minds.</p><!--l. 281-->

      <p class="indent">Shared goals are characteristic of a form of joint action but
      the relation between possession of a shared goal and performing a joint action is
      not straightforward. Compare ordinary individual action. Acting intentionally is
      not just a matter of acting and simultaneously intending; nor is it even just a
      matter of being caused to act by an intention (<a href=
      "#Xen_1211">Searle</a>&nbsp;<a href="#Xen_1211">1983</a>, pp.&nbsp;136-7).
      Relatedly, we should not suppose that the mere presence, or even the mere
      efficacy, of a shared goal is sufficient for joint action. Take any collection of
      actions directed to a single goal, G, involving two or more agents. Let us say
      that these actions, taken collectively, are <span class="cmti-12">driven by a
      shared goal</span> when G is a shared goal of the agents, when, in performing
      actions directed to this goal, they are acting on the associated expectations and
      any other attitudes in ways that are rational, and when their so acting functions
      to coordinate their actions in a way that would normally facilitate the goals
      occurring as a common effect of all their efforts. (I assume that, although the
      expectations mentioned in Section 7 may not be necessary for possessing a shared
      goal, some such expectations are always involved.) I claim that actions driven by
      shared goals are joint actions.</p><!--l. 283-->

      <p class="indent">Why accept this? Recall the six indicators of joint action
      identified earlier (Section 4). Where these three conditions hold of an
      interaction, most of these indicators will be present. The interaction will be
      distinct from a case in which the agents pursue the goal in parallel and without
      a shared goal (this was the second indicator). The action will be voluntary with
      respect to its jointness insofar as jointness is partly due to agents acting on
      their expectations in ways that are rational, as contrasted with interactions
      where coordination involves only involuntary forms of emergent coordination
      (third indicator). If the goal occurs, this will normally be in part because of
      the coordination provided by the shared goal; in this sense, the coordination
      serves to direct the agents actions, taken together, to the goal and this amounts
      to more than each agents actions being individually directed to the goal (fourth
      indicator). Finally, the agents dispositions to adapt their actions to each
      others is built into the requirement that the shared goal function to coordinate
      their actions by means of their acting on the associated expectations (sixth
      indicator). In short, actions driven by shared goals have many features
      indicative of joint action. This is reason to hold that they are in fact joint
      actions.</p><!--l. 285-->

      <p class="indent">Not every case in which actions are driven by shared goals fits
      intuitively with paradigm examples of joint action. Consider two drivers on a
      collision course in a narrow street. Suppose (perhaps unrealistically) that each
      acts with the goal of avoiding a collision between their cars, expects the other
      to do the same and expects that they will avoid collision thanks to their
      combined efforts. This is sufficient for avoiding a collision to be a shared
      goal. (Note that specifying the goal requires care: the drivers actions would
      have different goals if, for instance, the only goal of each drivers action were
      to avoid hitting the other.) Suppose also that their actions are driven by a
      shared goal in the sense defined above. So, on the above account, their avoiding
      collision is a joint action. (Not all cases of avoiding a collision are joint
      actions, only those, if any, which are driven by shared goals.) But intuitively
      this case may not seem to fit with paradigms of joint action because the
      interaction is so minor. If this counts as joint action, then, given the right
      goal-relations and expectations, so could passing someone in a corridor. Should
      we modify the account of joint action in order to exclude this sort of case?
      There is an obstacle to doing that. We could elaborate a series of interactions
      driven by the shared goal of avoiding collision where each interaction is
      slightly less minor than its predecessor in the series. Whether or not intuitions
      support drawing a boundary, it seems that no such boundary is theoretically
      significant for understanding the role of joint action in development. Since our
      aim is to characterise joint action as it fosters development, we should risk
      deviating from intuition to avoid otherwise unnecessary complexity.</p>
      <!--l. 287-->

      <p class="indent">There is another sort of case in which actions driven by a
      shared goal may not intuitively fit with paradigm joint actions. Consider again
      the two drivers whose goal is to avoid collision. Now suppose, in addition, that
      the first driver hawkishly accelerates while covertly preparing to brake if
      necessary, causing the second driver to brake hard. Given the present account,
      their actions nevertheless constitute joint action. This may not fit intuitively
      with paradigm joint actions because the first driver dominates the second (and
      does so by means of deception).<span class="footnote-mark"><a href="#fn15x0" id=
      "fn15x0-bk" name="fn15x0-bk"><sup class=
      "textsuperscript">15</sup></a></span><a id="x1-9001f15" name="x1-9001f15"></a>
      Again it is possible to elaborate a series of cases involving gradually varying
      degrees of domination. While outright coercion is incompatible with joint action
      on the account I have offered (see Section 7 above), neither domination nor other
      failures to be cooperative are excluded. This may conflict with intuitions about
      joint action but reduces the complexity of the account. That the account is
      nevertheless an account of joint action is shown by the presence of the other
      indicators of joint action mentioned above.</p><!--l. 289-->

      <p class="indent">Accounts of joint action sometimes invoke special kinds of
      mental state (<a href="#Xen_1426">Gilbert</a>&nbsp;<a href=
      "#Xen_1426">1992</a>;&nbsp;<a href="#Xen_1369">Searle</a>&nbsp;<a href=
      "#Xen_1369">2002</a>), special kinds of reasoning (<a href="#Xen_1383">Gold &amp;
      Sugden</a>&nbsp;<a href="#Xen_1383">2007a</a>), special kinds of interdependence
      (<a href="#Xen_1356">Bratman</a>&nbsp;<a href="#Xen_1356">1993</a>;&nbsp;<a href=
      "#Xen_1827">Miller</a>&nbsp;<a href="#Xen_1827">2001</a>) or, apparently, special
      kinds of agent (<a href="#Xen_2391">Helm</a>&nbsp;<a href="#Xen_2391">2008</a>).
      The present account, if successful, shows that there is a simple kind of joint
      action characterising which requires no such special ingredients. The simple kind
      of joint action involves only ordinary individual goal directed-actions being
      coordinated in part by expectations about others goal-directed actions and their
      common effects.</p><!--l. 291-->

      <p class="indent">There are, of course, questions and puzzles about joint action
      which the simple account offered here does not address and which may call for
      greater complexity. These include issues about commitment (<a href=
      "#Xen_1287">Gilbert</a>&nbsp;<a href="#Xen_1287">2006</a>), the coordination of
      decisions to act jointly (<a href="#Xen_1297">Velleman</a>&nbsp;<a href=
      "#Xen_1297">1997</a>), the kind of reasoning needed for coordinating choices
      (<a href="#Xen_1373">Sugden</a>&nbsp;<a href="#Xen_1373">2000</a>), and the
      possibilities of intending others actions (<a href=
      "#Xen_1369">Searle</a>&nbsp;<a href="#Xen_1369">2002</a>,&nbsp;<a href=
      "#Xen_1692">1994</a>) and of acting on others intentions (<a href=
      "#Xen_1427">Roth</a>&nbsp;<a href="#Xen_1427">2004</a>). Failure to address these
      questions or tackle these puzzles might be an objection if the simple account
      were meant to be the whole story about joint action. In fact the simple account
      is not supposed to apply to every case of joint action. For instance, painting a
      house together will probably involve intentionally coordinating plans and so
      require sharing intentions rather than merely sharing goals. On the other hand,
      in some cases joint action does not require planning, commitment, coordinating
      decisions to act or special kinds of reasoning. Knoblich and Sebanz offer an
      example:</p>

      <div class="quote">
        <!--l. 294-->

        <p class="noindent">the way people lift a two-handled basket depends on whether
        they lift it alone or together. When alone, a person would normally grasp each
        handle with one hand. When together, one person would normally grasp the left
        handle with his/her right hand and the other person would grasp the right
        handle with his/her left hand. (<a href="#Xen_1429">Knoblich &amp;
        Sebanz</a>&nbsp;<a href="#Xen_1429">2008</a>, p.&nbsp;2026)</p>
      </div><!--l. 297-->

      <p class="noindent">Because handles provide an obvious way for two people to lift
      the basket (these authors even postulate a joint affordance) and people are often
      skilled at coordinated lifting, planning is typically unnecessary and it is
      plausible that shared goals are sufficient for joint actions of this
      sort.<span class="footnote-mark"><a href="#fn16x0" id="fn16x0-bk" name=
      "fn16x0-bk"><sup class="textsuperscript">16</sup></a></span><a id="x1-9002f16"
      name="x1-9002f16"></a> <a id="x1-9003r9" name="x1-9003r9"></a></p>

      <h3 class="sectionHead"><span class="titlemark">9.</span> <a id="x1-100009" name=
      "x1-100009"></a>Conclusion</h3><!--l. 303-->

      <p class="noindent">The question was this. Given the premise that joint action
      plays some role in explaining how children come to understand minds, what could
      joint action be? The negative point was that it couldnt involve sharing
      intentions for reasons connected to the fact that sharing intentions involves
      coordinating planning and so requires sophistication in ascribing propositional
      attitudes. The positive claim was that there is a simple account of joint action
      which is compatible with the developmental premise. On the simple account, joint
      action involves sharing goals and sharing goals requires only an understanding of
      goal-directed actions and their common effects.</p><!--l. 305-->

      <p class="indent">This simple account of joint action is not offered as a
      replacement for Bratman's account or any accounts competing with his. Bratman's
      account assumes that joint action involves shared intention where the functions
      of shared intention include coordinating paradigmatically long-term plans. Such
      an account may be required to characterise complex cases where success demands
      that agents plans mesh. But some cases of joint action (such as carrying a
      two-handled basket together) do not involve plans in the relevant sense of
      planning. The agents need to coordinate their activities but not their plans. The
      simple account applies only in such cases. In philosophical accounts of
      individual action, actions explainable by intending are sometimes distinguished
      from other kinds of individual action including response behaviours, arrational
      actions and merely purposive activities (<a href="#Xen_181">Dickinson &amp;
      Balleine</a>&nbsp;<a href="#Xen_181">1993</a>;&nbsp;<a href=
      "#Xen_1683">Hursthouse</a>&nbsp;<a href="#Xen_1683">1991</a>;&nbsp;<a href=
      "#Xen_25">Velleman</a>&nbsp;<a href="#Xen_25">2000</a>). Comparable distinctions
      are certain to be needed for understanding joint action; the differences between
      shared intentions and shared goals mark one such distinction between kinds of
      joint action.</p>

      <h3 class="likesectionHead"><a id="x1-110009" name="x1-110009"></a><span class=
      "cmr-10x-x-109">References</span></h3>

      <div class="thebibliography">
        <p class="bibitem"><span class="biblabel"><a id="Xen_1686" name=
        "Xen_1686"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Apperly,
        I.</span><span class="cmr-10x-x-109">&nbsp;A. &amp; Butterfill, S. (2009). Do
        humans have two systems to track beliefs</span> <span class="cmr-10x-x-109">and
        belief-like states?</span> <span class="cmti-10x-x-109">Psychological
        Review</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">2009</span><span class="cmr-10x-x-109">(116), 4.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_69" name=
        "Xen_69"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Astington, J. (1991). Intention in the child&rsquo;s theory of
        mind. In D.</span><span class="cmr-10x-x-109">&nbsp;Frye &amp;</span>
        <span class="cmr-10x-x-109">C.</span><span class="cmr-10x-x-109">&nbsp;Moore
        (Eds.),</span> <span class="cmti-10x-x-109">Children&rsquo;s Theories of Mind:
        mental states and social understanding</span> <span class=
        "cmr-10x-x-109">(pp.</span><span class="cmr-10x-x-109">&nbsp;157&ndash;172).
        Hove: Erlbaum.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_70" name=
        "Xen_70"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Astington, J. &amp; Gopnik, A. (1991). Developing understanding
        of desire and intention.</span> <span class="cmr-10x-x-109">In
        A.</span><span class="cmr-10x-x-109">&nbsp;Whiten (Ed.),</span> <span class=
        "cmti-10x-x-109">Natural Theories of the Mind: evolution, development
        and</span> <span class="cmti-10x-x-109">simulation of everyday
        mindreading</span> <span class="cmr-10x-x-109">(pp.</span><span class=
        "cmr-10x-x-109">&nbsp;39&ndash;50). Oxford: Blackwell.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1789" name=
        "Xen_1789"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Baillargeon, R., Scott, R.</span><span class=
        "cmr-10x-x-109">&nbsp;M., &amp; He, Z. (2010). False-belief understanding in
        infants.</span> <span class="cmti-10x-x-109">Trends in Cognitive
        Sciences</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">14</span><span class="cmr-10x-x-109">(3),
        110&ndash;118.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_610" name=
        "Xen_610"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bartsch,
        K. &amp; Wellman, H.</span><span class="cmr-10x-x-109">&nbsp;M. (1995).</span>
        <span class="cmti-10x-x-109">Children talk about the mind</span><span class=
        "cmr-10x-x-109">. New York ;</span> <span class="cmr-10x-x-109">Oxford: Oxford
        University Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1204" name=
        "Xen_1204"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Behne,
        T., Carpenter, M., &amp; Tomasello, M. (2005). One-year-olds comprehend
        the</span> <span class="cmr-10x-x-109">communicative intentions behind gestures
        in a hiding game.</span> <span class="cmti-10x-x-109">Developmental
        Science</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">8</span><span class="cmr-10x-x-109">(6),
        492&ndash;499.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1681" name=
        "Xen_1681"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bizzu,
        E. (2001). Motor control. In R.</span><span class="cmr-10x-x-109">&nbsp;A.
        Wilson &amp; F.</span><span class="cmr-10x-x-109">&nbsp;C. Keil (Eds.),</span>
        <span class="cmti-10x-x-109">The MIT</span> <span class=
        "cmti-10x-x-109">Encyclopedia of the Cognitive Sciences</span><span class=
        "cmr-10x-x-109">. Cambridge, Mass: MIT Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1359" name=
        "Xen_1359"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bratman,
        M. (1984). Two faces of intention.</span> <span class="cmti-10x-x-109">The
        Philosophical Review</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">93</span><span class="cmr-10x-x-109">(3),</span> <span class=
        "cmr-10x-x-109">375&ndash;405.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1543" name=
        "Xen_1543"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bratman,
        M. (1987).</span> <span class="cmti-10x-x-109">Intentions, Plans, and Practical
        Reasoning</span><span class="cmr-10x-x-109">. Cambridge MA:</span> <span class=
        "cmr-10x-x-109">Harvard University Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1197" name=
        "Xen_1197"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bratman,
        M. (1992). Shared cooperative activity.</span> <span class="cmti-10x-x-109">The
        Philosophical Review</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">101</span><span class="cmr-10x-x-109">(2),</span> <span class=
        "cmr-10x-x-109">327&ndash;341.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1356" name=
        "Xen_1356"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bratman,
        M. (1993). Shared intention.</span> <span class=
        "cmti-10x-x-109">Ethics</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">104</span><span class="cmr-10x-x-109">,
        97&ndash;113.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1357" name=
        "Xen_1357"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bratman,
        M. (1999 [1997]). I intend that we j. In</span> <span class=
        "cmti-10x-x-109">Faces of Intention</span><span class="cmr-10x-x-109">.
        Cambridge:</span> <span class="cmr-10x-x-109">Cambridge University
        Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1694" name=
        "Xen_1694"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bratman,
        M. (2007).</span> <span class="cmti-10x-x-109">Structures of
        Agency</span><span class="cmr-10x-x-109">. Oxford: Oxford University
        Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1768" name=
        "Xen_1768"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Bratman,
        M. (2009). Modest sociality and the distinctiveness of intention.</span>
        <span class="cmti-10x-x-109">Philosophical Studies</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">144</span><span class=
        "cmr-10x-x-109">(1), 149&ndash;165.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1679" name=
        "Xen_1679"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Brownell, C.</span><span class="cmr-10x-x-109">&nbsp;A.,
        Ramani, G.</span><span class="cmr-10x-x-109">&nbsp;B., &amp; Zerwas, S. (2006).
        Becoming a social partner</span> <span class="cmr-10x-x-109">with peers:
        cooperation and social understanding in one- and two-year-olds.</span>
        <span class="cmti-10x-x-109">Child</span> <span class=
        "cmti-10x-x-109">Development</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">77</span><span class="cmr-10x-x-109">(4),
        803&ndash;21.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1682" name=
        "Xen_1682"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Carpenter, M. (2009). Just how joint is joint action in
        infancy?</span> <span class="cmti-10x-x-109">Topics in Cognitive</span>
        <span class="cmti-10x-x-109">Science</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">1</span><span class="cmr-10x-x-109">(2),
        380&ndash;392.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1438" name=
        "Xen_1438"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Csibra,
        G. (2008). Goal attribution to inanimate agents by 6.5-month-old
        infants.</span> <span class="cmti-10x-x-109">Cognition</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">107</span><span class=
        "cmr-10x-x-109">(2), 705&ndash;717.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1434" name=
        "Xen_1434"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Csibra,
        G., B</span><span class="cmr-10x-x-109">&Atilde;&OElig;r</span><span class=
        "cmr-10x-x-109">&Atilde;&rsaquo;, S., Ko</span><span class=
        "cmr-10x-x-109">&Atilde;&rsaquo;s, O., &amp; Gergely, G. (2003). One-year-old
        infants use</span> <span class="cmr-10x-x-109">teleological representations of
        actions productively.</span> <span class="cmti-10x-x-109">Cognitive
        Science</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">27</span><span class="cmr-10x-x-109">(1),
        111&ndash;133.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1292" name=
        "Xen_1292"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Dejean,
        A., Solano, P.</span><span class="cmr-10x-x-109">&nbsp;J., Ayroles, J.,
        Corbara, B., &amp; Orivel, J. (2005). Insect</span> <span class=
        "cmr-10x-x-109">behaviour: Arboreal ants build traps to capture prey.</span>
        <span class="cmti-10x-x-109">Nature</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">434</span><span class="cmr-10x-x-109">,
        973.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_181" name=
        "Xen_181"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Dickinson, A. &amp; Balleine, B. (1993). Actions and responses:
        the dual psychology of</span> <span class="cmr-10x-x-109">behaviour. In
        N.</span><span class="cmr-10x-x-109">&nbsp;Eilan, R.</span><span class=
        "cmr-10x-x-109">&nbsp;McCarthy, &amp; B.</span><span class=
        "cmr-10x-x-109">&nbsp;Brewer (Eds.),</span> <span class=
        "cmti-10x-x-109">Spatial representation:</span> <span class=
        "cmti-10x-x-109">problems in philosophy and psychology</span> <span class=
        "cmr-10x-x-109">(pp.</span><span class="cmr-10x-x-109">&nbsp;277&ndash;293).
        Oxford: Oxford University</span> <span class="cmr-10x-x-109">Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_496" name=
        "Xen_496"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Dickinson, A. &amp; Balleine, B. (2000). Causal cognition and
        goal-directed action. In</span> <span class=
        "cmr-10x-x-109">C.</span><span class="cmr-10x-x-109">&nbsp;Heyes &amp;
        L.</span><span class="cmr-10x-x-109">&nbsp;Huber (Eds.),</span> <span class=
        "cmti-10x-x-109">The Evolution of Cognition</span><span class="cmr-10x-x-109">.
        Cambridge, Mass.: MIT</span> <span class="cmr-10x-x-109">Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_194" name=
        "Xen_194"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Frankfurt, H. (1971). Freedom of the will and the concept of a
        person.</span> <span class="cmti-10x-x-109">The Journal</span> <span class=
        "cmti-10x-x-109">of Philosophy</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">68</span><span class="cmr-10x-x-109">(1),
        5&ndash;20.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1436" name=
        "Xen_1436"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Gergely,
        G. &amp; Csibra, G. (2003). Teleological reasoning in infancy: the nave
        theory</span> <span class="cmr-10x-x-109">of rational action.</span>
        <span class="cmti-10x-x-109">Trends in Cognitive Sciences</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">7</span><span class=
        "cmr-10x-x-109">(7), 287&ndash;292.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1207" name=
        "Xen_1207"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Gergely,
        G., Nadasky, Z., Csibra, G., &amp; Biro, S. (1995). Taking the intentional
        stance</span> <span class="cmr-10x-x-109">at 12 months of age.</span>
        <span class="cmti-10x-x-109">Cognition</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">56</span><span class=
        "cmr-10x-x-109">, 165&ndash;193.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1861" name=
        "Xen_1861"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Gilbert,
        M. (1990). Walking together: A paradigmatic social phenomenon.</span>
        <span class="cmti-10x-x-109">Midwest</span> <span class=
        "cmti-10x-x-109">Studies in Philosophy</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">15</span><span class=
        "cmr-10x-x-109">, 14.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1426" name=
        "Xen_1426"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Gilbert,
        M. (1992).</span> <span class="cmti-10x-x-109">On Social
        Facts</span><span class="cmr-10x-x-109">. Princeton, NJ: Princeton University
        Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1287" name=
        "Xen_1287"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Gilbert,
        M. (2006). Rationality in collective action.</span> <span class=
        "cmti-10x-x-109">Philosophy of the Social Sciences</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">36</span><span class=
        "cmr-10x-x-109">(1), 3&ndash;17.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_2392" name=
        "Xen_2392"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Goebl,
        W. &amp; Palmer, C. (2009). Synchronization of timing and motion among</span>
        <span class="cmr-10x-x-109">performing musicians.</span> <span class=
        "cmti-10x-x-109">Music Perception: An Interdisciplinary
        Journal</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">26</span><span class="cmr-10x-x-109">(5),</span> <span class=
        "cmr-10x-x-109">427&ndash;438.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1383" name=
        "Xen_1383"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Gold, N.
        &amp; Sugden, R. (2007a). Collective intentions and team agency.</span>
        <span class="cmti-10x-x-109">Journal of</span> <span class=
        "cmti-10x-x-109">Philosophy</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">104</span><span class="cmr-10x-x-109">(3),
        109&ndash;137.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_2393" name=
        "Xen_2393"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Gold, N.
        &amp; Sugden, R. (2007b). Theories of team agency. In F.</span><span class=
        "cmr-10x-x-109">&nbsp;Peter &amp; H.</span><span class="cmr-10x-x-109">&nbsp;B.
        Schmid</span> <span class="cmr-10x-x-109">(Eds.),</span> <span class=
        "cmti-10x-x-109">Rationality and Commitment</span> <span class=
        "cmr-10x-x-109">(pp.</span><span class="cmr-10x-x-109">&nbsp;280&ndash;312).
        Oxford: Oxford University Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1365" name=
        "Xen_1365"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Gr</span><span class="cmr-10x-x-109">&acirc;&euro;&deg;fenhain,
        M., Behne, T., Carpenter, M., &amp; Tomasello, M. (2009). Young
        children&rsquo;s</span> <span class="cmr-10x-x-109">understanding of joint
        commitments.</span> <span class="cmti-10x-x-109">Developmental
        Psychology</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">45</span><span class="cmr-10x-x-109">(5),
        1430&ndash;1443.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1535" name=
        "Xen_1535"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Haggard,
        P. (1998). Planning of action sequences.</span> <span class=
        "cmti-10x-x-109">Acta Psychologica</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">99</span><span class="cmr-10x-x-109">(2),
        201&ndash;215.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1774" name=
        "Xen_1774"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Hawthorne, J.</span><span class="cmr-10x-x-109">&nbsp;O.
        (2004).</span> <span class="cmti-10x-x-109">Knowledge and
        Lotteries</span><span class="cmr-10x-x-109">. Oxford: Oxford University
        Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_2391" name=
        "Xen_2391"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Helm,
        B.</span><span class="cmr-10x-x-109">&nbsp;W. (2008). Plural agents.</span>
        <span class="cmti-10x-x-109">Nous</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">42</span><span class="cmr-10x-x-109">(1),
        17&ndash;49.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1300" name=
        "Xen_1300"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Hughes,
        C., Fujisawa, K.</span><span class="cmr-10x-x-109">&nbsp;K., Ensor, R., Lecce,
        S., &amp; Marfleet, R. (2006). Cooperation</span> <span class=
        "cmr-10x-x-109">and conversations about the mind: A study of individual
        differences in 2-year-olds and</span> <span class="cmr-10x-x-109">their
        siblings.</span> <span class="cmti-10x-x-109">British Journal of Developmental
        Psychology</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">24</span><span class="cmr-10x-x-109">(1),
        53&ndash;72.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1683" name=
        "Xen_1683"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Hursthouse, R. (1991). Arational actions.</span> <span class=
        "cmti-10x-x-109">The Journal of Philosophy</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">88</span><span class=
        "cmr-10x-x-109">(2), 57&ndash;68.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1812" name=
        "Xen_1812"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Knoblich, G., Butterfill, S., &amp; Sebanz, N. (2010).
        Psychological research on joint</span> <span class="cmr-10x-x-109">action:
        Theory and data. In B.</span><span class="cmr-10x-x-109">&nbsp;Ross
        (Ed.),</span> <span class="cmti-10x-x-109">Psychology of Learning and
        Motivation</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmr-10x-x-109">volume</span><span class="cmr-10x-x-109">&nbsp;51
        (pp.</span><span class="cmr-10x-x-109">&nbsp;59&ndash;101). Academic
        Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1429" name=
        "Xen_1429"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Knoblich, G. &amp; Sebanz, N. (2008). Evolving intentions for
        social interaction: from</span> <span class="cmr-10x-x-109">entrainment to
        joint action.</span> <span class="cmti-10x-x-109">Philosophical Transactions of
        the Royal Society B</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">363</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmr-10x-x-109">2021&ndash;2031.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1821" name=
        "Xen_1821"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Kov</span><span class="cmr-10x-x-109">&Acirc;&middot;cs,</span>
        <span class="cmr-10x-x-109">&Acirc;&iexcl;.</span><span class=
        "cmr-10x-x-109">&nbsp;M., T</span><span class=
        "cmr-10x-x-109">&Atilde;&circ;gl</span><span class=
        "cmr-10x-x-109">&Acirc;&middot;s, E., &amp; Endress, A.</span><span class=
        "cmr-10x-x-109">&nbsp;D. (2010). The social sense: Susceptibility</span>
        <span class="cmr-10x-x-109">to others&rsquo; beliefs in human infants and
        adults.</span> <span class="cmti-10x-x-109">Science</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">330</span><span class=
        "cmr-10x-x-109">(6012), 1830&ndash;1834.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1290" name=
        "Xen_1290"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Kutz, C.
        (2000). Acting together.</span> <span class="cmti-10x-x-109">Philosophy and
        Phenomenological Research</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">61</span><span class="cmr-10x-x-109">(1),</span>
        <span class="cmr-10x-x-109">1&ndash;31.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_2394" name=
        "Xen_2394"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Ludwig,
        K. (2007). Collective intentional behavior from the standpoint of
        semantics.</span> <span class="cmti-10x-x-109">Nous</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">41</span><span class=
        "cmr-10x-x-109">(3), 355&ndash;393.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1437" name=
        "Xen_1437"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Luo, Y.
        &amp; Baillargeon, R. (2005). Can a self-propelled box have a goal?</span>
        <span class="cmti-10x-x-109">Psychological</span> <span class=
        "cmti-10x-x-109">Science</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">16</span><span class="cmr-10x-x-109">(8),
        601&ndash;608.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1827" name=
        "Xen_1827"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Miller,
        S. (2001).</span> <span class="cmti-10x-x-109">Social Action: A Teleological
        Account</span><span class="cmr-10x-x-109">. Cambridge: Cambridge</span>
        <span class="cmr-10x-x-109">University Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1198" name=
        "Xen_1198"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Moll, H.
        &amp; Tomasello, M. (2007). Cooperation and human cognition: the
        vygotskian</span> <span class="cmr-10x-x-109">intelligence hypothesis.</span>
        <span class="cmti-10x-x-109">Philosophical Transactions of the Royal Society
        B</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">362</span><span class="cmr-10x-x-109">(1480),</span>
        <span class="cmr-10x-x-109">639&ndash;648.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1092" name=
        "Xen_1092"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Onishi,
        K.</span><span class="cmr-10x-x-109">&nbsp;H. &amp; Baillargeon, R. (2005). Do
        15-month-old infants understand false</span> <span class=
        "cmr-10x-x-109">beliefs?</span> <span class=
        "cmti-10x-x-109">Science</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">308</span><span class="cmr-10x-x-109">(8),
        255&ndash;258.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1433" name=
        "Xen_1433"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Premack,
        D. (1990). The infant&rsquo;s theory of self-propelled objects.</span>
        <span class="cmti-10x-x-109">Cognition</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">36</span><span class=
        "cmr-10x-x-109">(1),</span> <span class="cmr-10x-x-109">1&ndash;16.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_139" name=
        "Xen_139"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Price,
        C. (2001).</span> <span class="cmti-10x-x-109">Functions in
        Mind</span><span class="cmr-10x-x-109">. Oxford: Clarendon Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1693" name=
        "Xen_1693"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Richardson, M.</span><span class="cmr-10x-x-109">&nbsp;J.,
        Campbell, W.</span><span class="cmr-10x-x-109">&nbsp;L., &amp; Schmidt,
        R.</span><span class="cmr-10x-x-109">&nbsp;C. (2009). Movement</span>
        <span class="cmr-10x-x-109">interference during action observation as emergent
        coordination.</span> <span class="cmti-10x-x-109">Neuroscience
        Letters</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">449</span><span class="cmr-10x-x-109">(2),
        117&ndash;122.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1427" name=
        "Xen_1427"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Roth,
        A.</span><span class="cmr-10x-x-109">&nbsp;S. (2004). Shared agency and
        contralateral commitments.</span> <span class="cmti-10x-x-109">The</span>
        <span class="cmti-10x-x-109">Philosophical Review</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">113</span><span class=
        "cmr-10x-x-109">(3), 359&ndash;410.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1860" name=
        "Xen_1860"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Schmidt,
        R.</span><span class="cmr-10x-x-109">&nbsp;C., Fitzpatrick, P., Caron, R.,
        &amp; Mergeche, J. (2010). Understanding</span> <span class=
        "cmr-10x-x-109">social motor coordination.</span> <span class=
        "cmti-10x-x-109">Human Movement Science</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">In Press, Corrected
        Proof</span><span class="cmr-10x-x-109">.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1211" name=
        "Xen_1211"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Searle,
        J.</span><span class="cmr-10x-x-109">&nbsp;R. (1983).</span> <span class=
        "cmti-10x-x-109">Intentionality: An Essay in the Philosophy of
        Mind</span><span class="cmr-10x-x-109">. Cambridge:</span> <span class=
        "cmr-10x-x-109">Cambridge University Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1369" name=
        "Xen_1369"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Searle,
        J.</span><span class="cmr-10x-x-109">&nbsp;R. (1990 [2002]). Collective
        intentions and actions. In</span> <span class="cmti-10x-x-109">Consciousness
        and</span> <span class="cmti-10x-x-109">Language</span> <span class=
        "cmr-10x-x-109">(pp.</span><span class="cmr-10x-x-109">&nbsp;90&ndash;105).
        Cambridge: Cambridge University Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1692" name=
        "Xen_1692"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Searle,
        J.</span><span class="cmr-10x-x-109">&nbsp;R. (1994).</span> <span class=
        "cmti-10x-x-109">The Construction of Social Reality</span><span class=
        "cmr-10x-x-109">. New York: The Free Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_2395" name=
        "Xen_2395"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Smith,
        T.</span><span class="cmr-10x-x-109">&nbsp;H. (forthcoming 2011). Playing
        one&rsquo;s part.</span> <span class="cmti-10x-x-109">Review of Philosophy
        and</span> <span class="cmti-10x-x-109">Psychology</span><span class=
        "cmr-10x-x-109">.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1373" name=
        "Xen_1373"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Sugden,
        R. (2000). Team preferences.</span> <span class="cmti-10x-x-109">Economics and
        Philosophy</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">16</span><span class="cmr-10x-x-109">,
        175&ndash;204.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1261" name=
        "Xen_1261"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Surian,
        L., Caldi, S., &amp; Sperber, D. (2007). Attribution of beliefs by
        13-month-old</span> <span class="cmr-10x-x-109">infants.</span> <span class=
        "cmti-10x-x-109">Psychological Science</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">18</span><span class=
        "cmr-10x-x-109">(7), 580&ndash;586.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1358" name=
        "Xen_1358"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Tollefsen, D. (2005). Let&rsquo;s pretend: Children and joint
        action.</span> <span class="cmti-10x-x-109">Philosophy of the</span>
        <span class="cmti-10x-x-109">Social Sciences</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">35</span><span class=
        "cmr-10x-x-109">(75), 74&ndash;97.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1828" name=
        "Xen_1828"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Tomasello, M. (2008).</span> <span class=
        "cmti-10x-x-109">Origins of HUman Communication</span><span class=
        "cmr-10x-x-109">. Cambridge, Mass.: MIT</span> <span class=
        "cmr-10x-x-109">Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1421" name=
        "Xen_1421"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Tomasello, M. &amp; Carpenter, M. (2007). Shared
        intentionality.</span> <span class="cmti-10x-x-109">Developmental
        Science</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">10</span><span class="cmr-10x-x-109">(1),
        121&ndash;5.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1090" name=
        "Xen_1090"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Tomasello, M., Carpenter, M., Call, J., Behne, T., &amp; Moll,
        H. (2005). Understanding</span> <span class="cmr-10x-x-109">and sharing
        intentions: The origins of cultural cognition.</span> <span class=
        "cmti-10x-x-109">Behavioral and Brain</span> <span class=
        "cmti-10x-x-109">Sciences</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">28</span><span class="cmr-10x-x-109">,
        675&ndash;735.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_557" name=
        "Xen_557"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Tomasello, M. &amp; Rakoczy, H. (2003). What makes human
        cognition unique? from</span> <span class="cmr-10x-x-109">individual to shared
        to collective intentionality.</span> <span class="cmti-10x-x-109">Mind and
        Language</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">18</span><span class="cmr-10x-x-109">(2),
        121&ndash;147.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1403" name=
        "Xen_1403"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Tuomela,
        R. (1995).</span> <span class="cmti-10x-x-109">The Importance of Us: A
        Philosophical Study of Basic Social</span> <span class=
        "cmti-10x-x-109">Notions</span><span class="cmr-10x-x-109">. Stanford: Stanford
        University Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1297" name=
        "Xen_1297"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Velleman, D. (1997). How to share an intention.</span>
        <span class="cmti-10x-x-109">Philosophy and Phenomenological</span>
        <span class="cmti-10x-x-109">Research</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">57</span><span class=
        "cmr-10x-x-109">(1), 29&ndash;50.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_25" name=
        "Xen_25"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Velleman, D. (2000).</span> <span class="cmti-10x-x-109">The
        Possibility of Practical Reason</span><span class="cmr-10x-x-109">. Oxford:
        Oxford University</span> <span class="cmr-10x-x-109">Press.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1778" name=
        "Xen_1778"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Vesper,
        C., Butterfill, S., Knoblich, G., &amp; Sebanz, N. (2010). A minimal
        architecture</span> <span class="cmr-10x-x-109">for joint action.</span>
        <span class="cmti-10x-x-109">Neural Networks</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">23</span><span class=
        "cmr-10x-x-109">(8-9), 998&ndash;1003.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_87" name=
        "Xen_87"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Wellman,
        H., Cross, D., &amp; Watson, J. (2001). Meta-analysis of theory of mind</span>
        <span class="cmr-10x-x-109">development: The truth about false-belief.</span>
        <span class="cmti-10x-x-109">Child Development</span><span class=
        "cmr-10x-x-109">,</span> <span class="cmti-10x-x-109">72</span><span class=
        "cmr-10x-x-109">(3), 655&ndash;684.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_89" name=
        "Xen_89"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Wimmer,
        H. &amp; Perner, J. (1983). Beliefs about beliefs: Representation and</span>
        <span class="cmr-10x-x-109">constraining function of wrong beliefs in young
        children&rsquo;s understanding of deception.</span> <span class=
        "cmti-10x-x-109">Cognition</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">13</span><span class="cmr-10x-x-109">,
        103&ndash;128.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_717" name=
        "Xen_717"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Woodward, A.</span><span class="cmr-10x-x-109">&nbsp;L. (1998).
        Infants selectively encode the goal object of an actor&rsquo;s</span>
        <span class="cmr-10x-x-109">reach.</span> <span class=
        "cmti-10x-x-109">Cognition</span><span class="cmr-10x-x-109">,</span>
        <span class="cmti-10x-x-109">69</span><span class="cmr-10x-x-109">,
        1&ndash;34.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_1439" name=
        "Xen_1439"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class=
        "cmr-10x-x-109">Woodward, A.</span><span class="cmr-10x-x-109">&nbsp;L. &amp;
        Sommerville, J.</span><span class="cmr-10x-x-109">&nbsp;A. (2000).
        Twelve-month-old infants interpret</span> <span class="cmr-10x-x-109">action in
        context.</span> <span class="cmti-10x-x-109">Psychological
        Science</span><span class="cmr-10x-x-109">,</span> <span class=
        "cmti-10x-x-109">11</span><span class="cmr-10x-x-109">(1),
        73&ndash;77.</span></p>

        <p class="bibitem"><span class="biblabel"><a id="Xen_161" name=
        "Xen_161"></a><span class="bibsp"><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span><span class=
        "cmr-10x-x-109">&nbsp;</span></span></span><span class="cmr-10x-x-109">Wright,
        L. (1976).</span> <span class="cmti-10x-x-109">Teleological
        Explanations</span><span class="cmr-10x-x-109">. Berkeley: University of
        California Press.</span></p>
      </div>


      <div class="footnotes">
        <h3>Footnotes</h3>
        <!--l. 69-->

        <p class="indent"><span class="footnote-mark"><a href="#fn1x0-bk" id="fn1x0"
        name="fn1x0"><sup class="textsuperscript">1</sup></a></span><span class=
        "cmr-10">Other notable contributions not discussed in this paper due to lack of
        space include</span> <a href="#Xen_1369"><span class=
        "cmr-10">Searle</span></a><span class="cmr-10">&nbsp;(</span><a href=
        "#Xen_1369"><span class="cmr-10">2002</span></a><span class="cmr-10">),</span>
        <a href="#Xen_1287"><span class="cmr-10">Gilbert</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1287"><span class=
        "cmr-10">2006</span></a><span class="cmr-10">),</span> <a href=
        "#Xen_1403"><span class="cmr-10">Tuomela</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1403"><span class=
        "cmr-10">1995</span></a><span class="cmr-10">),</span> <a href=
        "#Xen_1373"><span class="cmr-10">Sugden</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1373"><span class=
        "cmr-10">2000</span></a><span class="cmr-10">),</span> <a href=
        "#Xen_1383"><span class="cmr-10">Gold &amp; Sugden</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1383"><span class=
        "cmr-10">2007a</span></a><span class="cmr-10">),</span> <a href=
        "#Xen_1290"><span class="cmr-10">Kutz</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1290"><span class=
        "cmr-10">2000</span></a><span class="cmr-10">) and</span> <a href=
        "#Xen_1427"><span class="cmr-10">Roth</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1427"><span class=
        "cmr-10">2004</span></a><span class="cmr-10">).</span></p><!--l. 71-->

        <p class="indent"><span class="footnote-mark"><a href="#fn2x0-bk" id="fn2x0"
        name="fn2x0"><sup class="textsuperscript">2</sup></a></span><a href=
        "#Xen_1357"><span class="cmr-10">Bratman</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1357"><span class=
        "cmr-10">1997</span></a><span class="cmr-10">, p.</span><span class=
        "cmr-10">&nbsp;142). See</span> <a href="#Xen_1197"><span class=
        "cmr-10">Bratman</span></a><span class="cmr-10">&nbsp;(</span><a href=
        "#Xen_1197"><span class="cmr-10">1992</span></a><span class="cmr-10">,
        pp.</span><span class="cmr-10">&nbsp;338-9) for further details on the
        relation</span> <span class="cmr-10">between shared intentions and shared
        intentional activities.</span></p><!--l. 79-->

        <p class="indent"><span class="footnote-mark"><a href="#fn3x0-bk" id="fn3x0"
        name="fn3x0"><sup class="textsuperscript">3</sup></a></span><span class=
        "cmr-10">In</span> <a href="#Xen_1356"><span class=
        "cmr-10">Bratman</span></a><span class="cmr-10">&nbsp;(</span><a href=
        "#Xen_1356"><span class="cmr-10">1993</span></a><span class="cmr-10">), the
        following were offered as jointly sufficient</span> <span class="cmti-10">and
        individually necessary</span> <span class="cmr-10">conditions; the retreat to
        sufficient conditions occurs in</span> <a href="#Xen_1357"><span class=
        "cmr-10">Bratman</span></a><span class="cmr-10">&nbsp;(</span><a href=
        "#Xen_1357"><span class="cmr-10">1997</span></a><span class="cmr-10">,
        pp.</span><span class="cmr-10">&nbsp;143-4) where he notes</span> <span class=
        "cmr-10">that for all that I have said, shared intention might be multiply
        realizable.</span></p><!--l. 115-->

        <p class="indent"><span class="footnote-mark"><a href="#fn4x0-bk" id="fn4x0"
        name="fn4x0"><sup class="textsuperscript">4</sup></a></span><span class=
        "cmr-10">The most widely discussed propositional attitude has been belief; see
        (</span><a href="#Xen_87"><span class="cmr-10">Wellman</span> <span class=
        "cmr-10">et</span><span class="cmr-10">&nbsp;al.</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_87"><span class=
        "cmr-10">2001</span></a><span class="cmr-10">;</span><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_89"><span class="cmr-10">Wimmer &amp;
        Perner</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_89"><span class="cmr-10">1983</span></a><span class="cmr-10">) but also
        (</span><a href="#Xen_1789"><span class="cmr-10">Baillargeon
        et</span><span class="cmr-10">&nbsp;al.</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1789"><span class=
        "cmr-10">2010</span></a><span class="cmr-10">;</span><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1821"><span class=
        "cmr-10">Kov</span><span class="cmr-10">&Acirc;&middot;cs</span> <span class=
        "cmr-10">et</span><span class="cmr-10">&nbsp;al.</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1821"><span class=
        "cmr-10">2010</span></a><span class="cmr-10">;</span><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1092"><span class="cmr-10">Onishi &amp;
        Baillargeon</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1092"><span class="cmr-10">2005</span></a><span class=
        "cmr-10">;</span><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1261"><span class="cmr-10">Surian et</span><span class=
        "cmr-10">&nbsp;al.</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1261"><span class="cmr-10">2007</span></a><span class="cmr-10">). Apperly
        and Butterfill (</span><a href="#Xen_1686"><span class="cmr-10">Apperly
        &amp;</span> <span class="cmr-10">Butterfill</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1686"><span class=
        "cmr-10">2009</span></a><span class="cmr-10">) argue for the possibility that
        while children in their first and second year have</span> <span class=
        "cmr-10">abilities to track beliefs, they cannot ascribe beliefs or other
        propositional attitudes as</span> <span class="cmr-10">such.</span></p>
        <!--l. 120-->

        <p class="indent"><span class="footnote-mark"><a href="#fn5x0-bk" id="fn5x0"
        name="fn5x0"><sup class="textsuperscript">5</sup></a></span> <span class=
        "cmr-10">There is evidence that children of this age have difficulty
        understanding intentions</span> <span class="cmr-10">(</span><a href=
        "#Xen_69"><span class="cmr-10">Astington</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_69"><span class=
        "cmr-10">1991</span></a><span class="cmr-10">;</span><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_70"><span class="cmr-10">Astington &amp;
        Gopnik</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_70"><span class="cmr-10">1991</span></a><span class="cmr-10">). A range
        of researchers have argued</span> <span class="cmr-10">that infants form
        expectations about goal-directed activity (</span><a href=
        "#Xen_1438"><span class="cmr-10">Csibra</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1438"><span class=
        "cmr-10">2008</span></a><span class="cmr-10">;</span><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1207"><span class="cmr-10">Gergely</span>
        <span class="cmr-10">et</span><span class=
        "cmr-10">&nbsp;al.</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1207"><span class="cmr-10">1995</span></a><span class=
        "cmr-10">;</span><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_717"><span class="cmr-10">Woodward</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_717"><span class=
        "cmr-10">1998</span></a><span class="cmr-10">;</span><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1439"><span class="cmr-10">Woodward &amp;
        Sommerville</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1439"><span class="cmr-10">2000</span></a><span class="cmr-10">). It may
        be that the understanding</span> <span class="cmr-10">of goal-directed activity
        examined by these studies falls short of an understanding of</span>
        <span class="cmr-10">intention.</span></p><!--l. 123-->

        <p class="indent"><span class="footnote-mark"><a href="#fn6x0-bk" id="fn6x0"
        name="fn6x0"><sup class="textsuperscript">6</sup></a></span><span class=
        "cmr-10">A related objection may apply to Claire Hughes appeal to reciprocal
        exchanges.</span> <span class="cmr-10">She specifies that such exchanges depend
        on modelling the others intentions/desires</span> <span class="cmr-10">(i.e.
        reflecting on the others inner states) and monitoring the others understanding
        of</span> <span class="cmr-10">ones own intentions and desires (i.e. detecting
        mistaken beliefs about ones own inner</span> <span class="cmr-10">states)
        (</span><a href="#Xen_1300"><span class="cmr-10">Hughes et</span><span class=
        "cmr-10">&nbsp;al.</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1300"><span class="cmr-10">2006</span></a><span class="cmr-10">,
        p.</span><span class="cmr-10">&nbsp;56). Thus engaging in reciprocal exchanges
        appears to</span> <span class="cmr-10">require fluid and sophisticated
        ascriptions of mental states. Accordingly, engaging in</span> <span class=
        "cmr-10">such exchanges cannot significantly explain how children develop
        abilities to think about</span> <span class="cmr-10">minds.</span></p>
        <!--l. 128-->

        <p class="indent"><span class="footnote-mark"><a href="#fn7x0-bk" id="fn7x0"
        name="fn7x0"><sup class="textsuperscript">7</sup></a></span><a href=
        "#Xen_1358"><span class="cmr-10">Tollefsen</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1358"><span class=
        "cmr-10">2005</span></a><span class="cmr-10">) objects to Bratman's account of
        shared intention on the grounds that it requires</span> <span class=
        "cmr-10">common knowledge. This objection also fails because common knowledge
        is explicitly required by</span> <span class="cmr-10">what I am calling
        Bratman's substantial account only, which gives sufficient but not
        necessary</span> <span class="cmr-10">conditions for shared intention. (A
        second potential problem for Tollefsens argument is that it</span> <span class=
        "cmr-10">requires the premise that young children engage in joint actions of
        the kind Bratman's account aims</span> <span class="cmr-10">to characterise, the
        kind whose paradigms involve coordinating potentially long-term</span>
        <span class="cmr-10">plans.)</span></p><!--l. 141-->

        <p class="indent"><span class="footnote-mark"><a href="#fn8x0-bk" id="fn8x0"
        name="fn8x0"><sup class="textsuperscript">8</sup></a></span><a href=
        "#Xen_1358"><span class="cmr-10">Tollefsen</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_1358"><span class=
        "cmr-10">2005</span></a><span class="cmr-10">) argues that Bratman's account of
        joint action is incorrect on the</span> <span class="cmr-10">grounds that
        children engage in joint action but not in shared intentional activity
        as</span> <span class="cmr-10">characterised by Bratman. The considerations
        below identify a missing premise in her</span> <span class=
        "cmr-10">argument.</span></p><!--l. 181-->

        <p class="indent"><span class="footnote-mark"><a href="#fn9x0-bk" id="fn9x0"
        name="fn9x0"><sup class="textsuperscript">9</sup></a></span><span class=
        "cmr-10">There may be other notions of joint action. For instance, Ludwig
        asserts that [t]he concept of a</span> <span class="cmr-10">joint action as
        such is just that of an event of which there are multiple agents
        (</span><a href="#Xen_2394"><span class="cmr-10">2007</span></a><span class=
        "cmr-10">, p.</span><span class="cmr-10">&nbsp;366). As he</span> <span class=
        "cmr-10">notes, it is a straightforward consequence of this view that if there
        are joint actions then there are</span> <span class="cmr-10">joint actions
        without shared intentions.</span></p><!--l. 206-->

        <p class="indent"><span class="footnote-mark"><a href="#fn10x0-bk" id="fn10x0"
        name="fn10x0"><sup class="textsuperscript">10</sup></a></span><span class=
        "cmr-10">This is suggested by Vellemans (</span><a href="#Xen_25"><span class=
        "cmr-10">2000</span></a><span class="cmr-10">, pp.</span><span class=
        "cmr-10">&nbsp;10-30) discussion of purposeful activity.</span></p>
        <!--l. 208-->

        <p class="indent"><span class="footnote-mark"><a href="#fn11x0-bk" id="fn11x0"
        name="fn11x0"><sup class="textsuperscript">11</sup></a></span><span class=
        "cmr-10">This intuition provides a loose connection between the notion of
        shared goals and Millers notion</span> <span class="cmr-10">of a collective end
        (</span><a href="#Xen_1827"><span class="cmr-10">Miller</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_1827"><span class=
        "cmr-10">2001</span></a><span class="cmr-10">). While it would be useful to
        discuss differences and similarities in</span> <span class="cmr-10">substance
        and motivation, there is no space to do that here.</span></p><!--l. 245-->

        <p class="noindent"><span class="footnote-mark"><a href="#fn12x0-bk" id=
        "fn12x0" name="fn12x0"><sup class=
        "textsuperscript">12</sup></a></span><span class="cmr-10">The on balance
        qualification in conditions (c) and (d) rules out cases where agents do have
        the</span> <span class="cmr-10">specified expectations about goals and outcomes
        but also have further, conflicting expectations</span> <span class=
        "cmr-10">which outweigh them.</span></p><!--l. 262-->

        <p class="indent"><span class="footnote-mark"><a href="#fn13x0-bk" id="fn13x0"
        name="fn13x0"><sup class="textsuperscript">13</sup></a></span><span class=
        "cmr-10">On goals as functions of actions see, for example, Wright
        (</span><a href="#Xen_161"><span class="cmr-10">Wright</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_161"><span class=
        "cmr-10">1976</span></a><span class="cmr-10">) and Price (</span><a href=
        "#Xen_139"><span class="cmr-10">Price</span></a><span class=
        "cmr-10">&nbsp;</span><a href="#Xen_139"><span class=
        "cmr-10">2001</span></a><span class="cmr-10">).</span> <span class="cmr-10">A
        variety of research supports the claim that young children, non-human primates
        and corvids track</span> <span class="cmr-10">the functions of things
        (including Rakoczy and Tomasello 2007; Casler and Kelemen 2007; Csibra</span>
        <span class="cmr-10">and Gergely 2007; Kelemen 1999; German and Defeyter 2000;
        Hauser 1997; Emery and</span> <span class="cmr-10">Clayton 2004). On the
        abilities of these groups to represent goals specifically, see further</span>
        <span class="cmr-10">footnote</span><span class="cmr-10">&nbsp;</span><a href=
        "#x1-4002f5"><span class="cmr-10">5</span>
        <!--tex4ht:ref: fn:goals --></a><span class="cmr-10">.</span></p><!--l. 270-->

        <p class="indent"><span class="footnote-mark"><a href="#fn14x0-bk" id="fn14x0"
        name="fn14x0"><sup class="textsuperscript">14</sup></a></span><span class=
        "cmr-10">This view is endorsed by</span> <a href="#Xen_1356"><span class=
        "cmr-10">Bratman</span></a><span class="cmr-10">&nbsp;(</span><a href=
        "#Xen_1356"><span class="cmr-10">1993</span></a><span class="cmr-10">,
        p.</span><span class="cmr-10">&nbsp;103) and rejected by</span> <a href=
        "#Xen_2394"><span class="cmr-10">Ludwig</span></a><span class=
        "cmr-10">&nbsp;(</span><a href="#Xen_2394"><span class=
        "cmr-10">2007</span></a><span class="cmr-10">,</span> <span class=
        "cmr-10">pp.</span><span class="cmr-10">&nbsp;387-8).</span></p><!--l. 287-->

        <p class="indent"><span class="footnote-mark"><a href="#fn15x0-bk" id="fn15x0"
        name="fn15x0"><sup class="textsuperscript">15</sup></a></span><span class=
        "cmr-10">This example is can be modelled as a Hawk-Dove game and is adapted
        from a discussion of Gold</span> <span class="cmr-10">and Sugden
        (</span><a href="#Xen_2393"><span class="cmr-10">Gold &amp;
        Sugden</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_2393"><span class="cmr-10">2007b</span></a><span class="cmr-10">,
        pp.</span><span class="cmr-10">&nbsp;304-8). As these authors note, the
        combination of actions is</span> <span class="cmr-10">not a rational
        consequence of team reasoning (</span><a href="#Xen_1373"><span class=
        "cmr-10">Sugden</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1373"><span class="cmr-10">2000</span></a><span class="cmr-10">, on team
        reasoning see) and so does not</span> <span class="cmr-10">involve the
        associated notion of group agency (</span><a href="#Xen_1383"><span class=
        "cmr-10">Gold &amp; Sugden</span></a><span class="cmr-10">&nbsp;</span><a href=
        "#Xen_1383"><span class="cmr-10">2007a</span></a><span class=
        "cmr-10">).</span></p><!--l. 297-->

        <p class="indent"><span class="footnote-mark"><a href="#fn16x0-bk" id="fn16x0"
        name="fn16x0"><sup class="textsuperscript">16</sup></a></span><span class=
        "cmr-10">Knoblich and Sebanz claim that lifting the basket together requires
        joint intentionality which in</span> <span class="cmr-10">turn requires shared
        intentions in roughly Bratman's sense: [t]here needs to be an intentional</span>
        <span class="cmr-10">structure that allows an actor to relate his/her own
        intention and the others intention to an intention</span> <span class=
        "cmr-10">that drives the joint activity (</span><a href=
        "#Xen_1429"><span class="cmr-10">2008</span></a><span class="cmr-10">,
        p.</span><span class="cmr-10">&nbsp;2025). I reject this claim for the reasons
        given</span> <span class="cmr-10">above.</span></p>
      </div>
</file>

<file path="src/content/writing/perceiving_expressions_emotion.md">
---
title: "Perceiving Expressions of Emotion: What evidence could bear on questions
  about perceptual experience of mental states?"
authors: Stephen A. Butterfill
year: 2015
isForthcoming: false
journal: Consciousness and Cognition
volume: "36"
pages: 438--451
doi: 10.1016/j.concog.2015.03.008
pdfUrl: /pdf/perceiving_expressions_emotion.pdf
---

## Abstract

What evidence could bear on questions about whether humans ever perceptually experience any of another’s mental states, and how might those questions be made precise enough to test experimentally? This paper focusses on emotions and their expression. It is proposed that research on perceptual experiences of physical properties provides one model for thinking about what evidence concerning expressions of emotion might reveal about perceptual experiences of others’ mental states. This proposal motivates consideration of the hypothesis that categorical perception of expressions of emotion occurs, can be facilitated by information about agents’ emotions, and gives rise to phenomenal expectations. It is argued that the truth of this hypothesis would support a modest version of the claim that humans sometimes perceptually experience some of another’s mental states. Much available evidence is consistent with, but insufficient to establish, the truth of the hypothesis. We are probably not yet in a position to know whether humans ever perceptually experience others’ mental states.
</file>

<file path="src/content/writing/primal_self.md">
---
title: "From foraging to autonoetic consciousness: The primal self as a
  consequence of embodied prospective foraging"
authors: Thomas T. Hills and Stephen A. Butterfill
year: 2015
isForthcoming: false
journal: Current Zoology
volume: "61"
number: "2"
pages: 368--381
pdfUrl: /pdf/primal_self.pdf
---

## Abstract

The capacity to adapt to resource distributions by modulating the frequency of exploratory and exploitative behaviors is common across metazoans and is arguably a principal selective force in the evolution of cognition. Here we (1) review recent work investigating behavioral and biological commonalities between external foraging in space and internal foraging over environments specified by cognitive representations, and (2) explore the implications of these commonalities for understanding the origins of the self. Behavioural commonalities include the capacity for what is known as area-restricted search in the ecological literature: this is search focussed around locations where resources have been found in the past, but moving away from locations where few resources are found, and capable of producing movement patterns mimicking Lévy flights. Area-restricted search shares a neural basis across metazoans, and these biological commonalities in vertebrates suggest an evolutionary homology between external and internal foraging. Internal foraging, and in particular a form we call embodied prospective foraging, makes available additional capacities for prediction based on search through a cognitive representation of the external environment, and allows predictions about outcomes of possible future actions. We demonstrate that cognitive systems that use embodied prospective foraging require a primitive sense of self, needed to distinguish actual from simulated action. This relationship has implications for understanding the evolution of autonoetic consciousness and self-awareness.
</file>

<file path="src/content/writing/puzzle_thought_experience_motoric.md">
---
title: On a Puzzle about Relations between Thought, Experience and the Motoric
authors: Corrado Sinigaglia and Stephen A. Butterfill
year: 2015
isForthcoming: false
journal: Synthese
volume: "192"
number: "6"
pages: 1923-1936
doi: 10.1007/s11229-015-0672-x
pdfUrl: /pdf/puzzle_thought_experience_motoric.pdf
---

## Abstract

Motor representations live a kind of double life. Although paradigmatically involved in performing actions, they also occur when merely observing others act and sometimes influence thoughts about the goals of observed actions. Further, these influences are content-respecting: what you think about an action sometimes depends in part on how that action is represented motorically in you.  The existence of such content-respecting influences is puzzling. After all, motor representations do not feature alongside beliefs or intentions  in reasoning about action; indeed, thoughts are inferentially isolated from motor representations. So how could motor representations have content-respecting influences on thoughts? Our aim is to solve this puzzle.  In so doing, we shall provide the basis for an account of how experience links the motoric with thought. Such an account matters for understanding how humans think about action: in some cases, we have reasons for thoughts about actions that we would not have if we were unable to represent those actions motorically.
</file>

<file path="src/content/writing/what_does_knowledge_explain.html.md">
---
title: What Does Knowledge Explain? Commentary on Jennifer Nagel
authors: Stephen A. Butterfill
year: 2013
isForthcoming: false
booktitle: Oxford Studies in Epistemology
volume: "4"
pages: "309-320"
pdfUrl: /pdf/what_does_knowledge_explain.pdf
---

## Abstract

Knowledge is a mental state: Nagel may be right about this  but wrong to suppose that knowledge is prior to belief in the sense that being able to recognize belief somehow depends on having a concept of knowledge.  This commentary identifies objections  to Nagel's arguments for priority.  Some of these objections arise from Nagel's selective use of developmental evidence on mindreading: additional findings reveal a more complex (and more interesting) picture of how abilities to recognize and track knowledge and belief develop.  If Nagel's arguments for priority fail, why hold that knowledge is a mental state?  An alternative approach might draw on arguments that intention is a mental state.  Knowledge and intention play complementary and interlocking roles in planning and practical reasoning.  Perhaps it is these roles, not claims about priority, which complicate attempts to reduce either knowledge or intention to belief or desire or some combination of these.


<p>Nagel's paper is on PhilPapers: <a href="http://philpapers.org/archive/NAGKAA-3.1.pdf">philpapers.org/archive/NAGKAA-3.1.pdf</a></p>
</file>

<file path="src/content/config.ts">
import { defineCollection, z } from 'astro:content';
import { glob } from 'astro/loaders';

// Schema for 'writing' (publications)
const writingCollection = defineCollection({
  schema: z.object({
    title: z.string(),
    authors: z.string(),
    pubDate: z.date().optional(),
    year: z.number(),
    isForthcoming: z.boolean().optional(),
    journal: z.string().optional(),
    booktitle: z.string().optional(),
    volume: z.string().optional(),
    number: z.string().optional(),
    pages: z.string().optional(),
    doi: z.string().optional(),
    pdfUrl: z.string().optional(), // Will only be present if a PDF exists
  }),
});

// Schema for 'talks'
const talksCollection = defineCollection({
  schema: z.object({
    title: z.string(),
    authors: z.string(),
    pubDate: z.date(),
    endDate: z.date().optional(),
    event: z.string().optional(),
    address: z.string().optional(),
    handoutUrl: z.string().optional(),
    slidesUrl: z.string().optional(),
    slideImages: z.array(z.string()).optional(), // For Reveal.js slide decks
  }),
});

// Schema for 'teaching' (courses)
const teachingCollection = defineCollection({
    loader: glob({ pattern: '*.html.md', base: './src/content/teaching' }),
    schema: z.object({
        title: z.string(),
        year: z.string(),
        term: z.string(),
        authors: z.string(),
        place: z.string(),
        abstract: z.string(),
        // Defines an object where keys are strings (e.g., '01') and values are strings (lecture titles)
        lectures: z.record(z.string()),
    }),
});

export const collections = {
  'writing': writingCollection,
  'talks': talksCollection,
  'teaching': teachingCollection,
};
</file>

<file path="src/lib/components/ui/command/command-empty.svelte">
<script lang="ts">
	import { Command as CommandPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: CommandPrimitive.EmptyProps = $props();
</script>

<CommandPrimitive.Empty
	bind:ref
	data-slot="command-empty"
	class={cn("py-6 text-center text-sm text-gray-600 dark:text-gray-400", className)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/command/command-group.svelte">
<script lang="ts">
	import { Command as CommandPrimitive, useId } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		children,
		heading,
		value,
		...restProps
	}: CommandPrimitive.GroupProps & {
		heading?: string;
	} = $props();
</script>

<CommandPrimitive.Group
	bind:ref
	data-slot="command-group"
	class={cn("text-gray-900 dark:text-gray-100 overflow-hidden p-1", className)}
	value={value ?? heading ?? `----${useId()}`}
	{...restProps}
>
	{#if heading}
		<CommandPrimitive.GroupHeading
			class="text-gray-600 dark:text-gray-400 px-2 py-1.5 text-xs font-medium"
		>
			{heading}
		</CommandPrimitive.GroupHeading>
	{/if}
	<CommandPrimitive.GroupItems {children} />
</CommandPrimitive.Group>
</file>

<file path="src/lib/components/ui/command/command-input.svelte">
<script lang="ts">
	import { Command as CommandPrimitive } from "bits-ui";
	import SearchIcon from "@lucide/svelte/icons/search";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		value = $bindable(""),
		...restProps
	}: CommandPrimitive.InputProps = $props();
</script>

<div class="flex h-9 items-center gap-2 border-b border-gray-200 dark:border-gray-700 px-3" data-slot="command-input-wrapper">
	<SearchIcon class="size-4 shrink-0 text-gray-500 dark:text-gray-400" />
	<CommandPrimitive.Input
		data-slot="command-input"
		class={cn(
			"placeholder:text-gray-500 dark:placeholder:text-gray-400 focus:outline-none flex h-10 w-full rounded-md bg-transparent py-3 text-sm text-gray-900 dark:text-gray-100 disabled:cursor-not-allowed disabled:opacity-50",
			className
		)}
		bind:ref
		{...restProps}
		bind:value
	/>
	<!-- Note: focus:outline-none removes the blue focus border for this input.
	     This is intentional for the command palette input as the visual focus 
	     is provided by the dialog container styling instead. Do not re-add 
	     the outline as it creates a misplaced blue border. -->
</div>
</file>

<file path="src/lib/components/ui/command/command-item.svelte">
<script lang="ts">
	import { Command as CommandPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: CommandPrimitive.ItemProps = $props();
</script>

<CommandPrimitive.Item
	bind:ref
	data-slot="command-item"
	class={cn(
		"aria-selected:bg-blue-100 dark:aria-selected:bg-blue-900 aria-selected:text-blue-900 dark:aria-selected:text-blue-100 hover:bg-gray-100 dark:hover:bg-gray-800 [&_svg:not([class*='text-'])]:text-gray-500 dark:[&_svg:not([class*='text-'])]:text-gray-400 outline-hidden relative flex cursor-default select-none items-center gap-2 rounded-sm px-2 py-1.5 text-sm data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg:not([class*='size-'])]:size-4 [&_svg]:pointer-events-none [&_svg]:shrink-0",
		className
	)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/command/command.svelte">
<script lang="ts">
	import { Command as CommandPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		value = $bindable(""),
		class: className,
		...restProps
	}: CommandPrimitive.RootProps = $props();
</script>

<CommandPrimitive.Root
	bind:value
	bind:ref
	data-slot="command"
	class={cn(
		"bg-white dark:bg-gray-900 text-gray-900 dark:text-gray-100 flex h-full w-full flex-col overflow-hidden rounded-md",
		className
	)}
	{...restProps}
/>
</file>

<file path="src/lib/components/ui/dialog/dialog-content.svelte">
<script lang="ts">
	import { Dialog as DialogPrimitive } from "bits-ui";
	import XIcon from "@lucide/svelte/icons/x";
	import type { Snippet } from "svelte";
	import * as Dialog from "./index.js";
	import { cn, type WithoutChildrenOrChild } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		portalProps,
		children,
		showCloseButton = true,
		...restProps
	}: WithoutChildrenOrChild<DialogPrimitive.ContentProps> & {
		portalProps?: DialogPrimitive.PortalProps;
		children: Snippet;
		showCloseButton?: boolean;
	} = $props();
</script>

<Dialog.Portal {...portalProps}>
	<Dialog.Overlay />
	<DialogPrimitive.Content
		bind:ref
		data-slot="dialog-content"
		class={cn(
			"bg-white dark:bg-gray-900 text-gray-900 dark:text-gray-100 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 fixed left-[50%] top-[50%] z-50 grid w-full max-w-[calc(100%-2rem)] translate-x-[-50%] translate-y-[-50%] gap-4 rounded-lg border border-gray-200 dark:border-gray-700 p-6 shadow-lg duration-200 sm:max-w-lg",
			className
		)}
		{...restProps}
	>
		{@render children?.()}
		{#if showCloseButton}
			<DialogPrimitive.Close
				class="ring-offset-background focus:ring-ring rounded-xs focus:outline-hidden absolute right-4 top-4 opacity-70 transition-opacity hover:opacity-100 focus:ring-2 focus:ring-offset-2 disabled:pointer-events-none [&_svg:not([class*='size-'])]:size-4 [&_svg]:pointer-events-none [&_svg]:shrink-0"
			>
				<XIcon />
				<span class="sr-only">Close</span>
			</DialogPrimitive.Close>
		{/if}
	</DialogPrimitive.Content>
</Dialog.Portal>
</file>

<file path="src/lib/components/ui/dialog/dialog-overlay.svelte">
<script lang="ts">
	import { Dialog as DialogPrimitive } from "bits-ui";
	import { cn } from "$lib/utils.js";

	let {
		ref = $bindable(null),
		class: className,
		...restProps
	}: DialogPrimitive.OverlayProps = $props();
</script>

<DialogPrimitive.Overlay
	bind:ref
	data-slot="dialog-overlay"
	class={cn(
		"data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/80",
		className
	)}
	{...restProps}
/>
</file>

<file path="src/styles/global.css">
@tailwind base;
@tailwind components;
@tailwind utilities;
</file>

<file path="tsconfig.json">
{
  "extends": "astro/tsconfigs/strict",
  "include": [".astro/types.d.ts", "**/*"],
  "exclude": ["dist"],
  "compilerOptions": {
    // Add these two properties:
    "baseUrl": ".",
    "paths": {
      "$lib": ["src/lib"],
      "$lib/*": ["src/lib/*"]
    }
  }
}
</file>

<file path="src/layouts/PageLayout.astro">
---
// src/layouts/PageLayout.astro
import BaseLayout from './BaseLayout.astro';

interface ContextualAction {
  label: string;
  action: string;
  url?: string;
}

interface Props {
  title: string;
  contextualActions?: ContextualAction[];
}
const { title, contextualActions } = Astro.props;
---
<BaseLayout title={title} contextualActions={contextualActions}>
  <div class="container mx-auto max-w-4xl px-4 py-8">
    <article class="prose prose-slate dark:prose-invert max-w-none">
      <h1 class="text-3xl font-bold tracking-tight text-slate-800 dark:text-slate-200 mb-6">{title}</h1>
      <div class="text-slate-500 dark:text-slate-400 leading-relaxed">
        <slot /> <!-- The Markdown body content will go here -->
      </div>
    </article>
  </div>
</BaseLayout>
</file>

<file path="src/pages/writing/[...slug].astro">
---
// src/pages/writing/[...slug].astro
import { getCollection } from 'astro:content';
import PageLayout from '../../layouts/PageLayout.astro';

// This function runs at build time to find all publications
// and tell Astro to create a page for each one.
export async function getStaticPaths() {
  const writingEntries = await getCollection('writing');
  return writingEntries.map(entry => ({
    params: { slug: entry.slug },
    props: { entry },
  }));
}

import CopyForChat from '../../components/CopyForChat.svelte';

const { entry } = Astro.props;
const { Content } = await entry.render(); // This gets the Markdown content

// Format the content specifically for the clipboard
// Use the raw markdown body instead of the rendered Content component
const textForLLM = `
Title: ${entry.data.title}
Authors: ${entry.data.authors}
Year: ${entry.data.year}

---

${entry.body}
`.trim();


const pdfUrl = entry.data.pdfUrl;

// Define actions specific to this page
const pageActions = [];
if (pdfUrl) {
  pageActions.push({
    label: 'Download PDF',
    action: 'openUrl',
    url: pdfUrl
  });
}

---
<PageLayout title={entry.data.title} contextualActions={pageActions}>
  <div class="flex justify-end mb-4">
    <CopyForChat contentToCopy={textForLLM} client:load />
  </div>
  <div class="mb-4 text-sm text-slate-600 dark:text-slate-400">
    <p><strong>Authors:</strong> {entry.data.authors}</p>
    <p><strong>Published:</strong> {entry.data.year}</p>
    {entry.data.journal && <p><strong>Journal:</strong> {entry.data.journal}</p>}
    {entry.data.doi && <p><a href={`http://dx.doi.org/${entry.data.doi}`} target="_blank">DOI: {entry.data.doi}</a></p>}
  </div>
  <Content /> <!-- This renders the main body of the Markdown file -->
</PageLayout>
</file>

<file path="astro.config.mjs">
// @ts-check
import { defineConfig } from 'astro/config';
import svelte from '@astrojs/svelte';
import tailwind from '@astrojs/tailwind'; // 1. Import the integration

// https://astro.build/config
export default defineConfig({
  // 2. Add the integration to the array
  integrations: [svelte(), tailwind()],
});
</file>

<file path="package.json">
{
  "name": "",
  "type": "module",
  "version": "0.0.1",
  "scripts": {
    "dev": "astro dev",
    "build": "node ./scripts/generate-llms.mjs && astro build",
    "preview": "astro preview",
    "astro": "astro"
  },
  "dependencies": {
    "@astrojs/svelte": "^7.1.0",
    "@astrojs/tailwind": "^6.0.2",
    "astro": "^5.12.9",
    "reveal.js": "^5.2.1",
    "svelte": "^5.38.0",
    "typescript": "^5.9.2"
  },
  "devDependencies": {
    "@internationalized/date": "^3.8.2",
    "@lucide/svelte": "^0.515.0",
    "@tailwindcss/vite": "^4.1.11",
    "bits-ui": "^2.9.2",
    "clsx": "^2.1.1",
    "cson-parser": "^4.0.9",
    "fs-extra": "^11.3.1",
    "pug": "^3.0.3",
    "tailwind-merge": "^3.3.1",
    "tailwind-variants": "^2.1.0",
    "tailwindcss": "^3.4.17",
    "tailwindcss-animate": "^1.0.7",
    "tw-animate-css": "^1.3.6",
    "yaml": "^2.8.1"
  }
}
</file>

<file path="src/layouts/BaseLayout.astro">
---
// src/layouts/BaseLayout.astro
import CommandPalette from '../components/CommandPalette.svelte';
import SearchTrigger from '../components/SearchTrigger.svelte';
import { getCollection } from 'astro:content';
import ThemeToggle from '../components/ThemeToggle.svelte';
import '../styles/global.css'; 

// Fetch ALL content at build time
const writing = await getCollection('writing');
const talks = await getCollection('talks');
const teaching = await getCollection('teaching');

// Format it into a simple structure for the command palette
const allPages = [
  ...writing.map(item => ({ title: item.data.title, url: `/writing/${item.slug}/` })),
  ...talks.map(item => ({ title: item.data.title, url: `/talks/${item.slug}/` })),
  ...teaching.map(item => ({ title: item.data.title, url: `/teaching/${item.slug}/` })),
];

interface ContextualAction {
  label: string;
  action: string;
  url?: string;
}

interface Props {
  title: string;
  // Allow pages to pass extra actions
  contextualActions?: ContextualAction[];
}

const { title, contextualActions = [] } = Astro.props;
---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <title>{title}</title>
    <script is:inline>
      // This script is executed as-is, before the page is rendered.
      const theme = (() => {
        if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
          return localStorage.getItem('theme');
        }
        if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
          return 'dark';
        }
        return 'light';
      })();

      if (theme === 'dark') {
        document.documentElement.classList.add('dark');
      } else {
        document.documentElement.classList.remove('dark');
      }
    </script>
  </head>
  <body class="bg-white dark:bg-slate-900 text-slate-800 dark:text-slate-200 min-h-screen">
    <header class="border-b border-slate-200 dark:border-slate-800 bg-white/95 dark:bg-slate-900/95 backdrop-blur supports-[backdrop-filter]:bg-white/60 dark:supports-[backdrop-filter]:bg-slate-900/60">
      <div class="container mx-auto max-w-6xl px-4 py-4">
        <div class="flex items-center justify-between">
          <div class="flex items-center space-x-4">
            <a href="/" class="text-xl font-semibold text-slate-800 dark:text-slate-200 hover:text-blue-600 dark:hover:text-blue-400 transition-colors">
              Stephen Butterfill
            </a>
          </div>
          <div class="flex items-center space-x-4">
            <SearchTrigger client:load />
            <nav class="flex items-center space-x-6">
              <a href="/writing/" class="text-sm font-medium text-slate-500 dark:text-slate-400 hover:text-slate-800 dark:hover:text-slate-200 transition-colors">
                Writing
              </a>
              <a href="/talks/" class="text-sm font-medium text-slate-500 dark:text-slate-400 hover:text-slate-800 dark:hover:text-slate-200 transition-colors">
                Talks
              </a>
              <a href="/teaching/" class="text-sm font-medium text-slate-500 dark:text-slate-400 hover:text-slate-800 dark:hover:text-slate-200 transition-colors">
                Teaching
              </a>
              <ThemeToggle client:load />
            </nav>
          </div>
        </div>
      </div>
    </header>
    <main class="flex-1">
      <slot /> <!-- Page content will be injected here -->
    </main>
    <footer class="border-t border-slate-200 dark:border-slate-800 bg-slate-100/50 dark:bg-slate-800/50 py-6 text-center text-sm text-slate-500 dark:text-slate-400">
      <div class="container mx-auto max-w-6xl px-4">
        <p>&copy; {new Date().getFullYear()} Stephen Butterfill. All rights reserved.</p>
      </div>
    </footer>
    <CommandPalette allPages={allPages} contextualActions={contextualActions} client:load />
  </body>
</html>
</file>

<file path="src/pages/index.astro">
---
// src/pages/index.astro
import BaseLayout from '../layouts/BaseLayout.astro';
import Sidebar from '../components/Sidebar.astro';
import { getCollection } from 'astro:content';

// Get all publications
const allWriting = (await getCollection('writing')).sort(
  (a, b) => {
    const aDate = new Date(a.data.year, 0, 1).valueOf();
    const bDate = new Date(b.data.year, 0, 1).valueOf();
    return bDate - aDate;
  }
);
---
<BaseLayout title="Stephen Butterfill | Home">
  <div class="container mx-auto max-w-6xl px-4 py-8">
    <div class="grid grid-cols-1 lg:grid-cols-4 gap-8">
      <!-- Sidebar -->
      <aside class="lg:col-span-1">
        <Sidebar />
      </aside>
      
      <!-- Main Content -->
      <main class="lg:col-span-3 space-y-8">
        <!-- Hero Section -->
        <section id="about" class="text-center space-y-4">
          <h1 class="text-4xl font-bold tracking-tight text-slate-800 dark:text-slate-200">Stephen Butterfill</h1>
          <p class="text-lg text-slate-500 dark:text-slate-400 max-w-2xl mx-auto">
            Philosopher specializing in joint action, mindreading, and cognitive development
          </p>
        </section>

        <!-- Writing Section -->
        <section id="writing" class="space-y-6">
          <h2 class="text-2xl font-semibold tracking-tight text-slate-800 dark:text-slate-200 border-b border-slate-200 dark:border-slate-800 pb-2">
            Writing
          </h2>
          <div class="grid gap-4">
            {allWriting.map(item => (
              <article class="group border border-slate-200 dark:border-slate-800 rounded-lg p-4 hover:bg-slate-100/50 dark:hover:bg-slate-800/50 transition-colors">
                <a href={`/writing/${item.slug}/`} class="block space-y-2">
                  <h3 class="font-medium text-slate-800 dark:text-slate-200 group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors">
                    {item.data.title}
                  </h3>
                  <div class="flex items-center gap-2 text-sm text-slate-500 dark:text-slate-400">
                    <span>{item.data.year}</span>
                    {item.data.journal && (
                      <>
                        <span>•</span>
                        <span>{item.data.journal}</span>
                      </>
                    )}
                  </div>
                </a>
              </article>
            ))}
          </div>
        </section>

        <!-- Talks Section -->
        <section id="talks" class="space-y-6">
          <h2 class="text-2xl font-semibold tracking-tight text-slate-800 dark:text-slate-200 border-b border-slate-200 dark:border-slate-800 pb-2">
            Talks
          </h2>
          <p class="text-slate-600 dark:text-slate-400">
            Coming soon...
          </p>
        </section>

        <!-- Teaching Section -->
        <section id="teaching" class="space-y-6">
          <h2 class="text-2xl font-semibold tracking-tight text-slate-800 dark:text-slate-200 border-b border-slate-200 dark:border-slate-800 pb-2">
            Teaching
          </h2>
          <p class="text-slate-600 dark:text-slate-400">
            Coming soon...
          </p>
        </section>
      </main>
    </div>
  </div>
</BaseLayout>
</file>

<file path="migration-script.mjs">
import fs from 'fs-extra';
import path from 'path';
import CSON from 'cson-parser';
import pug from 'pug';
import yaml from 'yaml';

// --- CONFIGURATION ---
const OLD_DOCS_DIR = '../www-butterfill-old/src/documents';
const OLD_RAW_DIR = '../www-butterfill-old/src/raw';
const OLD_FILES_DIR = '../www-butterfill-old/src/files';

const NEW_CONTENT_DIR = 'src/content';
const PUBLIC_DIR = 'public';

// --- CORE FUNCTIONS ---

/**
 * Finds all relevant source files from the old project directory.
 * @returns {Promise<string[]>} A promise that resolves to an array of relative file paths.
 */
async function findSourceFiles() {
  if (!await fs.pathExists(OLD_DOCS_DIR)) {
    console.error(`Error: Source directory not found at ${path.resolve(OLD_DOCS_DIR)}`);
    console.error('Please make sure the www-butterfill-old directory is in the same parent folder as www-butterfill-new.');
    process.exit(1);
  }

  const allFiles = await fs.readdir(OLD_DOCS_DIR, { recursive: true });

  return allFiles.filter(file => {
    const fullPath = path.join(OLD_DOCS_DIR, file);
    if (!fs.statSync(fullPath).isFile()) {
      return false;
    }

    const normalizedPath = file.replace(/\\/g, '/');
    const collection = normalizedPath.split('/')[0];

    return ['writing', 'talks', 'teaching'].includes(collection) &&
           (file.endsWith('.html') || file.endsWith('.html.md') || file.endsWith('.jade'));
  });
}

/**
 * Extracts the CSON/YAML frontmatter and the body from a file's content.
 * @param {string} content The content of the file.
 * @returns {{frontmatter: string|null, body: string}}
 */
function extractFrontmatter(content) {
  const match = content.match(/^---\s*c?s?o?n?\s*\r?\n([\s\S]*?)\r?\n---/);
  if (match) {
    return {
      frontmatter: match[1],
      body: content.slice(match[0].length),
    };
  }
  return { frontmatter: null, body: content };
}

/**
 * Transforms the old CSON frontmatter object to the new schema.
 * @param {object} oldData The parsed CSON frontmatter.
 * @param {string} oldFilePath The relative path of the old file.
 * @returns {{newData: object, bodyAbstract: string}}
 */
function transformFrontmatter(oldData, oldFilePath) {
    if (!oldData) return { newData: {}, bodyAbstract: '' };

    const newData = {};
    const normalizedPath = oldFilePath.replace(/\\/g, '/');

    newData.title = oldData.title || 'Untitled';
    newData.authors = oldData.authors || 'Unknown';

    if (oldData.date) {
        try {
            newData.pubDate = new Date(oldData.date);
        } catch (e) {
            console.warn(`Invalid date format for ${oldFilePath}: ${oldData.date}`);
        }
    }

    if (normalizedPath.startsWith('writing/')) {
        newData.year = oldData.year ? parseInt(oldData.year, 10) : (newData.pubDate ? newData.pubDate.getFullYear() : undefined);
        if (isNaN(newData.year)) delete newData.year;
        if (oldData.isForthcoming !== undefined) newData.isForthcoming = oldData.isForthcoming;
        if (oldData.journal) newData.journal = oldData.journal;
        if (oldData.booktitle) newData.booktitle = oldData.booktitle;
        if (oldData.volume) newData.volume = String(oldData.volume);
        if (oldData.number) newData.number = String(oldData.number);
        if (oldData.pages) newData.pages = oldData.pages;
        if (oldData.doi) newData.doi = oldData.doi;
    } else if (normalizedPath.startsWith('talks/')) {
        if (oldData.date_end && newData.pubDate) {
            const startDate = newData.pubDate;
            const endDate = new Date(startDate.getFullYear(), startDate.getMonth(), parseInt(oldData.date_end, 10));
            newData.endDate = endDate;
        }
        if (oldData.event) newData.event = oldData.event;
        if (oldData.address) newData.address = oldData.address;
    } else if (normalizedPath.startsWith('teaching/')) {
        if (oldData.year) newData.year = String(oldData.year);
        if (oldData.term) newData.term = oldData.term;
        if (oldData.place) newData.place = oldData.place;
        if (oldData.lectures) newData.lectures = oldData.lectures;
        if (oldData.abstract) newData.abstract = oldData.abstract;
    }

    let bodyAbstract = '';
    if (oldData.abstract && !normalizedPath.startsWith('teaching/')) {
        bodyAbstract = `## Abstract\n\n${oldData.abstract.trim()}\n\n`;
    }

    return { newData, bodyAbstract };
}

/**
 * Processes the body of a file, converting Jade to HTML if necessary.
 * @param {string} body The body content of the file.
 * @param {string} oldFilePath The relative path of the old file.
 * @returns {string} The processed body content.
 */
function processBody(body, oldFilePath) {
  if (oldFilePath.endsWith('.jade')) {
    try {
      return pug.render(body, { pretty: true });
    } catch (e) {
      console.error(`Error rendering pug for ${oldFilePath}:`, e);
      return `<!-- PUG_RENDER_ERROR: ${e.message} -->\n${body}`;
    }
  }
  return body.trim();
}

/**
 * Handles copying assets (PDFs, images) and updating frontmatter with new paths.
 * @param {object} newData - The frontmatter object to update.
 * @param {object} oldData - The original frontmatter data.
 * @param {string} oldFilePath - The relative path of the old file.
 */
async function handleAssets(newData, oldData, oldFilePath) {
  const basename = path.basename(oldFilePath, path.extname(oldFilePath)).replace(/\.html$/, '');
  const normalizedPath = oldFilePath.replace(/\\/g, '/');

  if (oldData.pdf && normalizedPath.startsWith('writing/')) {
    const oldPdfPath = path.join(OLD_RAW_DIR, 'pdf', `${basename}.pdf`);
    if (await fs.pathExists(oldPdfPath)) {
      const newPdfPath = path.join(PUBLIC_DIR, 'pdf', `${basename}.pdf`);
      await fs.ensureDir(path.dirname(newPdfPath));
      await fs.copy(oldPdfPath, newPdfPath);
      newData.pdfUrl = `/pdf/${basename}.pdf`;
    }
  }

  if (normalizedPath.startsWith('talks/')) {
    if (oldData.handout) {
      const oldPdfPath = path.join(OLD_RAW_DIR, 'pdf', 'talks', `${basename}.handout.pdf`);
      if (await fs.pathExists(oldPdfPath)) {
        const newPdfPath = path.join(PUBLIC_DIR, 'pdf', 'talks', `${basename}.handout.pdf`);
        await fs.ensureDir(path.dirname(newPdfPath));
        await fs.copy(oldPdfPath, newPdfPath);
        newData.handoutUrl = `/pdf/talks/${basename}.handout.pdf`;
      }
    }
    if (oldData.slides) {
      const oldPdfPath = path.join(OLD_RAW_DIR, 'pdf', 'talks', `${basename}.slides.pdf`);
      if (await fs.pathExists(oldPdfPath)) {
        const newPdfPath = path.join(PUBLIC_DIR, 'pdf', 'talks', `${basename}.slides.pdf`);
        await fs.ensureDir(path.dirname(newPdfPath));
        await fs.copy(oldPdfPath, newPdfPath);
        newData.slidesUrl = `/pdf/talks/${basename}.slides.pdf`;
      }
    }
  }

  if (normalizedPath.startsWith('talks/')) {
    const oldImgDirPath = path.join(OLD_FILES_DIR, 'img', 'talks', basename);
    if (await fs.pathExists(oldImgDirPath)) {
      const newImgDirPath = path.join(PUBLIC_DIR, 'img', 'talks', basename);
      await fs.ensureDir(newImgDirPath);
      await fs.copy(oldImgDirPath, newImgDirPath);
      
      const imageFiles = (await fs.readdir(oldImgDirPath)).filter(f => !f.startsWith('.'));
      imageFiles.sort(); 
      newData.slideImages = imageFiles.map(file => `/img/talks/${basename}/${file}`);
    }
  }
}

/**
 * Generates a redirect rule string for a given file.
 * @param {string} oldFilePath - The relative path of the old file from OLD_DOCS_DIR.
 * @returns {string|null} A redirect rule string or null if not applicable.
 */
function generateRedirectRule(oldFilePath) {
  const normalizedPath = oldFilePath.replace(/\\/g, '/');
  const oldUrl = `/${normalizedPath}`.replace(/\.jade$/, '.html');
  
  // **FIXED LOGIC**: Create the new URL by replacing the extension, not by using basename
  const newRelativePath = normalizedPath.replace(/\.(jade|html\.md|html)$/, '');
  const newUrl = `/${newRelativePath}/`;

  return `${oldUrl}    ${newUrl}    301`;
}

/**
 * Main migration function.
 */
async function main() {
  console.log('Starting migration...');

  console.log('Clearing old content...');
  await fs.emptyDir(path.join(NEW_CONTENT_DIR, 'writing'));
  await fs.emptyDir(path.join(NEW_CONTENT_DIR, 'talks'));
  await fs.emptyDir(path.join(NEW_CONTENT_DIR, 'teaching'));
  await fs.emptyDir(path.join(PUBLIC_DIR, 'pdf'));
  await fs.emptyDir(path.join(PUBLIC_DIR, 'img'));
  console.log('Old content cleared.');

  const redirects = [];
  const sourceFiles = await findSourceFiles();
  console.log(`Found ${sourceFiles.length} source files to migrate.`);

  if (sourceFiles.length === 0) {
    console.error("No source files found. Check paths and filter logic.");
    return;
  }

  for (const oldFilePath of sourceFiles) {
    console.log(`Processing: ${oldFilePath}`);
    const fullOldPath = path.join(OLD_DOCS_DIR, oldFilePath);
    const content = await fs.readFile(fullOldPath, 'utf-8');

    const { frontmatter: fmString, body: rawBody } = extractFrontmatter(content);
    
    let oldData = {};
    if (fmString) {
        try {
            oldData = CSON.parse(fmString);
        } catch (e) {
            console.error(`  - Could not parse CSON for ${oldFilePath}. Error: ${e.message}`);
            continue;
        }
    }

    const { newData, bodyAbstract } = transformFrontmatter(oldData, oldFilePath);
    
    await handleAssets(newData, oldData, oldFilePath);

    const processedBody = processBody(rawBody, oldFilePath);

    const redirectRule = generateRedirectRule(oldFilePath);
    if (redirectRule) {
        redirects.push(redirectRule);
    }

    const yamlFrontmatter = yaml.stringify(newData);
    const newContent = `---\n${yamlFrontmatter}---\n\n${bodyAbstract}${processedBody}`;

    // **FIXED LOGIC**: Construct the new file path using the full relative path
    const newRelativePath = oldFilePath.replace(/\.(jade|html\.md|html)$/, '.md');
    const newFilePath = path.join(NEW_CONTENT_DIR, newRelativePath);
    
    await fs.ensureDir(path.dirname(newFilePath));
    await fs.writeFile(newFilePath, newContent);
  }

  const redirectsFilePath = path.join(PUBLIC_DIR, '_redirects');
  await fs.writeFile(redirectsFilePath, redirects.join('\n'));
  console.log(`\nGenerated ${redirects.length} redirects in ${redirectsFilePath}`);

  console.log('\nMigration completed successfully!');
}

main().catch(console.error);
</file>

</files>
